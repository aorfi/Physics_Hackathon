{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce digits of power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import sklearn.neural_network\n",
    "\n",
    "import neural_network_repn\n",
    "setattr(sklearn,'neural_network',neural_network_repn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten\n",
    "(X, y), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [2,4,6,8,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(sigmoid(np.dot(y,W[0])+b[0]),W[1])+b[1])\n",
    "def grad_score(x,use_hill,N,theta,mlp):\n",
    "    grad = np.zeros(x.shape)\n",
    "    for i in range(len(x)):\n",
    "        yp = x.copy(); yp[i] += 10/255; \n",
    "        ym = x.copy(); ym[i] -= 10/255;\n",
    "        grad[i] = 255/2*(score(yp,use_hill,N,theta,mlp)-score(ym,use_hill,N,theta,mlp))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255\n",
    "XT=x_test/255\n",
    "yT=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(len(X), 28**2)\n",
    "XT=XT.reshape(len(XT), 28**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call MNIST and keep 3s and 7s\n",
    "#mnist = sklearn.datasets.fetch_mldata(\"MNIST original\")\n",
    "\n",
    "# Rescale the data and extract all images for two digits\n",
    "#X, y = mnist.data / 255., mnist.target\n",
    "\n",
    "index = np.where((y == 3) | (y == 7))[0]\n",
    "X0,y = X[index], y[index]\n",
    "\n",
    "Index = np.where((yT == 3) | (yT == 7))[0]\n",
    "XT,yT = XT[Index], yT[Index]\n",
    "\n",
    "X0_train1, X0_train2, y_train1, y_train2 = sklearn.model_selection.train_test_split(X0, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68532570\n",
      "Iteration 2, loss = 0.54169603\n",
      "Iteration 3, loss = 0.45590040\n",
      "Iteration 4, loss = 0.41169568\n",
      "Iteration 5, loss = 0.38324967\n",
      "Iteration 6, loss = 0.36206907\n",
      "Iteration 7, loss = 0.34476738\n",
      "Iteration 8, loss = 0.32997236\n",
      "Iteration 9, loss = 0.31707564\n",
      "Iteration 10, loss = 0.30546932\n",
      "Iteration 11, loss = 0.29496854\n",
      "Iteration 12, loss = 0.28542674\n",
      "Iteration 13, loss = 0.27649154\n",
      "Iteration 14, loss = 0.26825278\n",
      "Iteration 15, loss = 0.26055771\n",
      "Iteration 16, loss = 0.25338672\n",
      "Iteration 17, loss = 0.24660802\n",
      "Iteration 18, loss = 0.24032469\n",
      "Iteration 19, loss = 0.23432757\n",
      "Iteration 20, loss = 0.22868381\n",
      "Iteration 21, loss = 0.22334239\n",
      "Iteration 22, loss = 0.21829543\n",
      "Iteration 23, loss = 0.21351398\n",
      "Iteration 24, loss = 0.20899333\n",
      "Iteration 25, loss = 0.20460361\n",
      "Iteration 26, loss = 0.20048637\n",
      "Iteration 27, loss = 0.19651923\n",
      "Iteration 28, loss = 0.19276620\n",
      "Iteration 29, loss = 0.18915783\n",
      "Iteration 30, loss = 0.18572761\n",
      "Iteration 31, loss = 0.18240273\n",
      "Iteration 32, loss = 0.17923768\n",
      "Iteration 33, loss = 0.17619652\n",
      "Iteration 34, loss = 0.17327373\n",
      "Iteration 35, loss = 0.17050258\n",
      "Iteration 36, loss = 0.16779226\n",
      "Iteration 37, loss = 0.16519403\n",
      "Iteration 38, loss = 0.16269833\n",
      "Iteration 39, loss = 0.16029354\n",
      "Iteration 40, loss = 0.15796500\n",
      "Iteration 41, loss = 0.15573541\n",
      "Iteration 42, loss = 0.15359528\n",
      "Iteration 43, loss = 0.15149640\n",
      "Iteration 44, loss = 0.14948246\n",
      "Iteration 45, loss = 0.14753437\n",
      "Iteration 46, loss = 0.14566573\n",
      "Iteration 47, loss = 0.14384194\n",
      "Iteration 48, loss = 0.14208409\n",
      "Iteration 49, loss = 0.14036741\n",
      "Iteration 50, loss = 0.13870422\n",
      "Iteration 51, loss = 0.13707670\n",
      "Iteration 52, loss = 0.13552712\n",
      "Iteration 53, loss = 0.13401840\n",
      "Iteration 54, loss = 0.13251702\n",
      "Iteration 55, loss = 0.13108444\n",
      "Iteration 56, loss = 0.12970059\n",
      "Iteration 57, loss = 0.12830638\n",
      "Iteration 58, loss = 0.12699148\n",
      "Iteration 59, loss = 0.12568630\n",
      "Iteration 60, loss = 0.12445163\n",
      "Iteration 61, loss = 0.12320771\n",
      "Iteration 62, loss = 0.12204496\n",
      "Iteration 63, loss = 0.12084134\n",
      "Iteration 64, loss = 0.11973119\n",
      "Iteration 65, loss = 0.11861331\n",
      "Iteration 66, loss = 0.11756292\n",
      "Iteration 67, loss = 0.11646731\n",
      "Iteration 68, loss = 0.11542550\n",
      "Iteration 69, loss = 0.11443823\n",
      "Iteration 70, loss = 0.11345976\n",
      "Iteration 71, loss = 0.11250635\n",
      "Iteration 72, loss = 0.11155558\n",
      "Iteration 73, loss = 0.11063804\n",
      "Iteration 74, loss = 0.10972317\n",
      "Iteration 75, loss = 0.10883954\n",
      "Iteration 76, loss = 0.10801603\n",
      "Iteration 77, loss = 0.10716393\n",
      "Iteration 78, loss = 0.10632705\n",
      "Iteration 79, loss = 0.10552631\n",
      "Iteration 80, loss = 0.10472214\n",
      "Iteration 81, loss = 0.10393558\n",
      "Iteration 82, loss = 0.10319346\n",
      "Iteration 83, loss = 0.10244344\n",
      "Iteration 84, loss = 0.10169878\n",
      "Iteration 85, loss = 0.10098090\n",
      "Iteration 86, loss = 0.10026575\n",
      "Iteration 87, loss = 0.09958252\n",
      "Iteration 88, loss = 0.09889802\n",
      "Iteration 89, loss = 0.09825830\n",
      "Iteration 90, loss = 0.09756364\n",
      "Iteration 91, loss = 0.09693158\n",
      "Iteration 92, loss = 0.09630748\n",
      "Iteration 93, loss = 0.09569503\n",
      "Iteration 94, loss = 0.09506800\n",
      "Iteration 95, loss = 0.09450028\n",
      "Iteration 96, loss = 0.09385849\n",
      "Iteration 97, loss = 0.09333259\n",
      "Iteration 98, loss = 0.09272008\n",
      "Iteration 99, loss = 0.09216195\n",
      "Iteration 100, loss = 0.09159152\n",
      "Iteration 101, loss = 0.09104977\n",
      "Iteration 102, loss = 0.09050694\n",
      "Iteration 103, loss = 0.08997095\n",
      "Iteration 104, loss = 0.08944222\n",
      "Iteration 105, loss = 0.08893732\n",
      "Iteration 106, loss = 0.08842934\n",
      "Iteration 107, loss = 0.08793327\n",
      "Iteration 108, loss = 0.08745513\n",
      "Iteration 109, loss = 0.08695374\n",
      "Iteration 110, loss = 0.08647716\n",
      "Iteration 111, loss = 0.08601372\n",
      "Iteration 112, loss = 0.08557632\n",
      "Iteration 113, loss = 0.08508485\n",
      "Iteration 114, loss = 0.08465314\n",
      "Iteration 115, loss = 0.08428219\n",
      "Iteration 116, loss = 0.08374113\n",
      "Iteration 117, loss = 0.08333244\n",
      "Iteration 118, loss = 0.08290965\n",
      "Iteration 119, loss = 0.08248570\n",
      "Iteration 120, loss = 0.08206918\n",
      "Iteration 121, loss = 0.08166259\n",
      "Iteration 122, loss = 0.08124862\n",
      "Iteration 123, loss = 0.08086789\n",
      "Iteration 124, loss = 0.08045562\n",
      "Iteration 125, loss = 0.08007562\n",
      "Iteration 126, loss = 0.07968322\n",
      "Iteration 127, loss = 0.07935150\n",
      "Iteration 128, loss = 0.07896731\n",
      "Iteration 129, loss = 0.07856744\n",
      "Iteration 130, loss = 0.07821807\n",
      "Iteration 131, loss = 0.07785921\n",
      "Iteration 132, loss = 0.07749534\n",
      "Iteration 133, loss = 0.07714325\n",
      "Iteration 134, loss = 0.07681469\n",
      "Iteration 135, loss = 0.07646957\n",
      "Iteration 136, loss = 0.07612286\n",
      "Iteration 137, loss = 0.07578464\n",
      "Iteration 138, loss = 0.07546371\n",
      "Iteration 139, loss = 0.07518423\n",
      "Iteration 140, loss = 0.07481677\n",
      "Iteration 141, loss = 0.07450121\n",
      "Iteration 142, loss = 0.07418136\n",
      "Iteration 143, loss = 0.07389425\n",
      "Iteration 144, loss = 0.07356194\n",
      "Iteration 145, loss = 0.07325396\n",
      "Iteration 146, loss = 0.07297290\n",
      "Iteration 147, loss = 0.07266634\n",
      "Iteration 148, loss = 0.07235994\n",
      "Iteration 149, loss = 0.07206455\n",
      "Iteration 150, loss = 0.07176358\n",
      "Iteration 151, loss = 0.07147305\n",
      "Iteration 152, loss = 0.07122552\n",
      "Iteration 153, loss = 0.07094800\n",
      "Iteration 154, loss = 0.07065062\n",
      "Iteration 155, loss = 0.07035567\n",
      "Iteration 156, loss = 0.07010470\n",
      "Iteration 157, loss = 0.06982005\n",
      "Iteration 158, loss = 0.06954393\n",
      "Iteration 159, loss = 0.06928523\n",
      "Iteration 160, loss = 0.06902757\n",
      "Iteration 161, loss = 0.06876655\n",
      "Iteration 162, loss = 0.06851281\n",
      "Iteration 163, loss = 0.06826324\n",
      "Iteration 164, loss = 0.06800142\n",
      "Iteration 165, loss = 0.06777240\n",
      "Iteration 166, loss = 0.06750758\n",
      "Iteration 167, loss = 0.06728010\n",
      "Iteration 168, loss = 0.06702287\n",
      "Iteration 169, loss = 0.06679149\n",
      "Iteration 170, loss = 0.06656884\n",
      "Iteration 171, loss = 0.06630415\n",
      "Iteration 172, loss = 0.06606267\n",
      "Iteration 173, loss = 0.06584120\n",
      "Iteration 174, loss = 0.06560888\n",
      "Iteration 175, loss = 0.06539839\n",
      "Iteration 176, loss = 0.06514599\n",
      "Iteration 177, loss = 0.06493433\n",
      "Iteration 178, loss = 0.06471133\n",
      "Iteration 179, loss = 0.06449579\n",
      "Iteration 180, loss = 0.06427132\n",
      "Iteration 181, loss = 0.06404203\n",
      "Iteration 182, loss = 0.06386052\n",
      "Iteration 183, loss = 0.06362406\n",
      "Iteration 184, loss = 0.06340781\n",
      "Iteration 185, loss = 0.06320418\n",
      "Iteration 186, loss = 0.06300024\n",
      "Iteration 187, loss = 0.06280867\n",
      "Iteration 188, loss = 0.06259588\n",
      "Iteration 189, loss = 0.06239784\n",
      "Iteration 190, loss = 0.06217875\n",
      "Iteration 191, loss = 0.06197784\n",
      "Iteration 192, loss = 0.06177782\n",
      "Iteration 193, loss = 0.06157947\n",
      "Iteration 194, loss = 0.06139168\n",
      "Iteration 195, loss = 0.06120330\n",
      "Iteration 196, loss = 0.06101446\n",
      "Iteration 197, loss = 0.06082677\n",
      "Iteration 198, loss = 0.06063413\n",
      "Iteration 199, loss = 0.06045952\n",
      "Iteration 200, loss = 0.06027049\n",
      "Iteration 201, loss = 0.06008439\n",
      "Iteration 202, loss = 0.05988927\n",
      "Iteration 203, loss = 0.05974559\n",
      "Iteration 204, loss = 0.05953532\n",
      "Iteration 205, loss = 0.05936947\n",
      "Iteration 206, loss = 0.05916008\n",
      "Iteration 207, loss = 0.05898521\n",
      "Iteration 208, loss = 0.05883024\n",
      "Iteration 209, loss = 0.05864715\n",
      "Iteration 210, loss = 0.05846686\n",
      "Iteration 211, loss = 0.05829221\n",
      "Iteration 212, loss = 0.05812132\n",
      "Iteration 213, loss = 0.05794940\n",
      "Iteration 214, loss = 0.05780038\n",
      "Iteration 215, loss = 0.05760650\n",
      "Iteration 216, loss = 0.05746312\n",
      "Iteration 217, loss = 0.05727858\n",
      "Iteration 218, loss = 0.05712399\n",
      "Iteration 219, loss = 0.05694416\n",
      "Iteration 220, loss = 0.05678511\n",
      "Iteration 221, loss = 0.05666324\n",
      "Iteration 222, loss = 0.05645980\n",
      "Iteration 223, loss = 0.05631359\n",
      "Iteration 224, loss = 0.05615260\n",
      "Iteration 225, loss = 0.05598195\n",
      "Iteration 226, loss = 0.05583746\n",
      "Iteration 227, loss = 0.05569115\n",
      "Iteration 228, loss = 0.05552253\n",
      "Iteration 229, loss = 0.05537460\n",
      "Iteration 230, loss = 0.05520860\n",
      "Iteration 231, loss = 0.05506665\n",
      "Iteration 232, loss = 0.05493096\n",
      "Iteration 233, loss = 0.05478007\n",
      "Iteration 234, loss = 0.05463540\n",
      "Iteration 235, loss = 0.05446863\n",
      "Iteration 236, loss = 0.05431385\n",
      "Iteration 237, loss = 0.05418071\n",
      "Iteration 238, loss = 0.05403517\n",
      "Iteration 239, loss = 0.05391130\n",
      "Iteration 240, loss = 0.05373603\n",
      "Iteration 241, loss = 0.05360677\n",
      "Iteration 242, loss = 0.05346964\n",
      "Iteration 243, loss = 0.05330710\n",
      "Iteration 244, loss = 0.05318601\n",
      "Iteration 245, loss = 0.05305180\n",
      "Iteration 246, loss = 0.05292891\n",
      "Iteration 247, loss = 0.05281786\n",
      "Iteration 248, loss = 0.05262505\n",
      "Iteration 249, loss = 0.05257068\n",
      "Iteration 250, loss = 0.05235097\n",
      "Iteration 251, loss = 0.05222972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.05208441\n",
      "Iteration 253, loss = 0.05193910\n",
      "Iteration 254, loss = 0.05181898\n",
      "Iteration 255, loss = 0.05167718\n",
      "Iteration 256, loss = 0.05153749\n",
      "Iteration 257, loss = 0.05140896\n",
      "Iteration 258, loss = 0.05131116\n",
      "Iteration 259, loss = 0.05121417\n",
      "Iteration 260, loss = 0.05099777\n",
      "Iteration 261, loss = 0.05088870\n",
      "Iteration 262, loss = 0.05073963\n",
      "Iteration 263, loss = 0.05060010\n",
      "Iteration 264, loss = 0.05047814\n",
      "Iteration 265, loss = 0.05033955\n",
      "Iteration 266, loss = 0.05020421\n",
      "Iteration 267, loss = 0.05007508\n",
      "Iteration 268, loss = 0.04995605\n",
      "Iteration 269, loss = 0.04985030\n",
      "Iteration 270, loss = 0.04969391\n",
      "Iteration 271, loss = 0.04959453\n",
      "Iteration 272, loss = 0.04944792\n",
      "Iteration 273, loss = 0.04932505\n",
      "Iteration 274, loss = 0.04919684\n",
      "Iteration 275, loss = 0.04908907\n",
      "Iteration 276, loss = 0.04897236\n",
      "Iteration 277, loss = 0.04882965\n",
      "Iteration 278, loss = 0.04872629\n",
      "Iteration 279, loss = 0.04862656\n",
      "Iteration 280, loss = 0.04846689\n",
      "Iteration 281, loss = 0.04835050\n",
      "Iteration 282, loss = 0.04825727\n",
      "Iteration 283, loss = 0.04813952\n",
      "Iteration 284, loss = 0.04806360\n",
      "Iteration 285, loss = 0.04787557\n",
      "Iteration 286, loss = 0.04776277\n",
      "Iteration 287, loss = 0.04765096\n",
      "Iteration 288, loss = 0.04754698\n",
      "Iteration 289, loss = 0.04742175\n",
      "Iteration 290, loss = 0.04729402\n",
      "Iteration 291, loss = 0.04718366\n",
      "Iteration 292, loss = 0.04707849\n",
      "Iteration 293, loss = 0.04696735\n",
      "Iteration 294, loss = 0.04686688\n",
      "Iteration 295, loss = 0.04673471\n",
      "Iteration 296, loss = 0.04662334\n",
      "Iteration 297, loss = 0.04650803\n",
      "Iteration 298, loss = 0.04639966\n",
      "Iteration 299, loss = 0.04629451\n",
      "Iteration 300, loss = 0.04620526\n",
      "Iteration 301, loss = 0.04607423\n",
      "Iteration 302, loss = 0.04596761\n",
      "Iteration 303, loss = 0.04585261\n",
      "Iteration 304, loss = 0.04574178\n",
      "Iteration 305, loss = 0.04564376\n",
      "Iteration 306, loss = 0.04553140\n",
      "Iteration 307, loss = 0.04542808\n",
      "Iteration 308, loss = 0.04532393\n",
      "Iteration 309, loss = 0.04521482\n",
      "Iteration 310, loss = 0.04509257\n",
      "Iteration 311, loss = 0.04499018\n",
      "Iteration 312, loss = 0.04489187\n",
      "Iteration 313, loss = 0.04477697\n",
      "Iteration 314, loss = 0.04467742\n",
      "Iteration 315, loss = 0.04457756\n",
      "Iteration 316, loss = 0.04447450\n",
      "Iteration 317, loss = 0.04437845\n",
      "Iteration 318, loss = 0.04427182\n",
      "Iteration 319, loss = 0.04415523\n",
      "Iteration 320, loss = 0.04406414\n",
      "Iteration 321, loss = 0.04394418\n",
      "Iteration 322, loss = 0.04385408\n",
      "Iteration 323, loss = 0.04374008\n",
      "Iteration 324, loss = 0.04365001\n",
      "Iteration 325, loss = 0.04355309\n",
      "Iteration 326, loss = 0.04344022\n",
      "Iteration 327, loss = 0.04335437\n",
      "Iteration 328, loss = 0.04326743\n",
      "Iteration 329, loss = 0.04315235\n",
      "Iteration 330, loss = 0.04305777\n",
      "Iteration 331, loss = 0.04294685\n",
      "Iteration 332, loss = 0.04287596\n",
      "Iteration 333, loss = 0.04276051\n",
      "Iteration 334, loss = 0.04267644\n",
      "Iteration 335, loss = 0.04256472\n",
      "Iteration 336, loss = 0.04246412\n",
      "Iteration 337, loss = 0.04237903\n",
      "Iteration 338, loss = 0.04227413\n",
      "Iteration 339, loss = 0.04217830\n",
      "Iteration 340, loss = 0.04206938\n",
      "Iteration 341, loss = 0.04201039\n",
      "Iteration 342, loss = 0.04195376\n",
      "Iteration 343, loss = 0.04180534\n",
      "Iteration 344, loss = 0.04170524\n",
      "Iteration 345, loss = 0.04161921\n",
      "Iteration 346, loss = 0.04152222\n",
      "Iteration 347, loss = 0.04142889\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 348, loss = 0.04132702\n",
      "Iteration 349, loss = 0.04130291\n",
      "Iteration 350, loss = 0.04128389\n",
      "Iteration 351, loss = 0.04126436\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 352, loss = 0.04124583\n",
      "Iteration 353, loss = 0.04124017\n",
      "Iteration 354, loss = 0.04123629\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 355, loss = 0.04123219\n",
      "Iteration 356, loss = 0.04123130\n",
      "Iteration 357, loss = 0.04123056\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 358, loss = 0.04122964\n",
      "Iteration 359, loss = 0.04122945\n",
      "Iteration 360, loss = 0.04122930\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 361, loss = 0.04122911\n",
      "Iteration 362, loss = 0.04122908\n",
      "Iteration 363, loss = 0.04122904\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(4,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_neural = sklearn.neural_network.MLPClassifier(activation = \"relu\", hidden_layer_sizes=(4,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "small_neural.fit(X0_train1,y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2038, 784)\n"
     ]
    }
   ],
   "source": [
    "use_hill=False;N=10;theta=0.5\n",
    "grad,pm = [],[]\n",
    "print(XT.shape)\n",
    "for i in range(len(XT)):\n",
    "    x0 = XT[i]\n",
    "    grad.append(grad_score(x0,use_hill,N,theta,small_neural))\n",
    "    pm.append((yT[i]-5)/2) #3 -> -1; 7 -> +1\n",
    "grad = np.asarray(grad)\n",
    "pm = np.asarray(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.5\n",
    "middle_SM = np.zeros((len(Ns),len(X0_train2)))\n",
    "for idx_N,N in enumerate(Ns):\n",
    "    X = hill(X0_train2,N,theta)\n",
    "    middle_SM[idx_N]=small_neural.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6198, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_SM=np.transpose(middle_SM)\n",
    "middle_SM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71727759\n",
      "Iteration 2, loss = 0.68896476\n",
      "Iteration 3, loss = 0.67292751\n",
      "Iteration 4, loss = 0.66452384\n",
      "Iteration 5, loss = 0.65927791\n",
      "Iteration 6, loss = 0.65522958\n",
      "Iteration 7, loss = 0.65181601\n",
      "Iteration 8, loss = 0.64894296\n",
      "Iteration 9, loss = 0.64647141\n",
      "Iteration 10, loss = 0.64431654\n",
      "Iteration 11, loss = 0.64233506\n",
      "Iteration 12, loss = 0.64048504\n",
      "Iteration 13, loss = 0.63871656\n",
      "Iteration 14, loss = 0.63701906\n",
      "Iteration 15, loss = 0.63534583\n",
      "Iteration 16, loss = 0.63371670\n",
      "Iteration 17, loss = 0.63211948\n",
      "Iteration 18, loss = 0.63055115\n",
      "Iteration 19, loss = 0.62899100\n",
      "Iteration 20, loss = 0.62744207\n",
      "Iteration 21, loss = 0.62590948\n",
      "Iteration 22, loss = 0.62437521\n",
      "Iteration 23, loss = 0.62286226\n",
      "Iteration 24, loss = 0.62133596\n",
      "Iteration 25, loss = 0.61980221\n",
      "Iteration 26, loss = 0.61828448\n",
      "Iteration 27, loss = 0.61675650\n",
      "Iteration 28, loss = 0.61525405\n",
      "Iteration 29, loss = 0.61369044\n",
      "Iteration 30, loss = 0.61214700\n",
      "Iteration 31, loss = 0.61060381\n",
      "Iteration 32, loss = 0.60905054\n",
      "Iteration 33, loss = 0.60746709\n",
      "Iteration 34, loss = 0.60588571\n",
      "Iteration 35, loss = 0.60430951\n",
      "Iteration 36, loss = 0.60270603\n",
      "Iteration 37, loss = 0.60110223\n",
      "Iteration 38, loss = 0.59948779\n",
      "Iteration 39, loss = 0.59782463\n",
      "Iteration 40, loss = 0.59617659\n",
      "Iteration 41, loss = 0.59452054\n",
      "Iteration 42, loss = 0.59283691\n",
      "Iteration 43, loss = 0.59115932\n",
      "Iteration 44, loss = 0.58942135\n",
      "Iteration 45, loss = 0.58770131\n",
      "Iteration 46, loss = 0.58595804\n",
      "Iteration 47, loss = 0.58420494\n",
      "Iteration 48, loss = 0.58241697\n",
      "Iteration 49, loss = 0.58063867\n",
      "Iteration 50, loss = 0.57881507\n",
      "Iteration 51, loss = 0.57696271\n",
      "Iteration 52, loss = 0.57510509\n",
      "Iteration 53, loss = 0.57323732\n",
      "Iteration 54, loss = 0.57137504\n",
      "Iteration 55, loss = 0.56941814\n",
      "Iteration 56, loss = 0.56747864\n",
      "Iteration 57, loss = 0.56554467\n",
      "Iteration 58, loss = 0.56352438\n",
      "Iteration 59, loss = 0.56152476\n",
      "Iteration 60, loss = 0.55950015\n",
      "Iteration 61, loss = 0.55743425\n",
      "Iteration 62, loss = 0.55536953\n",
      "Iteration 63, loss = 0.55319399\n",
      "Iteration 64, loss = 0.55108538\n",
      "Iteration 65, loss = 0.54891676\n",
      "Iteration 66, loss = 0.54670748\n",
      "Iteration 67, loss = 0.54445624\n",
      "Iteration 68, loss = 0.54217104\n",
      "Iteration 69, loss = 0.53985993\n",
      "Iteration 70, loss = 0.53751416\n",
      "Iteration 71, loss = 0.53513706\n",
      "Iteration 72, loss = 0.53270103\n",
      "Iteration 73, loss = 0.53020735\n",
      "Iteration 74, loss = 0.52771266\n",
      "Iteration 75, loss = 0.52510367\n",
      "Iteration 76, loss = 0.52244742\n",
      "Iteration 77, loss = 0.51972722\n",
      "Iteration 78, loss = 0.51696091\n",
      "Iteration 79, loss = 0.51414202\n",
      "Iteration 80, loss = 0.51120634\n",
      "Iteration 81, loss = 0.50822103\n",
      "Iteration 82, loss = 0.50516018\n",
      "Iteration 83, loss = 0.50203430\n",
      "Iteration 84, loss = 0.49887611\n",
      "Iteration 85, loss = 0.49568353\n",
      "Iteration 86, loss = 0.49245925\n",
      "Iteration 87, loss = 0.48927329\n",
      "Iteration 88, loss = 0.48609131\n",
      "Iteration 89, loss = 0.48292559\n",
      "Iteration 90, loss = 0.47982411\n",
      "Iteration 91, loss = 0.47675062\n",
      "Iteration 92, loss = 0.47373999\n",
      "Iteration 93, loss = 0.47073083\n",
      "Iteration 94, loss = 0.46771856\n",
      "Iteration 95, loss = 0.46466425\n",
      "Iteration 96, loss = 0.46166601\n",
      "Iteration 97, loss = 0.45863118\n",
      "Iteration 98, loss = 0.45562402\n",
      "Iteration 99, loss = 0.45258580\n",
      "Iteration 100, loss = 0.44954529\n",
      "Iteration 101, loss = 0.44651365\n",
      "Iteration 102, loss = 0.44346295\n",
      "Iteration 103, loss = 0.44043011\n",
      "Iteration 104, loss = 0.43737692\n",
      "Iteration 105, loss = 0.43434302\n",
      "Iteration 106, loss = 0.43126030\n",
      "Iteration 107, loss = 0.42824545\n",
      "Iteration 108, loss = 0.42521858\n",
      "Iteration 109, loss = 0.42209816\n",
      "Iteration 110, loss = 0.41905621\n",
      "Iteration 111, loss = 0.41600976\n",
      "Iteration 112, loss = 0.41295126\n",
      "Iteration 113, loss = 0.40983682\n",
      "Iteration 114, loss = 0.40679583\n",
      "Iteration 115, loss = 0.40374783\n",
      "Iteration 116, loss = 0.40067372\n",
      "Iteration 117, loss = 0.39763034\n",
      "Iteration 118, loss = 0.39458023\n",
      "Iteration 119, loss = 0.39154694\n",
      "Iteration 120, loss = 0.38849909\n",
      "Iteration 121, loss = 0.38549133\n",
      "Iteration 122, loss = 0.38245293\n",
      "Iteration 123, loss = 0.37943821\n",
      "Iteration 124, loss = 0.37641679\n",
      "Iteration 125, loss = 0.37342023\n",
      "Iteration 126, loss = 0.37044413\n",
      "Iteration 127, loss = 0.36745165\n",
      "Iteration 128, loss = 0.36447763\n",
      "Iteration 129, loss = 0.36150803\n",
      "Iteration 130, loss = 0.35860241\n",
      "Iteration 131, loss = 0.35560946\n",
      "Iteration 132, loss = 0.35271958\n",
      "Iteration 133, loss = 0.34973487\n",
      "Iteration 134, loss = 0.34686285\n",
      "Iteration 135, loss = 0.34394708\n",
      "Iteration 136, loss = 0.34107384\n",
      "Iteration 137, loss = 0.33821611\n",
      "Iteration 138, loss = 0.33534665\n",
      "Iteration 139, loss = 0.33254382\n",
      "Iteration 140, loss = 0.32967934\n",
      "Iteration 141, loss = 0.32690678\n",
      "Iteration 142, loss = 0.32410279\n",
      "Iteration 143, loss = 0.32135689\n",
      "Iteration 144, loss = 0.31857977\n",
      "Iteration 145, loss = 0.31587816\n",
      "Iteration 146, loss = 0.31317488\n",
      "Iteration 147, loss = 0.31043862\n",
      "Iteration 148, loss = 0.30776374\n",
      "Iteration 149, loss = 0.30515118\n",
      "Iteration 150, loss = 0.30245565\n",
      "Iteration 151, loss = 0.29983729\n",
      "Iteration 152, loss = 0.29724031\n",
      "Iteration 153, loss = 0.29467354\n",
      "Iteration 154, loss = 0.29210687\n",
      "Iteration 155, loss = 0.28955866\n",
      "Iteration 156, loss = 0.28704625\n",
      "Iteration 157, loss = 0.28455048\n",
      "Iteration 158, loss = 0.28207039\n",
      "Iteration 159, loss = 0.27963540\n",
      "Iteration 160, loss = 0.27718230\n",
      "Iteration 161, loss = 0.27477415\n",
      "Iteration 162, loss = 0.27241073\n",
      "Iteration 163, loss = 0.27003021\n",
      "Iteration 164, loss = 0.26767782\n",
      "Iteration 165, loss = 0.26546346\n",
      "Iteration 166, loss = 0.26308638\n",
      "Iteration 167, loss = 0.26086031\n",
      "Iteration 168, loss = 0.25857483\n",
      "Iteration 169, loss = 0.25632119\n",
      "Iteration 170, loss = 0.25412485\n",
      "Iteration 171, loss = 0.25196474\n",
      "Iteration 172, loss = 0.24985866\n",
      "Iteration 173, loss = 0.24763099\n",
      "Iteration 174, loss = 0.24552699\n",
      "Iteration 175, loss = 0.24344211\n",
      "Iteration 176, loss = 0.24136184\n",
      "Iteration 177, loss = 0.23932428\n",
      "Iteration 178, loss = 0.23729348\n",
      "Iteration 179, loss = 0.23530168\n",
      "Iteration 180, loss = 0.23332716\n",
      "Iteration 181, loss = 0.23137927\n",
      "Iteration 182, loss = 0.22944446\n",
      "Iteration 183, loss = 0.22753468\n",
      "Iteration 184, loss = 0.22564830\n",
      "Iteration 185, loss = 0.22374838\n",
      "Iteration 186, loss = 0.22193538\n",
      "Iteration 187, loss = 0.22009489\n",
      "Iteration 188, loss = 0.21829978\n",
      "Iteration 189, loss = 0.21654719\n",
      "Iteration 190, loss = 0.21479666\n",
      "Iteration 191, loss = 0.21306090\n",
      "Iteration 192, loss = 0.21132314\n",
      "Iteration 193, loss = 0.20962436\n",
      "Iteration 194, loss = 0.20795875\n",
      "Iteration 195, loss = 0.20629343\n",
      "Iteration 196, loss = 0.20469799\n",
      "Iteration 197, loss = 0.20306138\n",
      "Iteration 198, loss = 0.20146333\n",
      "Iteration 199, loss = 0.19990780\n",
      "Iteration 200, loss = 0.19836999\n",
      "Iteration 201, loss = 0.19680960\n",
      "Iteration 202, loss = 0.19529638\n",
      "Iteration 203, loss = 0.19385307\n",
      "Iteration 204, loss = 0.19236657\n",
      "Iteration 205, loss = 0.19087148\n",
      "Iteration 206, loss = 0.18944947\n",
      "Iteration 207, loss = 0.18802604\n",
      "Iteration 208, loss = 0.18666664\n",
      "Iteration 209, loss = 0.18525792\n",
      "Iteration 210, loss = 0.18386102\n",
      "Iteration 211, loss = 0.18251737\n",
      "Iteration 212, loss = 0.18119522\n",
      "Iteration 213, loss = 0.17989255\n",
      "Iteration 214, loss = 0.17860420\n",
      "Iteration 215, loss = 0.17729340\n",
      "Iteration 216, loss = 0.17604252\n",
      "Iteration 217, loss = 0.17477537\n",
      "Iteration 218, loss = 0.17364720\n",
      "Iteration 219, loss = 0.17233112\n",
      "Iteration 220, loss = 0.17114996\n",
      "Iteration 221, loss = 0.16996240\n",
      "Iteration 222, loss = 0.16879140\n",
      "Iteration 223, loss = 0.16763151\n",
      "Iteration 224, loss = 0.16651173\n",
      "Iteration 225, loss = 0.16537020\n",
      "Iteration 226, loss = 0.16427029\n",
      "Iteration 227, loss = 0.16319116\n",
      "Iteration 228, loss = 0.16208748\n",
      "Iteration 229, loss = 0.16100051\n",
      "Iteration 230, loss = 0.15995371\n",
      "Iteration 231, loss = 0.15892542\n",
      "Iteration 232, loss = 0.15787952\n",
      "Iteration 233, loss = 0.15685826\n",
      "Iteration 234, loss = 0.15587404\n",
      "Iteration 235, loss = 0.15485763\n",
      "Iteration 236, loss = 0.15388099\n",
      "Iteration 237, loss = 0.15291656\n",
      "Iteration 238, loss = 0.15199028\n",
      "Iteration 239, loss = 0.15101623\n",
      "Iteration 240, loss = 0.15009652\n",
      "Iteration 241, loss = 0.14917766\n",
      "Iteration 242, loss = 0.14824953\n",
      "Iteration 243, loss = 0.14735534\n",
      "Iteration 244, loss = 0.14646957\n",
      "Iteration 245, loss = 0.14558930\n",
      "Iteration 246, loss = 0.14474737\n",
      "Iteration 247, loss = 0.14388556\n",
      "Iteration 248, loss = 0.14304231\n",
      "Iteration 249, loss = 0.14221578\n",
      "Iteration 250, loss = 0.14137217\n",
      "Iteration 251, loss = 0.14056355\n",
      "Iteration 252, loss = 0.13974341\n",
      "Iteration 253, loss = 0.13895714\n",
      "Iteration 254, loss = 0.13817009\n",
      "Iteration 255, loss = 0.13739656\n",
      "Iteration 256, loss = 0.13663243\n",
      "Iteration 257, loss = 0.13586292\n",
      "Iteration 258, loss = 0.13512813\n",
      "Iteration 259, loss = 0.13441219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.13364122\n",
      "Iteration 261, loss = 0.13294005\n",
      "Iteration 262, loss = 0.13221867\n",
      "Iteration 263, loss = 0.13153380\n",
      "Iteration 264, loss = 0.13082748\n",
      "Iteration 265, loss = 0.13016063\n",
      "Iteration 266, loss = 0.12948950\n",
      "Iteration 267, loss = 0.12884053\n",
      "Iteration 268, loss = 0.12818120\n",
      "Iteration 269, loss = 0.12753881\n",
      "Iteration 270, loss = 0.12690060\n",
      "Iteration 271, loss = 0.12626513\n",
      "Iteration 272, loss = 0.12567181\n",
      "Iteration 273, loss = 0.12504343\n",
      "Iteration 274, loss = 0.12444514\n",
      "Iteration 275, loss = 0.12386888\n",
      "Iteration 276, loss = 0.12330343\n",
      "Iteration 277, loss = 0.12272019\n",
      "Iteration 278, loss = 0.12215459\n",
      "Iteration 279, loss = 0.12160550\n",
      "Iteration 280, loss = 0.12107575\n",
      "Iteration 281, loss = 0.12053788\n",
      "Iteration 282, loss = 0.12001190\n",
      "Iteration 283, loss = 0.11952200\n",
      "Iteration 284, loss = 0.11899082\n",
      "Iteration 285, loss = 0.11849125\n",
      "Iteration 286, loss = 0.11799853\n",
      "Iteration 287, loss = 0.11751882\n",
      "Iteration 288, loss = 0.11704670\n",
      "Iteration 289, loss = 0.11658362\n",
      "Iteration 290, loss = 0.11610909\n",
      "Iteration 291, loss = 0.11565436\n",
      "Iteration 292, loss = 0.11524269\n",
      "Iteration 293, loss = 0.11480760\n",
      "Iteration 294, loss = 0.11431615\n",
      "Iteration 295, loss = 0.11388847\n",
      "Iteration 296, loss = 0.11346565\n",
      "Iteration 297, loss = 0.11304485\n",
      "Iteration 298, loss = 0.11264045\n",
      "Iteration 299, loss = 0.11224821\n",
      "Iteration 300, loss = 0.11182781\n",
      "Iteration 301, loss = 0.11142473\n",
      "Iteration 302, loss = 0.11103163\n",
      "Iteration 303, loss = 0.11067759\n",
      "Iteration 304, loss = 0.11028200\n",
      "Iteration 305, loss = 0.10987881\n",
      "Iteration 306, loss = 0.10952758\n",
      "Iteration 307, loss = 0.10917499\n",
      "Iteration 308, loss = 0.10879897\n",
      "Iteration 309, loss = 0.10843337\n",
      "Iteration 310, loss = 0.10809695\n",
      "Iteration 311, loss = 0.10773775\n",
      "Iteration 312, loss = 0.10738771\n",
      "Iteration 313, loss = 0.10703072\n",
      "Iteration 314, loss = 0.10669920\n",
      "Iteration 315, loss = 0.10637090\n",
      "Iteration 316, loss = 0.10605503\n",
      "Iteration 317, loss = 0.10571998\n",
      "Iteration 318, loss = 0.10539905\n",
      "Iteration 319, loss = 0.10507719\n",
      "Iteration 320, loss = 0.10477149\n",
      "Iteration 321, loss = 0.10447518\n",
      "Iteration 322, loss = 0.10416490\n",
      "Iteration 323, loss = 0.10386494\n",
      "Iteration 324, loss = 0.10355655\n",
      "Iteration 325, loss = 0.10326113\n",
      "Iteration 326, loss = 0.10298378\n",
      "Iteration 327, loss = 0.10269507\n",
      "Iteration 328, loss = 0.10240588\n",
      "Iteration 329, loss = 0.10210492\n",
      "Iteration 330, loss = 0.10185129\n",
      "Iteration 331, loss = 0.10154520\n",
      "Iteration 332, loss = 0.10127606\n",
      "Iteration 333, loss = 0.10101839\n",
      "Iteration 334, loss = 0.10073689\n",
      "Iteration 335, loss = 0.10047842\n",
      "Iteration 336, loss = 0.10022039\n",
      "Iteration 337, loss = 0.09997732\n",
      "Iteration 338, loss = 0.09970318\n",
      "Iteration 339, loss = 0.09946357\n",
      "Iteration 340, loss = 0.09920500\n",
      "Iteration 341, loss = 0.09896060\n",
      "Iteration 342, loss = 0.09871527\n",
      "Iteration 343, loss = 0.09848126\n",
      "Iteration 344, loss = 0.09826594\n",
      "Iteration 345, loss = 0.09801233\n",
      "Iteration 346, loss = 0.09780167\n",
      "Iteration 347, loss = 0.09755203\n",
      "Iteration 348, loss = 0.09732184\n",
      "Iteration 349, loss = 0.09711204\n",
      "Iteration 350, loss = 0.09686381\n",
      "Iteration 351, loss = 0.09666857\n",
      "Iteration 352, loss = 0.09643429\n",
      "Iteration 353, loss = 0.09621132\n",
      "Iteration 354, loss = 0.09601726\n",
      "Iteration 355, loss = 0.09581348\n",
      "Iteration 356, loss = 0.09558584\n",
      "Iteration 357, loss = 0.09538156\n",
      "Iteration 358, loss = 0.09518458\n",
      "Iteration 359, loss = 0.09497127\n",
      "Iteration 360, loss = 0.09476577\n",
      "Iteration 361, loss = 0.09458088\n",
      "Iteration 362, loss = 0.09438953\n",
      "Iteration 363, loss = 0.09419469\n",
      "Iteration 364, loss = 0.09400144\n",
      "Iteration 365, loss = 0.09380472\n",
      "Iteration 366, loss = 0.09364312\n",
      "Iteration 367, loss = 0.09347241\n",
      "Iteration 368, loss = 0.09325309\n",
      "Iteration 369, loss = 0.09309082\n",
      "Iteration 370, loss = 0.09288465\n",
      "Iteration 371, loss = 0.09270842\n",
      "Iteration 372, loss = 0.09253723\n",
      "Iteration 373, loss = 0.09240099\n",
      "Iteration 374, loss = 0.09219418\n",
      "Iteration 375, loss = 0.09204313\n",
      "Iteration 376, loss = 0.09186312\n",
      "Iteration 377, loss = 0.09168225\n",
      "Iteration 378, loss = 0.09152857\n",
      "Iteration 379, loss = 0.09136435\n",
      "Iteration 380, loss = 0.09120106\n",
      "Iteration 381, loss = 0.09107573\n",
      "Iteration 382, loss = 0.09087712\n",
      "Iteration 383, loss = 0.09072741\n",
      "Iteration 384, loss = 0.09056519\n",
      "Iteration 385, loss = 0.09041225\n",
      "Iteration 386, loss = 0.09027591\n",
      "Iteration 387, loss = 0.09017187\n",
      "Iteration 388, loss = 0.08997010\n",
      "Iteration 389, loss = 0.08979408\n",
      "Iteration 390, loss = 0.08965728\n",
      "Iteration 391, loss = 0.08952916\n",
      "Iteration 392, loss = 0.08937410\n",
      "Iteration 393, loss = 0.08923894\n",
      "Iteration 394, loss = 0.08909328\n",
      "Iteration 395, loss = 0.08895290\n",
      "Iteration 396, loss = 0.08880058\n",
      "Iteration 397, loss = 0.08868336\n",
      "Iteration 398, loss = 0.08853541\n",
      "Iteration 399, loss = 0.08839942\n",
      "Iteration 400, loss = 0.08826967\n",
      "Iteration 401, loss = 0.08813990\n",
      "Iteration 402, loss = 0.08800567\n",
      "Iteration 403, loss = 0.08788502\n",
      "Iteration 404, loss = 0.08774640\n",
      "Iteration 405, loss = 0.08764457\n",
      "Iteration 406, loss = 0.08749600\n",
      "Iteration 407, loss = 0.08737416\n",
      "Iteration 408, loss = 0.08724498\n",
      "Iteration 409, loss = 0.08713092\n",
      "Iteration 410, loss = 0.08705219\n",
      "Iteration 411, loss = 0.08688651\n",
      "Iteration 412, loss = 0.08676890\n",
      "Iteration 413, loss = 0.08664407\n",
      "Iteration 414, loss = 0.08654066\n",
      "Iteration 415, loss = 0.08644822\n",
      "Iteration 416, loss = 0.08632191\n",
      "Iteration 417, loss = 0.08619534\n",
      "Iteration 418, loss = 0.08609549\n",
      "Iteration 419, loss = 0.08597595\n",
      "Iteration 420, loss = 0.08586181\n",
      "Iteration 421, loss = 0.08575416\n",
      "Iteration 422, loss = 0.08565172\n",
      "Iteration 423, loss = 0.08555510\n",
      "Iteration 424, loss = 0.08544652\n",
      "Iteration 425, loss = 0.08533255\n",
      "Iteration 426, loss = 0.08526628\n",
      "Iteration 427, loss = 0.08514590\n",
      "Iteration 428, loss = 0.08501278\n",
      "Iteration 429, loss = 0.08495989\n",
      "Iteration 430, loss = 0.08483719\n",
      "Iteration 431, loss = 0.08470841\n",
      "Iteration 432, loss = 0.08461837\n",
      "Iteration 433, loss = 0.08452453\n",
      "Iteration 434, loss = 0.08445223\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 435, loss = 0.08434699\n",
      "Iteration 436, loss = 0.08431280\n",
      "Iteration 437, loss = 0.08429432\n",
      "Iteration 438, loss = 0.08427517\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 439, loss = 0.08425455\n",
      "Iteration 440, loss = 0.08425180\n",
      "Iteration 441, loss = 0.08424743\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 442, loss = 0.08424433\n",
      "Iteration 443, loss = 0.08424316\n",
      "Iteration 444, loss = 0.08424241\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 445, loss = 0.08424166\n",
      "Iteration 446, loss = 0.08424153\n",
      "Iteration 447, loss = 0.08424131\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 448, loss = 0.08424118\n",
      "Iteration 449, loss = 0.08424113\n",
      "Iteration 450, loss = 0.08424110\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import sklearn.neural_network\n",
    "\n",
    "import neural_network_repn\n",
    "setattr(sklearn,'neural_network',neural_network_repn)'''\n",
    "master_neural = sklearn.neural_network.MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(5,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "#middle_SM = np.log(middle_SM)\n",
    "master_neural.fit(middle_SM,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden-layer feed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights and Biases for Hidden Layer\n",
    "w1 = small_neural.coefs_[0]\n",
    "b1 = small_neural.intercepts_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6198, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "theta = 0.5\n",
    "middle_HFI = np.zeros(shape=(len(X0_train2),len(Ns),4))\n",
    "for idx_N,N in enumerate(Ns):\n",
    "    X = hill(X0_train2,N,theta)\n",
    "    for i in range(len(X0_train2)):\n",
    "        h1=np.matmul(X0_train2[i],w1) + b1\n",
    "        h1=np.maximum(0, h1)\n",
    "        middle_HFI[i][idx_N]=h1\n",
    "print(middle_HFI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_HFI=middle_HFI.reshape(6198,4*len(Ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65377235\n",
      "Iteration 2, loss = 0.63225205\n",
      "Iteration 3, loss = 0.61178907\n",
      "Iteration 4, loss = 0.59311702\n",
      "Iteration 5, loss = 0.57545666\n",
      "Iteration 6, loss = 0.55871529\n",
      "Iteration 7, loss = 0.54260954\n",
      "Iteration 8, loss = 0.52732164\n",
      "Iteration 9, loss = 0.51260418\n",
      "Iteration 10, loss = 0.49849456\n",
      "Iteration 11, loss = 0.48488527\n",
      "Iteration 12, loss = 0.47175075\n",
      "Iteration 13, loss = 0.45903245\n",
      "Iteration 14, loss = 0.44673731\n",
      "Iteration 15, loss = 0.43482396\n",
      "Iteration 16, loss = 0.42325258\n",
      "Iteration 17, loss = 0.41205540\n",
      "Iteration 18, loss = 0.40122556\n",
      "Iteration 19, loss = 0.39070701\n",
      "Iteration 20, loss = 0.38055175\n",
      "Iteration 21, loss = 0.37071399\n",
      "Iteration 22, loss = 0.36118817\n",
      "Iteration 23, loss = 0.35199309\n",
      "Iteration 24, loss = 0.34309333\n",
      "Iteration 25, loss = 0.33449860\n",
      "Iteration 26, loss = 0.32619586\n",
      "Iteration 27, loss = 0.31816934\n",
      "Iteration 28, loss = 0.31042251\n",
      "Iteration 29, loss = 0.30293393\n",
      "Iteration 30, loss = 0.29570907\n",
      "Iteration 31, loss = 0.28872487\n",
      "Iteration 32, loss = 0.28198155\n",
      "Iteration 33, loss = 0.27548073\n",
      "Iteration 34, loss = 0.26919967\n",
      "Iteration 35, loss = 0.26312561\n",
      "Iteration 36, loss = 0.25727547\n",
      "Iteration 37, loss = 0.25162098\n",
      "Iteration 38, loss = 0.24616648\n",
      "Iteration 39, loss = 0.24088866\n",
      "Iteration 40, loss = 0.23579895\n",
      "Iteration 41, loss = 0.23088733\n",
      "Iteration 42, loss = 0.22613664\n",
      "Iteration 43, loss = 0.22154661\n",
      "Iteration 44, loss = 0.21711806\n",
      "Iteration 45, loss = 0.21283553\n",
      "Iteration 46, loss = 0.20870100\n",
      "Iteration 47, loss = 0.20470231\n",
      "Iteration 48, loss = 0.20084273\n",
      "Iteration 49, loss = 0.19710174\n",
      "Iteration 50, loss = 0.19348814\n",
      "Iteration 51, loss = 0.18999961\n",
      "Iteration 52, loss = 0.18662391\n",
      "Iteration 53, loss = 0.18335262\n",
      "Iteration 54, loss = 0.18019345\n",
      "Iteration 55, loss = 0.17712651\n",
      "Iteration 56, loss = 0.17417606\n",
      "Iteration 57, loss = 0.17130369\n",
      "Iteration 58, loss = 0.16852635\n",
      "Iteration 59, loss = 0.16583976\n",
      "Iteration 60, loss = 0.16323006\n",
      "Iteration 61, loss = 0.16070941\n",
      "Iteration 62, loss = 0.15826564\n",
      "Iteration 63, loss = 0.15589282\n",
      "Iteration 64, loss = 0.15359444\n",
      "Iteration 65, loss = 0.15136414\n",
      "Iteration 66, loss = 0.14920082\n",
      "Iteration 67, loss = 0.14710590\n",
      "Iteration 68, loss = 0.14506931\n",
      "Iteration 69, loss = 0.14309210\n",
      "Iteration 70, loss = 0.14117828\n",
      "Iteration 71, loss = 0.13931018\n",
      "Iteration 72, loss = 0.13750411\n",
      "Iteration 73, loss = 0.13574499\n",
      "Iteration 74, loss = 0.13403830\n",
      "Iteration 75, loss = 0.13237936\n",
      "Iteration 76, loss = 0.13076496\n",
      "Iteration 77, loss = 0.12919512\n",
      "Iteration 78, loss = 0.12767174\n",
      "Iteration 79, loss = 0.12618468\n",
      "Iteration 80, loss = 0.12474319\n",
      "Iteration 81, loss = 0.12333556\n",
      "Iteration 82, loss = 0.12196969\n",
      "Iteration 83, loss = 0.12063704\n",
      "Iteration 84, loss = 0.11934005\n",
      "Iteration 85, loss = 0.11807826\n",
      "Iteration 86, loss = 0.11684659\n",
      "Iteration 87, loss = 0.11565008\n",
      "Iteration 88, loss = 0.11447757\n",
      "Iteration 89, loss = 0.11333800\n",
      "Iteration 90, loss = 0.11223169\n",
      "Iteration 91, loss = 0.11114170\n",
      "Iteration 92, loss = 0.11009015\n",
      "Iteration 93, loss = 0.10905720\n",
      "Iteration 94, loss = 0.10805276\n",
      "Iteration 95, loss = 0.10706899\n",
      "Iteration 96, loss = 0.10611207\n",
      "Iteration 97, loss = 0.10517545\n",
      "Iteration 98, loss = 0.10426339\n",
      "Iteration 99, loss = 0.10336797\n",
      "Iteration 100, loss = 0.10249737\n",
      "Iteration 101, loss = 0.10164765\n",
      "Iteration 102, loss = 0.10081270\n",
      "Iteration 103, loss = 0.10000326\n",
      "Iteration 104, loss = 0.09920182\n",
      "Iteration 105, loss = 0.09842714\n",
      "Iteration 106, loss = 0.09766636\n",
      "Iteration 107, loss = 0.09692079\n",
      "Iteration 108, loss = 0.09619477\n",
      "Iteration 109, loss = 0.09548038\n",
      "Iteration 110, loss = 0.09478559\n",
      "Iteration 111, loss = 0.09410178\n",
      "Iteration 112, loss = 0.09343453\n",
      "Iteration 113, loss = 0.09277921\n",
      "Iteration 114, loss = 0.09214249\n",
      "Iteration 115, loss = 0.09151306\n",
      "Iteration 116, loss = 0.09089714\n",
      "Iteration 117, loss = 0.09029626\n",
      "Iteration 118, loss = 0.08970757\n",
      "Iteration 119, loss = 0.08912954\n",
      "Iteration 120, loss = 0.08856351\n",
      "Iteration 121, loss = 0.08800625\n",
      "Iteration 122, loss = 0.08746314\n",
      "Iteration 123, loss = 0.08692811\n",
      "Iteration 124, loss = 0.08640352\n",
      "Iteration 125, loss = 0.08589184\n",
      "Iteration 126, loss = 0.08538717\n",
      "Iteration 127, loss = 0.08489252\n",
      "Iteration 128, loss = 0.08440713\n",
      "Iteration 129, loss = 0.08393121\n",
      "Iteration 130, loss = 0.08346462\n",
      "Iteration 131, loss = 0.08300509\n",
      "Iteration 132, loss = 0.08255414\n",
      "Iteration 133, loss = 0.08211436\n",
      "Iteration 134, loss = 0.08167927\n",
      "Iteration 135, loss = 0.08125121\n",
      "Iteration 136, loss = 0.08083300\n",
      "Iteration 137, loss = 0.08041969\n",
      "Iteration 138, loss = 0.08001620\n",
      "Iteration 139, loss = 0.07961852\n",
      "Iteration 140, loss = 0.07922710\n",
      "Iteration 141, loss = 0.07884496\n",
      "Iteration 142, loss = 0.07846595\n",
      "Iteration 143, loss = 0.07809511\n",
      "Iteration 144, loss = 0.07772992\n",
      "Iteration 145, loss = 0.07737170\n",
      "Iteration 146, loss = 0.07701893\n",
      "Iteration 147, loss = 0.07667190\n",
      "Iteration 148, loss = 0.07633029\n",
      "Iteration 149, loss = 0.07599600\n",
      "Iteration 150, loss = 0.07566337\n",
      "Iteration 151, loss = 0.07533867\n",
      "Iteration 152, loss = 0.07501983\n",
      "Iteration 153, loss = 0.07470500\n",
      "Iteration 154, loss = 0.07439571\n",
      "Iteration 155, loss = 0.07408821\n",
      "Iteration 156, loss = 0.07379018\n",
      "Iteration 157, loss = 0.07349415\n",
      "Iteration 158, loss = 0.07320278\n",
      "Iteration 159, loss = 0.07291679\n",
      "Iteration 160, loss = 0.07263432\n",
      "Iteration 161, loss = 0.07235506\n",
      "Iteration 162, loss = 0.07208106\n",
      "Iteration 163, loss = 0.07181262\n",
      "Iteration 164, loss = 0.07154600\n",
      "Iteration 165, loss = 0.07128509\n",
      "Iteration 166, loss = 0.07102609\n",
      "Iteration 167, loss = 0.07077355\n",
      "Iteration 168, loss = 0.07052002\n",
      "Iteration 169, loss = 0.07027361\n",
      "Iteration 170, loss = 0.07003103\n",
      "Iteration 171, loss = 0.06978996\n",
      "Iteration 172, loss = 0.06955482\n",
      "Iteration 173, loss = 0.06932061\n",
      "Iteration 174, loss = 0.06909116\n",
      "Iteration 175, loss = 0.06886330\n",
      "Iteration 176, loss = 0.06863986\n",
      "Iteration 177, loss = 0.06841969\n",
      "Iteration 178, loss = 0.06820133\n",
      "Iteration 179, loss = 0.06798639\n",
      "Iteration 180, loss = 0.06777567\n",
      "Iteration 181, loss = 0.06756675\n",
      "Iteration 182, loss = 0.06735882\n",
      "Iteration 183, loss = 0.06715752\n",
      "Iteration 184, loss = 0.06695593\n",
      "Iteration 185, loss = 0.06675848\n",
      "Iteration 186, loss = 0.06656274\n",
      "Iteration 187, loss = 0.06637211\n",
      "Iteration 188, loss = 0.06617995\n",
      "Iteration 189, loss = 0.06599110\n",
      "Iteration 190, loss = 0.06580546\n",
      "Iteration 191, loss = 0.06562336\n",
      "Iteration 192, loss = 0.06544338\n",
      "Iteration 193, loss = 0.06526408\n",
      "Iteration 194, loss = 0.06508867\n",
      "Iteration 195, loss = 0.06491392\n",
      "Iteration 196, loss = 0.06474210\n",
      "Iteration 197, loss = 0.06457325\n",
      "Iteration 198, loss = 0.06440454\n",
      "Iteration 199, loss = 0.06424056\n",
      "Iteration 200, loss = 0.06407460\n",
      "Iteration 201, loss = 0.06391588\n",
      "Iteration 202, loss = 0.06375391\n",
      "Iteration 203, loss = 0.06359740\n",
      "Iteration 204, loss = 0.06344156\n",
      "Iteration 205, loss = 0.06328658\n",
      "Iteration 206, loss = 0.06313457\n",
      "Iteration 207, loss = 0.06298479\n",
      "Iteration 208, loss = 0.06283602\n",
      "Iteration 209, loss = 0.06268923\n",
      "Iteration 210, loss = 0.06254673\n",
      "Iteration 211, loss = 0.06240133\n",
      "Iteration 212, loss = 0.06225931\n",
      "Iteration 213, loss = 0.06211854\n",
      "Iteration 214, loss = 0.06198213\n",
      "Iteration 215, loss = 0.06184454\n",
      "Iteration 216, loss = 0.06170878\n",
      "Iteration 217, loss = 0.06157537\n",
      "Iteration 218, loss = 0.06144169\n",
      "Iteration 219, loss = 0.06131205\n",
      "Iteration 220, loss = 0.06118275\n",
      "Iteration 221, loss = 0.06105491\n",
      "Iteration 222, loss = 0.06092974\n",
      "Iteration 223, loss = 0.06080387\n",
      "Iteration 224, loss = 0.06067899\n",
      "Iteration 225, loss = 0.06055784\n",
      "Iteration 226, loss = 0.06043614\n",
      "Iteration 227, loss = 0.06031657\n",
      "Iteration 228, loss = 0.06019916\n",
      "Iteration 229, loss = 0.06008048\n",
      "Iteration 230, loss = 0.05996604\n",
      "Iteration 231, loss = 0.05985069\n",
      "Iteration 232, loss = 0.05973566\n",
      "Iteration 233, loss = 0.05962503\n",
      "Iteration 234, loss = 0.05951293\n",
      "Iteration 235, loss = 0.05940380\n",
      "Iteration 236, loss = 0.05929612\n",
      "Iteration 237, loss = 0.05918819\n",
      "Iteration 238, loss = 0.05908143\n",
      "Iteration 239, loss = 0.05897611\n",
      "Iteration 240, loss = 0.05887127\n",
      "Iteration 241, loss = 0.05876806\n",
      "Iteration 242, loss = 0.05866687\n",
      "Iteration 243, loss = 0.05856390\n",
      "Iteration 244, loss = 0.05846438\n",
      "Iteration 245, loss = 0.05836550\n",
      "Iteration 246, loss = 0.05826826\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 247, loss = 0.05819366\n",
      "Iteration 248, loss = 0.05816708\n",
      "Iteration 249, loss = 0.05814741\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 250, loss = 0.05813287\n",
      "Iteration 251, loss = 0.05812781\n",
      "Iteration 252, loss = 0.05812389\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 253, loss = 0.05812094\n",
      "Iteration 254, loss = 0.05811993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.05811916\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 256, loss = 0.05811856\n",
      "Iteration 257, loss = 0.05811836\n",
      "Iteration 258, loss = 0.05811821\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 259, loss = 0.05811809\n",
      "Iteration 260, loss = 0.05811805\n",
      "Iteration 261, loss = 0.05811802\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(a,b)=HFI.coefs_\\na2 = a.flatten()\\nc = []\\nfor i in range(len(a2)):\\n    if a2[i] >= 0:\\n        c.append(int(a2[i]+1/2))\\n    else:\\n        c.append(-int(np.abs(a2[i]-1/2)))\\n\\nc = np.array(c).reshape(a.shape)\\nHFI.coefs_[0] = c'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HFI = sklearn.neural_network.MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(5,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "'''for i, middle in enumerate(middle_HFI):\n",
    "    middle[middle==0] = 1\n",
    "    middle_HFI[i] = middle\n",
    "middle_HFI = np.log(np.abs(middle_HFI))'''\n",
    "HFI.fit(middle_HFI,y_train2)\n",
    "'''(a,b)=HFI.coefs_\n",
    "a2 = a.flatten()\n",
    "c = []\n",
    "for i in range(len(a2)):\n",
    "    if a2[i] >= 0:\n",
    "        c.append(int(a2[i]+1/2))\n",
    "    else:\n",
    "        c.append(-int(np.abs(a2[i]-1/2)))\n",
    "\n",
    "c = np.array(c).reshape(a.shape)\n",
    "HFI.coefs_[0] = c'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "master training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_per3(X,eps,delta):\n",
    "    return X - eps*np.sign(delta)\n",
    "def adv_per7(X,eps,delta):\n",
    "    return X + eps*np.sign(delta)\n",
    "delta = X0[y == 3].mean(axis=0) - X0[y == 7].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT_perturbed = np.zeros(XT.shape)\n",
    "theta = 0.5\n",
    "middle_SM = np.zeros((len(Ns),len(XT)))\n",
    "middle_HFI = np.zeros(shape=(len(XT),len(Ns),4))\n",
    "epsilon = np.arange(0,14,0.1)\n",
    "Thomas = np.zeros((len(Ns),len(epsilon)))\n",
    "score1,score2,score3 = [],[],[]\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    XT_perturbed = []\n",
    "    for i in range(len(pm)):\n",
    "        XT_perturbed.append(XT[i] - pm[i]*eps*grad[i])\n",
    "    XT_perturbed = np.asarray(XT_perturbed)\n",
    "    #XT_perturbed[yT == 4] = adv_per3(XT[yT == 4],eps,delta)\n",
    "    #XT_perturbed[yT == 9] = adv_per7(XT[yT == 9],eps,delta)\n",
    "    for idx_N,N in enumerate(Ns):\n",
    "        X = hill(XT_perturbed,N,theta)\n",
    "        middle_SM[idx_N]=small_neural.predict(X)\n",
    "        Thomas[idx_N][idx_eps]=small_neural.score(X,yT)\n",
    "        for i in range(len(X)):\n",
    "            h1=np.matmul(X[i],w1) + b1\n",
    "            h1=np.maximum(0, h1)\n",
    "            middle_HFI[i][idx_N]=h1\n",
    "    \n",
    "    middleT1=np.transpose(middle_SM)\n",
    "    middleT2=middle_HFI.reshape(len(X),4*len(Ns))\n",
    "    score1.append(master_neural.score(middleT1,yT))\n",
    "    score2.append(HFI.score(middleT2,yT))\n",
    "    score3.append(small_neural.score(XT_perturbed,yT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps=3\n",
    "XT_perturbed = []\n",
    "for i in range(len(pm)):\n",
    "    XT_perturbed.append(XT[i] - pm[i]*eps*grad[i])\n",
    "XT_perturbed = np.asarray(XT_perturbed)\n",
    "plt.imshow(XT_perturbed[yT==3][4].reshape(28,28),cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XT_perturbed = []\n",
    "for i in range(len(pm)):\n",
    "    XT_perturbed.append(XT[i] - pm[i]*eps*grad[i])\n",
    "XT_perturbed = np.asarray(XT_perturbed)\n",
    "plt.imshow(XT_perturbed[yT==7][7].reshape(28,28),cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(epsilon,score1)\n",
    "ax.plot(epsilon,Thomas[len(Thomas)-1])\n",
    "ax.plot(epsilon,score2)\n",
    "ax.plot(epsilon,score3)\n",
    "ax.set_title(r\"Comparison of Models; $\\theta$ = %.1f\"%theta)\n",
    "ax.set_xlabel(\"Epsilon\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "plt.legend(['Stacked Model','Highest Power','Hidden layer feed in','naive'], title = \"       Model\", loc=3)\n",
    "plt.savefig(\"rs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7\n",
      "4.2\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "final = True\n",
    "for i in range(len(score1)):\n",
    "    if (score1[i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False\n",
    "final = True\n",
    "for i in range(len(Thomas[len(Thomas)-1])):\n",
    "    if (Thomas[len(Thomas)-1][i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False\n",
    "final = True\n",
    "for i in range(len(score1)):\n",
    "    if (score2[i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X0_adv_test = np.zeros(X0_test.shape)\n",
    "theta = 0.5\n",
    "epsilon = np.arange(0,0.5,0.01)\n",
    "Ns = np.arange(11)\n",
    "score = np.zeros((len(Ns),len(epsilon)))\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    X0_adv_test[y_test == 3] = adv_per3(X0_test[y_test == 3],eps,delta)\n",
    "    X0_adv_test[y_test == 7] = adv_per7(X0_test[y_test == 7],eps,delta)\n",
    "    for idx_N,N in enumerate(Ns):\n",
    "        X1_adv_test = hill(X0_adv_test,N,theta)\n",
    "        score[idx_N,idx_eps]=mlp_orig.score(X1_adv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for line in score[range(2,12,2),:]:\n",
    "    ax.plot(epsilon,line)\n",
    "    ax.set_title(r\"Even powers; $\\theta$ = %.1f\"%theta)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.legend([2,4,6,8,10], title = \"       N\")\n",
    "    #plt.savefig(\"../Figures/Even-powers.png\")\n",
    "    \n",
    "fig,ax = plt.subplots()\n",
    "for line in score[range(1,11,2),:]:\n",
    "    ax.plot(epsilon,line)\n",
    "    ax.set_title(r\"Odd powers; $\\theta$ = %.1f\"%theta)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.legend(range(1,11,2), title = \"        N\")\n",
    "    #plt.savefig(\"../Figures/Odd-powers.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_naive = np.zeros((len(epsilon),1))\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    X0_adv_test[y_test == 3] = adv_per3(X0_test[y_test == 3],eps,delta)\n",
    "    X0_adv_test[y_test == 7] = adv_per7(X0_test[y_test == 7],eps,delta)\n",
    "    score_naive[idx_eps]=mlp_orig.score(X0_adv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in score_naive.T:\n",
    "    plt.plot(epsilon,line)\n",
    "    plt.title(\"Naive adversarial perturbation\")\n",
    "    plt.legend([\"No transformation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gradient manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(relu(np.dot(y,W[0])+b[0]),W[1])+b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(relu(np.dot(y,W[0])+b[0]),W[1])+b[1])\n",
    "def grad_score(x,use_hill,N,theta,mlp):\n",
    "    grad = np.zeros(x.shape)\n",
    "    for i in range(len(x)):\n",
    "        yp = x.copy(); yp[i] += 10/255; \n",
    "        ym = x.copy(); ym[i] -= 10/255;\n",
    "        grad[i] = 255/2*(score(yp,use_hill,N,theta,mlp)-score(ym,use_hill,N,theta,mlp))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_grad(x0,use_hill,N,theta,mlp,eps):\n",
    "    num_iter = 0; \n",
    "    x = x0.copy()\n",
    "    pred = mlp.predict(x0.reshape(1,-1))\n",
    "    new_pred = mlp.predict(x.reshape(1,-1))\n",
    "    pm = (pred-5)/2 #3 -> -1; 7 -> +1\n",
    "    while new_pred == pred:\n",
    "        grad = grad_score(x,use_hill,N,theta,mlp)\n",
    "        x -= pm*eps*grad[:]\n",
    "        new_pred = mlp.predict(x.reshape(1,-1))\n",
    "        print(score(x,use_hill,N,theta,mlp))\n",
    "        num_iter += 1\n",
    "    return x,num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hill=True;N=10;theta=0.5\n",
    "x0 = X0_test[y_test==3][1000]\n",
    "grad = grad_score(x0,True,N,theta,mlp_orig)\n",
    "plt.imshow(grad.reshape(28,28),cmap=plt.cm.gray)\n",
    "plt.axis('off');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = X0_test[y_test==7][1000]\n",
    "x1,num_iter = iter_grad(x0,True,10,theta,mlp_orig,5)\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(x0.reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[1].imshow(x1.reshape(28,28),cmap=plt.cm.gray)\n",
    "# ax[2].imshow(hill(x.reshape(28,28),N,theta),cmap=plt.cm.gray)\n",
    "\n",
    "[a.axis('off') for a in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [int(len(X0_test)*random.random()) for _ in range(10)]\n",
    "\n",
    "for i in sample:\n",
    "    x = X0_test[i]\n",
    "    grad_xF = grad_score(x,False,0,0,mlp_orig)\n",
    "    print(sum(grad_xF**2))\n",
    "    grad_xT = grad_score(x,True,10,0.5,mlp_orig)\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    ax[0].imshow(x.reshape(28,28),cmap=plt.cm.gray)\n",
    "    ax[1].imshow((grad_xF).reshape(28,28),cmap=plt.cm.gray)\n",
    "    ax[2].imshow((grad_xT).reshape(28,28),cmap=plt.cm.gray)\n",
    "    [a.axis('off') for a in ax]\n",
    "    plt.show()\n",
    "# print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score(x,False,0,0,mlp_orig))\n",
    "print(y_test[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 1360\n",
    "eps = 0.5\n",
    "use_hill = True\n",
    "N = 2\n",
    "theta = 0.8\n",
    "x = X0_test[y_test == 3][num].copy()\n",
    "pred = y_test[y_test == 3][num]\n",
    "# pred = score(x,use_hill,N,theta,mlp_orig)\n",
    "print(pred)\n",
    "num_iter = 0\n",
    "# while pred == y_test[num]:\n",
    "while pred < 0.98:\n",
    "    print(score(x,use_hill,N,theta,mlp_orig))\n",
    "    grad = grad_score(x,use_hill,N,theta,mlp_orig)\n",
    "    if y_test[num] == 3:\n",
    "        x += eps*grad[:]\n",
    "    elif y_test[num] == 7:\n",
    "        x -= eps*grad[:]\n",
    "#     pred = mlp_orig.predict(x.reshape(1,-1))\n",
    "    pred = score(x,use_hill,N,theta,mlp_orig)\n",
    "    num_iter += 1\n",
    "print(num_iter)\n",
    "fig,ax = plt.subplots(1,3)\n",
    "ax[0].imshow(X0_test[y_test == 3][num].reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[1].imshow(grad.reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[2].imshow(hill(x.reshape(28,28),N,theta),cmap=plt.cm.gray)\n",
    "[a.axis('off') for a in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(2,30,4):\n",
    "    y = hill(x.reshape(1,-1),i,theta)\n",
    "    print(i, mlp_orig.predict(y))\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(hill(X0_test[num].reshape(28,28),i,theta),cmap=plt.cm.gray)\n",
    "    ax[1].imshow(y.reshape(28,28),cmap=plt.cm.gray)\n",
    "    [a.axis('off') for a in ax]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [int(len(X0_test[y_test == 7])*random.random()) for _ in range(5)]\n",
    "print(sample)\n",
    "score7 = [print(score(X0_test[y_test == 7][i],False,0,0,mlp_orig)) for i in sample]\n",
    "for i in sample:\n",
    "    x = X0_test[y_test == 7][i]\n",
    "    plt.imshow(x.reshape(28,28),cmap=plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2; theta = 0.8\n",
    "sample = [1359, 189, 1314, 366, 1129, 1045]\n",
    "epsilon = np.arange(0.2,1.2,0.2)\n",
    "num_iter_table = [np.zeros((len(sample),len(epsilon)))]*2\n",
    "\n",
    "for i,x in enumerate(X0_test[y_test==7][sample]):\n",
    "    for j,eps in enumerate(epsilon):\n",
    "        num_iter_table[0][i,j] = iter_grad(x,7,False,N,theta,mlp_orig,eps)\n",
    "        num_iter_table[1][i,j] = iter_grad(x,7,True,N,theta,mlp_orig,eps)\n",
    "pprint(num_iter_table[0])\n",
    "pprint(num_iter_table[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
