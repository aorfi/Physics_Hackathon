{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce digits of power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "\n",
    "import sklearn.neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten\n",
    "(X, y), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255\n",
    "XT=x_test/255\n",
    "yT=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(60000, 28**2)\n",
    "XT=XT.reshape(10000, 28**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call MNIST and keep 3s and 7s\n",
    "#mnist = sklearn.datasets.fetch_mldata(\"MNIST original\")\n",
    "\n",
    "# Rescale the data and extract all images for two digits\n",
    "#X, y = mnist.data / 255., mnist.target\n",
    "\n",
    "index = np.where((y == 3) | (y == 7))[0]\n",
    "X0,y = X[index], y[index]\n",
    "\n",
    "Index = np.where((yT == 3) | (yT == 7))[0]\n",
    "XT,yT = XT[Index], yT[Index]\n",
    "\n",
    "X0_train1, X0_train2, y_train1, y_train2 = sklearn.model_selection.train_test_split(X0, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68372665\n",
      "Iteration 2, loss = 0.53419650\n",
      "Iteration 3, loss = 0.44971671\n",
      "Iteration 4, loss = 0.40623695\n",
      "Iteration 5, loss = 0.37799362\n",
      "Iteration 6, loss = 0.35693018\n",
      "Iteration 7, loss = 0.33972562\n",
      "Iteration 8, loss = 0.32518219\n",
      "Iteration 9, loss = 0.31230877\n",
      "Iteration 10, loss = 0.30076979\n",
      "Iteration 11, loss = 0.29031722\n",
      "Iteration 12, loss = 0.28071396\n",
      "Iteration 13, loss = 0.27189981\n",
      "Iteration 14, loss = 0.26366565\n",
      "Iteration 15, loss = 0.25606468\n",
      "Iteration 16, loss = 0.24888550\n",
      "Iteration 17, loss = 0.24222967\n",
      "Iteration 18, loss = 0.23593164\n",
      "Iteration 19, loss = 0.22999987\n",
      "Iteration 20, loss = 0.22438619\n",
      "Iteration 21, loss = 0.21905027\n",
      "Iteration 22, loss = 0.21401745\n",
      "Iteration 23, loss = 0.20923914\n",
      "Iteration 24, loss = 0.20469823\n",
      "Iteration 25, loss = 0.20035284\n",
      "Iteration 26, loss = 0.19624764\n",
      "Iteration 27, loss = 0.19230108\n",
      "Iteration 28, loss = 0.18853942\n",
      "Iteration 29, loss = 0.18494136\n",
      "Iteration 30, loss = 0.18151461\n",
      "Iteration 31, loss = 0.17821128\n",
      "Iteration 32, loss = 0.17503983\n",
      "Iteration 33, loss = 0.17202285\n",
      "Iteration 34, loss = 0.16910264\n",
      "Iteration 35, loss = 0.16635774\n",
      "Iteration 36, loss = 0.16366489\n",
      "Iteration 37, loss = 0.16106056\n",
      "Iteration 38, loss = 0.15857650\n",
      "Iteration 39, loss = 0.15618561\n",
      "Iteration 40, loss = 0.15388046\n",
      "Iteration 41, loss = 0.15167610\n",
      "Iteration 42, loss = 0.14953139\n",
      "Iteration 43, loss = 0.14744103\n",
      "Iteration 44, loss = 0.14545033\n",
      "Iteration 45, loss = 0.14351540\n",
      "Iteration 46, loss = 0.14163862\n",
      "Iteration 47, loss = 0.13981822\n",
      "Iteration 48, loss = 0.13809423\n",
      "Iteration 49, loss = 0.13636211\n",
      "Iteration 50, loss = 0.13470817\n",
      "Iteration 51, loss = 0.13310064\n",
      "Iteration 52, loss = 0.13152355\n",
      "Iteration 53, loss = 0.12999238\n",
      "Iteration 54, loss = 0.12852688\n",
      "Iteration 55, loss = 0.12708577\n",
      "Iteration 56, loss = 0.12567703\n",
      "Iteration 57, loss = 0.12433290\n",
      "Iteration 58, loss = 0.12300238\n",
      "Iteration 59, loss = 0.12172206\n",
      "Iteration 60, loss = 0.12046405\n",
      "Iteration 61, loss = 0.11928581\n",
      "Iteration 62, loss = 0.11806476\n",
      "Iteration 63, loss = 0.11692124\n",
      "Iteration 64, loss = 0.11580353\n",
      "Iteration 65, loss = 0.11468049\n",
      "Iteration 66, loss = 0.11359808\n",
      "Iteration 67, loss = 0.11255779\n",
      "Iteration 68, loss = 0.11157511\n",
      "Iteration 69, loss = 0.11054883\n",
      "Iteration 70, loss = 0.10959240\n",
      "Iteration 71, loss = 0.10866739\n",
      "Iteration 72, loss = 0.10770550\n",
      "Iteration 73, loss = 0.10676908\n",
      "Iteration 74, loss = 0.10588393\n",
      "Iteration 75, loss = 0.10499760\n",
      "Iteration 76, loss = 0.10414032\n",
      "Iteration 77, loss = 0.10330536\n",
      "Iteration 78, loss = 0.10248192\n",
      "Iteration 79, loss = 0.10169184\n",
      "Iteration 80, loss = 0.10089972\n",
      "Iteration 81, loss = 0.10011929\n",
      "Iteration 82, loss = 0.09934778\n",
      "Iteration 83, loss = 0.09862637\n",
      "Iteration 84, loss = 0.09789032\n",
      "Iteration 85, loss = 0.09717896\n",
      "Iteration 86, loss = 0.09646389\n",
      "Iteration 87, loss = 0.09577426\n",
      "Iteration 88, loss = 0.09512492\n",
      "Iteration 89, loss = 0.09443515\n",
      "Iteration 90, loss = 0.09381477\n",
      "Iteration 91, loss = 0.09316947\n",
      "Iteration 92, loss = 0.09251911\n",
      "Iteration 93, loss = 0.09192261\n",
      "Iteration 94, loss = 0.09130405\n",
      "Iteration 95, loss = 0.09070240\n",
      "Iteration 96, loss = 0.09011888\n",
      "Iteration 97, loss = 0.08954983\n",
      "Iteration 98, loss = 0.08896261\n",
      "Iteration 99, loss = 0.08841427\n",
      "Iteration 100, loss = 0.08785808\n",
      "Iteration 101, loss = 0.08731664\n",
      "Iteration 102, loss = 0.08678328\n",
      "Iteration 103, loss = 0.08626522\n",
      "Iteration 104, loss = 0.08574782\n",
      "Iteration 105, loss = 0.08523347\n",
      "Iteration 106, loss = 0.08476931\n",
      "Iteration 107, loss = 0.08427935\n",
      "Iteration 108, loss = 0.08378554\n",
      "Iteration 109, loss = 0.08329532\n",
      "Iteration 110, loss = 0.08282863\n",
      "Iteration 111, loss = 0.08238290\n",
      "Iteration 112, loss = 0.08192885\n",
      "Iteration 113, loss = 0.08147212\n",
      "Iteration 114, loss = 0.08102464\n",
      "Iteration 115, loss = 0.08059277\n",
      "Iteration 116, loss = 0.08016454\n",
      "Iteration 117, loss = 0.07976957\n",
      "Iteration 118, loss = 0.07931987\n",
      "Iteration 119, loss = 0.07892865\n",
      "Iteration 120, loss = 0.07851958\n",
      "Iteration 121, loss = 0.07812143\n",
      "Iteration 122, loss = 0.07773628\n",
      "Iteration 123, loss = 0.07733723\n",
      "Iteration 124, loss = 0.07695804\n",
      "Iteration 125, loss = 0.07658129\n",
      "Iteration 126, loss = 0.07622844\n",
      "Iteration 127, loss = 0.07584187\n",
      "Iteration 128, loss = 0.07548090\n",
      "Iteration 129, loss = 0.07513219\n",
      "Iteration 130, loss = 0.07476808\n",
      "Iteration 131, loss = 0.07442658\n",
      "Iteration 132, loss = 0.07407483\n",
      "Iteration 133, loss = 0.07372591\n",
      "Iteration 134, loss = 0.07342548\n",
      "Iteration 135, loss = 0.07309392\n",
      "Iteration 136, loss = 0.07273674\n",
      "Iteration 137, loss = 0.07240738\n",
      "Iteration 138, loss = 0.07209299\n",
      "Iteration 139, loss = 0.07177395\n",
      "Iteration 140, loss = 0.07146602\n",
      "Iteration 141, loss = 0.07118350\n",
      "Iteration 142, loss = 0.07086318\n",
      "Iteration 143, loss = 0.07055270\n",
      "Iteration 144, loss = 0.07027046\n",
      "Iteration 145, loss = 0.06996415\n",
      "Iteration 146, loss = 0.06967689\n",
      "Iteration 147, loss = 0.06937638\n",
      "Iteration 148, loss = 0.06908622\n",
      "Iteration 149, loss = 0.06880914\n",
      "Iteration 150, loss = 0.06854244\n",
      "Iteration 151, loss = 0.06825610\n",
      "Iteration 152, loss = 0.06799321\n",
      "Iteration 153, loss = 0.06775198\n",
      "Iteration 154, loss = 0.06745524\n",
      "Iteration 155, loss = 0.06718698\n",
      "Iteration 156, loss = 0.06694265\n",
      "Iteration 157, loss = 0.06667707\n",
      "Iteration 158, loss = 0.06643463\n",
      "Iteration 159, loss = 0.06617979\n",
      "Iteration 160, loss = 0.06590498\n",
      "Iteration 161, loss = 0.06566343\n",
      "Iteration 162, loss = 0.06542261\n",
      "Iteration 163, loss = 0.06517882\n",
      "Iteration 164, loss = 0.06493782\n",
      "Iteration 165, loss = 0.06469112\n",
      "Iteration 166, loss = 0.06445984\n",
      "Iteration 167, loss = 0.06423076\n",
      "Iteration 168, loss = 0.06400100\n",
      "Iteration 169, loss = 0.06378027\n",
      "Iteration 170, loss = 0.06356402\n",
      "Iteration 171, loss = 0.06333334\n",
      "Iteration 172, loss = 0.06311405\n",
      "Iteration 173, loss = 0.06288069\n",
      "Iteration 174, loss = 0.06268522\n",
      "Iteration 175, loss = 0.06243400\n",
      "Iteration 176, loss = 0.06222028\n",
      "Iteration 177, loss = 0.06200525\n",
      "Iteration 178, loss = 0.06180451\n",
      "Iteration 179, loss = 0.06157801\n",
      "Iteration 180, loss = 0.06138096\n",
      "Iteration 181, loss = 0.06116713\n",
      "Iteration 182, loss = 0.06096499\n",
      "Iteration 183, loss = 0.06079675\n",
      "Iteration 184, loss = 0.06057217\n",
      "Iteration 185, loss = 0.06036981\n",
      "Iteration 186, loss = 0.06016808\n",
      "Iteration 187, loss = 0.05998718\n",
      "Iteration 188, loss = 0.05976447\n",
      "Iteration 189, loss = 0.05959103\n",
      "Iteration 190, loss = 0.05937607\n",
      "Iteration 191, loss = 0.05918704\n",
      "Iteration 192, loss = 0.05899922\n",
      "Iteration 193, loss = 0.05880707\n",
      "Iteration 194, loss = 0.05862805\n",
      "Iteration 195, loss = 0.05842626\n",
      "Iteration 196, loss = 0.05824733\n",
      "Iteration 197, loss = 0.05806064\n",
      "Iteration 198, loss = 0.05787373\n",
      "Iteration 199, loss = 0.05769229\n",
      "Iteration 200, loss = 0.05752384\n",
      "Iteration 201, loss = 0.05735477\n",
      "Iteration 202, loss = 0.05714092\n",
      "Iteration 203, loss = 0.05698907\n",
      "Iteration 204, loss = 0.05680243\n",
      "Iteration 205, loss = 0.05662668\n",
      "Iteration 206, loss = 0.05646328\n",
      "Iteration 207, loss = 0.05630059\n",
      "Iteration 208, loss = 0.05611577\n",
      "Iteration 209, loss = 0.05595203\n",
      "Iteration 210, loss = 0.05581542\n",
      "Iteration 211, loss = 0.05563320\n",
      "Iteration 212, loss = 0.05545237\n",
      "Iteration 213, loss = 0.05529238\n",
      "Iteration 214, loss = 0.05512899\n",
      "Iteration 215, loss = 0.05497238\n",
      "Iteration 216, loss = 0.05480546\n",
      "Iteration 217, loss = 0.05464729\n",
      "Iteration 218, loss = 0.05447786\n",
      "Iteration 219, loss = 0.05433349\n",
      "Iteration 220, loss = 0.05420031\n",
      "Iteration 221, loss = 0.05402275\n",
      "Iteration 222, loss = 0.05386528\n",
      "Iteration 223, loss = 0.05371915\n",
      "Iteration 224, loss = 0.05356430\n",
      "Iteration 225, loss = 0.05341674\n",
      "Iteration 226, loss = 0.05326189\n",
      "Iteration 227, loss = 0.05312528\n",
      "Iteration 228, loss = 0.05295928\n",
      "Iteration 229, loss = 0.05281613\n",
      "Iteration 230, loss = 0.05267435\n",
      "Iteration 231, loss = 0.05252063\n",
      "Iteration 232, loss = 0.05239048\n",
      "Iteration 233, loss = 0.05223213\n",
      "Iteration 234, loss = 0.05207712\n",
      "Iteration 235, loss = 0.05194295\n",
      "Iteration 236, loss = 0.05181566\n",
      "Iteration 237, loss = 0.05167244\n",
      "Iteration 238, loss = 0.05151978\n",
      "Iteration 239, loss = 0.05137610\n",
      "Iteration 240, loss = 0.05124738\n",
      "Iteration 241, loss = 0.05110520\n",
      "Iteration 242, loss = 0.05097636\n",
      "Iteration 243, loss = 0.05085194\n",
      "Iteration 244, loss = 0.05069585\n",
      "Iteration 245, loss = 0.05057615\n",
      "Iteration 246, loss = 0.05044743\n",
      "Iteration 247, loss = 0.05031223\n",
      "Iteration 248, loss = 0.05017815\n",
      "Iteration 249, loss = 0.05008740\n",
      "Iteration 250, loss = 0.04992542\n",
      "Iteration 251, loss = 0.04979815\n",
      "Iteration 252, loss = 0.04966920\n",
      "Iteration 253, loss = 0.04954255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.04942268\n",
      "Iteration 255, loss = 0.04929619\n",
      "Iteration 256, loss = 0.04916690\n",
      "Iteration 257, loss = 0.04903840\n",
      "Iteration 258, loss = 0.04891781\n",
      "Iteration 259, loss = 0.04879069\n",
      "Iteration 260, loss = 0.04867284\n",
      "Iteration 261, loss = 0.04858847\n",
      "Iteration 262, loss = 0.04841987\n",
      "Iteration 263, loss = 0.04830306\n",
      "Iteration 264, loss = 0.04817299\n",
      "Iteration 265, loss = 0.04805704\n",
      "Iteration 266, loss = 0.04795346\n",
      "Iteration 267, loss = 0.04779520\n",
      "Iteration 268, loss = 0.04773450\n",
      "Iteration 269, loss = 0.04759187\n",
      "Iteration 270, loss = 0.04744168\n",
      "Iteration 271, loss = 0.04734428\n",
      "Iteration 272, loss = 0.04722771\n",
      "Iteration 273, loss = 0.04709962\n",
      "Iteration 274, loss = 0.04700799\n",
      "Iteration 275, loss = 0.04689512\n",
      "Iteration 276, loss = 0.04675601\n",
      "Iteration 277, loss = 0.04669735\n",
      "Iteration 278, loss = 0.04651842\n",
      "Iteration 279, loss = 0.04642021\n",
      "Iteration 280, loss = 0.04631836\n",
      "Iteration 281, loss = 0.04619587\n",
      "Iteration 282, loss = 0.04609253\n",
      "Iteration 283, loss = 0.04600752\n",
      "Iteration 284, loss = 0.04586330\n",
      "Iteration 285, loss = 0.04576961\n",
      "Iteration 286, loss = 0.04564100\n",
      "Iteration 287, loss = 0.04555986\n",
      "Iteration 288, loss = 0.04543635\n",
      "Iteration 289, loss = 0.04534579\n",
      "Iteration 290, loss = 0.04523781\n",
      "Iteration 291, loss = 0.04513192\n",
      "Iteration 292, loss = 0.04502479\n",
      "Iteration 293, loss = 0.04490512\n",
      "Iteration 294, loss = 0.04480869\n",
      "Iteration 295, loss = 0.04470465\n",
      "Iteration 296, loss = 0.04460580\n",
      "Iteration 297, loss = 0.04449253\n",
      "Iteration 298, loss = 0.04438458\n",
      "Iteration 299, loss = 0.04428343\n",
      "Iteration 300, loss = 0.04418750\n",
      "Iteration 301, loss = 0.04410762\n",
      "Iteration 302, loss = 0.04398576\n",
      "Iteration 303, loss = 0.04388236\n",
      "Iteration 304, loss = 0.04376135\n",
      "Iteration 305, loss = 0.04366748\n",
      "Iteration 306, loss = 0.04359532\n",
      "Iteration 307, loss = 0.04346536\n",
      "Iteration 308, loss = 0.04336337\n",
      "Iteration 309, loss = 0.04327624\n",
      "Iteration 310, loss = 0.04316517\n",
      "Iteration 311, loss = 0.04307665\n",
      "Iteration 312, loss = 0.04297867\n",
      "Iteration 313, loss = 0.04287982\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 314, loss = 0.04277410\n",
      "Iteration 315, loss = 0.04274160\n",
      "Iteration 316, loss = 0.04272169\n",
      "Iteration 317, loss = 0.04270250\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 318, loss = 0.04268203\n",
      "Iteration 319, loss = 0.04267708\n",
      "Iteration 320, loss = 0.04267300\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 321, loss = 0.04266915\n",
      "Iteration 322, loss = 0.04266799\n",
      "Iteration 323, loss = 0.04266724\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 324, loss = 0.04266642\n",
      "Iteration 325, loss = 0.04266622\n",
      "Iteration 326, loss = 0.04266608\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 327, loss = 0.04266590\n",
      "Iteration 328, loss = 0.04266587\n",
      "Iteration 329, loss = 0.04266584\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(4,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_neural = sklearn.neural_network.MLPClassifier(activation = \"relu\", hidden_layer_sizes=(4,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "small_neural.fit(X0_train1,y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.5\n",
    "middle_SM = np.zeros((len(Ns),len(X0_train2)))\n",
    "for idx_N,N in enumerate(Ns):\n",
    "    X = hill(X0_train2,N,theta)\n",
    "    middle_SM[idx_N]=small_neural.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6198, 10)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_SM=np.transpose(middle_SM)\n",
    "middle_SM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76236697\n",
      "Iteration 2, loss = 0.73688053\n",
      "Iteration 3, loss = 0.71781512\n",
      "Iteration 4, loss = 0.70376095\n",
      "Iteration 5, loss = 0.69351605\n",
      "Iteration 6, loss = 0.68611558\n",
      "Iteration 7, loss = 0.68075701\n",
      "Iteration 8, loss = 0.67666502\n",
      "Iteration 9, loss = 0.67349981\n",
      "Iteration 10, loss = 0.67084354\n",
      "Iteration 11, loss = 0.66839649\n",
      "Iteration 12, loss = 0.66603729\n",
      "Iteration 13, loss = 0.66361950\n",
      "Iteration 14, loss = 0.66098497\n",
      "Iteration 15, loss = 0.65797062\n",
      "Iteration 16, loss = 0.65445167\n",
      "Iteration 17, loss = 0.65037224\n",
      "Iteration 18, loss = 0.64608348\n",
      "Iteration 19, loss = 0.64235723\n",
      "Iteration 20, loss = 0.63954250\n",
      "Iteration 21, loss = 0.63721605\n",
      "Iteration 22, loss = 0.63503237\n",
      "Iteration 23, loss = 0.63282094\n",
      "Iteration 24, loss = 0.63055189\n",
      "Iteration 25, loss = 0.62828217\n",
      "Iteration 26, loss = 0.62595169\n",
      "Iteration 27, loss = 0.62358118\n",
      "Iteration 28, loss = 0.62117105\n",
      "Iteration 29, loss = 0.61870695\n",
      "Iteration 30, loss = 0.61624784\n",
      "Iteration 31, loss = 0.61375941\n",
      "Iteration 32, loss = 0.61130739\n",
      "Iteration 33, loss = 0.60882314\n",
      "Iteration 34, loss = 0.60637174\n",
      "Iteration 35, loss = 0.60392771\n",
      "Iteration 36, loss = 0.60152630\n",
      "Iteration 37, loss = 0.59908215\n",
      "Iteration 38, loss = 0.59663468\n",
      "Iteration 39, loss = 0.59423961\n",
      "Iteration 40, loss = 0.59174997\n",
      "Iteration 41, loss = 0.58930380\n",
      "Iteration 42, loss = 0.58682775\n",
      "Iteration 43, loss = 0.58434714\n",
      "Iteration 44, loss = 0.58187875\n",
      "Iteration 45, loss = 0.57939646\n",
      "Iteration 46, loss = 0.57686685\n",
      "Iteration 47, loss = 0.57435620\n",
      "Iteration 48, loss = 0.57182874\n",
      "Iteration 49, loss = 0.56927754\n",
      "Iteration 50, loss = 0.56672325\n",
      "Iteration 51, loss = 0.56417706\n",
      "Iteration 52, loss = 0.56162677\n",
      "Iteration 53, loss = 0.55898579\n",
      "Iteration 54, loss = 0.55640362\n",
      "Iteration 55, loss = 0.55377304\n",
      "Iteration 56, loss = 0.55119650\n",
      "Iteration 57, loss = 0.54852777\n",
      "Iteration 58, loss = 0.54588554\n",
      "Iteration 59, loss = 0.54322569\n",
      "Iteration 60, loss = 0.54052635\n",
      "Iteration 61, loss = 0.53783612\n",
      "Iteration 62, loss = 0.53514452\n",
      "Iteration 63, loss = 0.53239410\n",
      "Iteration 64, loss = 0.52967606\n",
      "Iteration 65, loss = 0.52695882\n",
      "Iteration 66, loss = 0.52414584\n",
      "Iteration 67, loss = 0.52142393\n",
      "Iteration 68, loss = 0.51857691\n",
      "Iteration 69, loss = 0.51582439\n",
      "Iteration 70, loss = 0.51299933\n",
      "Iteration 71, loss = 0.51012522\n",
      "Iteration 72, loss = 0.50732462\n",
      "Iteration 73, loss = 0.50446156\n",
      "Iteration 74, loss = 0.50161935\n",
      "Iteration 75, loss = 0.49877733\n",
      "Iteration 76, loss = 0.49585091\n",
      "Iteration 77, loss = 0.49292966\n",
      "Iteration 78, loss = 0.49001576\n",
      "Iteration 79, loss = 0.48714857\n",
      "Iteration 80, loss = 0.48414052\n",
      "Iteration 81, loss = 0.48120816\n",
      "Iteration 82, loss = 0.47824107\n",
      "Iteration 83, loss = 0.47529864\n",
      "Iteration 84, loss = 0.47231048\n",
      "Iteration 85, loss = 0.46934264\n",
      "Iteration 86, loss = 0.46635608\n",
      "Iteration 87, loss = 0.46336634\n",
      "Iteration 88, loss = 0.46030840\n",
      "Iteration 89, loss = 0.45730891\n",
      "Iteration 90, loss = 0.45427231\n",
      "Iteration 91, loss = 0.45122788\n",
      "Iteration 92, loss = 0.44818057\n",
      "Iteration 93, loss = 0.44514162\n",
      "Iteration 94, loss = 0.44207838\n",
      "Iteration 95, loss = 0.43901140\n",
      "Iteration 96, loss = 0.43593993\n",
      "Iteration 97, loss = 0.43295379\n",
      "Iteration 98, loss = 0.42978137\n",
      "Iteration 99, loss = 0.42676886\n",
      "Iteration 100, loss = 0.42368484\n",
      "Iteration 101, loss = 0.42056098\n",
      "Iteration 102, loss = 0.41751068\n",
      "Iteration 103, loss = 0.41437398\n",
      "Iteration 104, loss = 0.41134462\n",
      "Iteration 105, loss = 0.40824858\n",
      "Iteration 106, loss = 0.40524796\n",
      "Iteration 107, loss = 0.40208089\n",
      "Iteration 108, loss = 0.39896568\n",
      "Iteration 109, loss = 0.39587612\n",
      "Iteration 110, loss = 0.39280934\n",
      "Iteration 111, loss = 0.38976887\n",
      "Iteration 112, loss = 0.38669178\n",
      "Iteration 113, loss = 0.38367531\n",
      "Iteration 114, loss = 0.38060415\n",
      "Iteration 115, loss = 0.37755627\n",
      "Iteration 116, loss = 0.37451723\n",
      "Iteration 117, loss = 0.37147033\n",
      "Iteration 118, loss = 0.36847395\n",
      "Iteration 119, loss = 0.36548693\n",
      "Iteration 120, loss = 0.36245317\n",
      "Iteration 121, loss = 0.35947982\n",
      "Iteration 122, loss = 0.35654541\n",
      "Iteration 123, loss = 0.35348191\n",
      "Iteration 124, loss = 0.35063821\n",
      "Iteration 125, loss = 0.34767349\n",
      "Iteration 126, loss = 0.34475148\n",
      "Iteration 127, loss = 0.34177218\n",
      "Iteration 128, loss = 0.33890850\n",
      "Iteration 129, loss = 0.33606574\n",
      "Iteration 130, loss = 0.33317162\n",
      "Iteration 131, loss = 0.33041126\n",
      "Iteration 132, loss = 0.32738283\n",
      "Iteration 133, loss = 0.32475013\n",
      "Iteration 134, loss = 0.32189924\n",
      "Iteration 135, loss = 0.31905255\n",
      "Iteration 136, loss = 0.31625435\n",
      "Iteration 137, loss = 0.31354875\n",
      "Iteration 138, loss = 0.31079559\n",
      "Iteration 139, loss = 0.30806202\n",
      "Iteration 140, loss = 0.30554368\n",
      "Iteration 141, loss = 0.30275030\n",
      "Iteration 142, loss = 0.30012417\n",
      "Iteration 143, loss = 0.29752676\n",
      "Iteration 144, loss = 0.29488440\n",
      "Iteration 145, loss = 0.29229326\n",
      "Iteration 146, loss = 0.28983404\n",
      "Iteration 147, loss = 0.28722486\n",
      "Iteration 148, loss = 0.28470109\n",
      "Iteration 149, loss = 0.28216890\n",
      "Iteration 150, loss = 0.27981055\n",
      "Iteration 151, loss = 0.27731246\n",
      "Iteration 152, loss = 0.27487072\n",
      "Iteration 153, loss = 0.27255024\n",
      "Iteration 154, loss = 0.27012422\n",
      "Iteration 155, loss = 0.26770495\n",
      "Iteration 156, loss = 0.26550626\n",
      "Iteration 157, loss = 0.26311845\n",
      "Iteration 158, loss = 0.26083445\n",
      "Iteration 159, loss = 0.25859635\n",
      "Iteration 160, loss = 0.25638993\n",
      "Iteration 161, loss = 0.25418167\n",
      "Iteration 162, loss = 0.25197326\n",
      "Iteration 163, loss = 0.24983562\n",
      "Iteration 164, loss = 0.24767764\n",
      "Iteration 165, loss = 0.24554667\n",
      "Iteration 166, loss = 0.24348952\n",
      "Iteration 167, loss = 0.24141015\n",
      "Iteration 168, loss = 0.23935574\n",
      "Iteration 169, loss = 0.23737101\n",
      "Iteration 170, loss = 0.23538268\n",
      "Iteration 171, loss = 0.23341925\n",
      "Iteration 172, loss = 0.23143850\n",
      "Iteration 173, loss = 0.22959221\n",
      "Iteration 174, loss = 0.22769675\n",
      "Iteration 175, loss = 0.22572629\n",
      "Iteration 176, loss = 0.22390034\n",
      "Iteration 177, loss = 0.22206438\n",
      "Iteration 178, loss = 0.22028759\n",
      "Iteration 179, loss = 0.21848805\n",
      "Iteration 180, loss = 0.21670915\n",
      "Iteration 181, loss = 0.21498997\n",
      "Iteration 182, loss = 0.21326662\n",
      "Iteration 183, loss = 0.21156315\n",
      "Iteration 184, loss = 0.20989413\n",
      "Iteration 185, loss = 0.20828134\n",
      "Iteration 186, loss = 0.20668923\n",
      "Iteration 187, loss = 0.20507470\n",
      "Iteration 188, loss = 0.20349524\n",
      "Iteration 189, loss = 0.20188669\n",
      "Iteration 190, loss = 0.20037028\n",
      "Iteration 191, loss = 0.19886246\n",
      "Iteration 192, loss = 0.19728776\n",
      "Iteration 193, loss = 0.19588458\n",
      "Iteration 194, loss = 0.19434484\n",
      "Iteration 195, loss = 0.19294862\n",
      "Iteration 196, loss = 0.19148572\n",
      "Iteration 197, loss = 0.19009244\n",
      "Iteration 198, loss = 0.18871153\n",
      "Iteration 199, loss = 0.18737906\n",
      "Iteration 200, loss = 0.18596770\n",
      "Iteration 201, loss = 0.18463295\n",
      "Iteration 202, loss = 0.18334134\n",
      "Iteration 203, loss = 0.18208732\n",
      "Iteration 204, loss = 0.18077484\n",
      "Iteration 205, loss = 0.17952816\n",
      "Iteration 206, loss = 0.17829374\n",
      "Iteration 207, loss = 0.17708623\n",
      "Iteration 208, loss = 0.17588566\n",
      "Iteration 209, loss = 0.17470791\n",
      "Iteration 210, loss = 0.17351051\n",
      "Iteration 211, loss = 0.17232702\n",
      "Iteration 212, loss = 0.17119778\n",
      "Iteration 213, loss = 0.17008213\n",
      "Iteration 214, loss = 0.16897428\n",
      "Iteration 215, loss = 0.16784899\n",
      "Iteration 216, loss = 0.16680312\n",
      "Iteration 217, loss = 0.16579166\n",
      "Iteration 218, loss = 0.16465983\n",
      "Iteration 219, loss = 0.16364061\n",
      "Iteration 220, loss = 0.16260864\n",
      "Iteration 221, loss = 0.16160720\n",
      "Iteration 222, loss = 0.16060697\n",
      "Iteration 223, loss = 0.15967149\n",
      "Iteration 224, loss = 0.15871510\n",
      "Iteration 225, loss = 0.15781217\n",
      "Iteration 226, loss = 0.15679950\n",
      "Iteration 227, loss = 0.15586839\n",
      "Iteration 228, loss = 0.15500450\n",
      "Iteration 229, loss = 0.15406323\n",
      "Iteration 230, loss = 0.15319143\n",
      "Iteration 231, loss = 0.15234869\n",
      "Iteration 232, loss = 0.15142946\n",
      "Iteration 233, loss = 0.15059787\n",
      "Iteration 234, loss = 0.14976075\n",
      "Iteration 235, loss = 0.14896187\n",
      "Iteration 236, loss = 0.14815153\n",
      "Iteration 237, loss = 0.14727068\n",
      "Iteration 238, loss = 0.14649703\n",
      "Iteration 239, loss = 0.14576122\n",
      "Iteration 240, loss = 0.14496696\n",
      "Iteration 241, loss = 0.14419858\n",
      "Iteration 242, loss = 0.14345072\n",
      "Iteration 243, loss = 0.14272632\n",
      "Iteration 244, loss = 0.14199126\n",
      "Iteration 245, loss = 0.14123582\n",
      "Iteration 246, loss = 0.14051786\n",
      "Iteration 247, loss = 0.13982792\n",
      "Iteration 248, loss = 0.13923295\n",
      "Iteration 249, loss = 0.13845454\n",
      "Iteration 250, loss = 0.13778619\n",
      "Iteration 251, loss = 0.13712733\n",
      "Iteration 252, loss = 0.13648226\n",
      "Iteration 253, loss = 0.13582777\n",
      "Iteration 254, loss = 0.13518707\n",
      "Iteration 255, loss = 0.13455270\n",
      "Iteration 256, loss = 0.13393985\n",
      "Iteration 257, loss = 0.13338497\n",
      "Iteration 258, loss = 0.13273958\n",
      "Iteration 259, loss = 0.13222035\n",
      "Iteration 260, loss = 0.13158175\n",
      "Iteration 261, loss = 0.13095848\n",
      "Iteration 262, loss = 0.13037320\n",
      "Iteration 263, loss = 0.12981565\n",
      "Iteration 264, loss = 0.12927893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 265, loss = 0.12872797\n",
      "Iteration 266, loss = 0.12817763\n",
      "Iteration 267, loss = 0.12765660\n",
      "Iteration 268, loss = 0.12711941\n",
      "Iteration 269, loss = 0.12656323\n",
      "Iteration 270, loss = 0.12610142\n",
      "Iteration 271, loss = 0.12559985\n",
      "Iteration 272, loss = 0.12503339\n",
      "Iteration 273, loss = 0.12457817\n",
      "Iteration 274, loss = 0.12408123\n",
      "Iteration 275, loss = 0.12358680\n",
      "Iteration 276, loss = 0.12311494\n",
      "Iteration 277, loss = 0.12262968\n",
      "Iteration 278, loss = 0.12222839\n",
      "Iteration 279, loss = 0.12170591\n",
      "Iteration 280, loss = 0.12126002\n",
      "Iteration 281, loss = 0.12079790\n",
      "Iteration 282, loss = 0.12046157\n",
      "Iteration 283, loss = 0.11996477\n",
      "Iteration 284, loss = 0.11952179\n",
      "Iteration 285, loss = 0.11910143\n",
      "Iteration 286, loss = 0.11866953\n",
      "Iteration 287, loss = 0.11827150\n",
      "Iteration 288, loss = 0.11783529\n",
      "Iteration 289, loss = 0.11742124\n",
      "Iteration 290, loss = 0.11704072\n",
      "Iteration 291, loss = 0.11664266\n",
      "Iteration 292, loss = 0.11626435\n",
      "Iteration 293, loss = 0.11588820\n",
      "Iteration 294, loss = 0.11549603\n",
      "Iteration 295, loss = 0.11511760\n",
      "Iteration 296, loss = 0.11475049\n",
      "Iteration 297, loss = 0.11437865\n",
      "Iteration 298, loss = 0.11400392\n",
      "Iteration 299, loss = 0.11367399\n",
      "Iteration 300, loss = 0.11331832\n",
      "Iteration 301, loss = 0.11294676\n",
      "Iteration 302, loss = 0.11262951\n",
      "Iteration 303, loss = 0.11230010\n",
      "Iteration 304, loss = 0.11197763\n",
      "Iteration 305, loss = 0.11160420\n",
      "Iteration 306, loss = 0.11126834\n",
      "Iteration 307, loss = 0.11093645\n",
      "Iteration 308, loss = 0.11061175\n",
      "Iteration 309, loss = 0.11034222\n",
      "Iteration 310, loss = 0.11003894\n",
      "Iteration 311, loss = 0.10969264\n",
      "Iteration 312, loss = 0.10940976\n",
      "Iteration 313, loss = 0.10907713\n",
      "Iteration 314, loss = 0.10881139\n",
      "Iteration 315, loss = 0.10851834\n",
      "Iteration 316, loss = 0.10817545\n",
      "Iteration 317, loss = 0.10790408\n",
      "Iteration 318, loss = 0.10760512\n",
      "Iteration 319, loss = 0.10740547\n",
      "Iteration 320, loss = 0.10715393\n",
      "Iteration 321, loss = 0.10679763\n",
      "Iteration 322, loss = 0.10650357\n",
      "Iteration 323, loss = 0.10625234\n",
      "Iteration 324, loss = 0.10597114\n",
      "Iteration 325, loss = 0.10570140\n",
      "Iteration 326, loss = 0.10548245\n",
      "Iteration 327, loss = 0.10517964\n",
      "Iteration 328, loss = 0.10495424\n",
      "Iteration 329, loss = 0.10469157\n",
      "Iteration 330, loss = 0.10449377\n",
      "Iteration 331, loss = 0.10419761\n",
      "Iteration 332, loss = 0.10394314\n",
      "Iteration 333, loss = 0.10368674\n",
      "Iteration 334, loss = 0.10350505\n",
      "Iteration 335, loss = 0.10322472\n",
      "Iteration 336, loss = 0.10303315\n",
      "Iteration 337, loss = 0.10283735\n",
      "Iteration 338, loss = 0.10253043\n",
      "Iteration 339, loss = 0.10233827\n",
      "Iteration 340, loss = 0.10212272\n",
      "Iteration 341, loss = 0.10188625\n",
      "Iteration 342, loss = 0.10164893\n",
      "Iteration 343, loss = 0.10146490\n",
      "Iteration 344, loss = 0.10124723\n",
      "Iteration 345, loss = 0.10104859\n",
      "Iteration 346, loss = 0.10082273\n",
      "Iteration 347, loss = 0.10060745\n",
      "Iteration 348, loss = 0.10040393\n",
      "Iteration 349, loss = 0.10020223\n",
      "Iteration 350, loss = 0.10001576\n",
      "Iteration 351, loss = 0.09978578\n",
      "Iteration 352, loss = 0.09963168\n",
      "Iteration 353, loss = 0.09940667\n",
      "Iteration 354, loss = 0.09922465\n",
      "Iteration 355, loss = 0.09903420\n",
      "Iteration 356, loss = 0.09893140\n",
      "Iteration 357, loss = 0.09871297\n",
      "Iteration 358, loss = 0.09848266\n",
      "Iteration 359, loss = 0.09830917\n",
      "Iteration 360, loss = 0.09814879\n",
      "Iteration 361, loss = 0.09801019\n",
      "Iteration 362, loss = 0.09775264\n",
      "Iteration 363, loss = 0.09758073\n",
      "Iteration 364, loss = 0.09740184\n",
      "Iteration 365, loss = 0.09723790\n",
      "Iteration 366, loss = 0.09710842\n",
      "Iteration 367, loss = 0.09692515\n",
      "Iteration 368, loss = 0.09674049\n",
      "Iteration 369, loss = 0.09658176\n",
      "Iteration 370, loss = 0.09642257\n",
      "Iteration 371, loss = 0.09626704\n",
      "Iteration 372, loss = 0.09611011\n",
      "Iteration 373, loss = 0.09595029\n",
      "Iteration 374, loss = 0.09580733\n",
      "Iteration 375, loss = 0.09568859\n",
      "Iteration 376, loss = 0.09555217\n",
      "Iteration 377, loss = 0.09535921\n",
      "Iteration 378, loss = 0.09524891\n",
      "Iteration 379, loss = 0.09508596\n",
      "Iteration 380, loss = 0.09492327\n",
      "Iteration 381, loss = 0.09475131\n",
      "Iteration 382, loss = 0.09462079\n",
      "Iteration 383, loss = 0.09448212\n",
      "Iteration 384, loss = 0.09434801\n",
      "Iteration 385, loss = 0.09421122\n",
      "Iteration 386, loss = 0.09412963\n",
      "Iteration 387, loss = 0.09391898\n",
      "Iteration 388, loss = 0.09376682\n",
      "Iteration 389, loss = 0.09368266\n",
      "Iteration 390, loss = 0.09351469\n",
      "Iteration 391, loss = 0.09339895\n",
      "Iteration 392, loss = 0.09330292\n",
      "Iteration 393, loss = 0.09310870\n",
      "Iteration 394, loss = 0.09303876\n",
      "Iteration 395, loss = 0.09286929\n",
      "Iteration 396, loss = 0.09273264\n",
      "Iteration 397, loss = 0.09264037\n",
      "Iteration 398, loss = 0.09247807\n",
      "Iteration 399, loss = 0.09236739\n",
      "Iteration 400, loss = 0.09225459\n",
      "Iteration 401, loss = 0.09213640\n",
      "Iteration 402, loss = 0.09204666\n",
      "Iteration 403, loss = 0.09188427\n",
      "Iteration 404, loss = 0.09180941\n",
      "Iteration 405, loss = 0.09171667\n",
      "Iteration 406, loss = 0.09162064\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 407, loss = 0.09144210\n",
      "Iteration 408, loss = 0.09141364\n",
      "Iteration 409, loss = 0.09140635\n",
      "Iteration 410, loss = 0.09136883\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 411, loss = 0.09134580\n",
      "Iteration 412, loss = 0.09134022\n",
      "Iteration 413, loss = 0.09133531\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 414, loss = 0.09133093\n",
      "Iteration 415, loss = 0.09132982\n",
      "Iteration 416, loss = 0.09132896\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 417, loss = 0.09132773\n",
      "Iteration 418, loss = 0.09132782\n",
      "Iteration 419, loss = 0.09132750\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 420, loss = 0.09132721\n",
      "Iteration 421, loss = 0.09132719\n",
      "Iteration 422, loss = 0.09132715\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_neural = sklearn.neural_network.MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(5,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "master_neural.fit(middle_SM,y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden-layer feed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights and Biases for Hidden Layer\n",
    "w1 = small_neural.coefs_[0]\n",
    "b1 = small_neural.intercepts_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6198, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "theta = 0.5\n",
    "middle_HFI = np.zeros(shape=(len(X0_train2),len(Ns),4))\n",
    "for idx_N,N in enumerate(Ns):\n",
    "    X = hill(X0_train2,N,theta)\n",
    "    for i in range(len(X0_train2)):\n",
    "        h1=np.matmul(X0_train2[i],w1) + b1\n",
    "        h1=np.maximum(0, h1)\n",
    "        middle_HFI[i][idx_N]=h1\n",
    "print(middle_HFI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_HFI=middle_HFI.reshape(6198,4*len(Ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71372312\n",
      "Iteration 2, loss = 0.67318656\n",
      "Iteration 3, loss = 0.63844979\n",
      "Iteration 4, loss = 0.61526176\n",
      "Iteration 5, loss = 0.59724408\n",
      "Iteration 6, loss = 0.58170473\n",
      "Iteration 7, loss = 0.56760580\n",
      "Iteration 8, loss = 0.55429293\n",
      "Iteration 9, loss = 0.54156462\n",
      "Iteration 10, loss = 0.52931306\n",
      "Iteration 11, loss = 0.51735706\n",
      "Iteration 12, loss = 0.50567837\n",
      "Iteration 13, loss = 0.49425487\n",
      "Iteration 14, loss = 0.48303673\n",
      "Iteration 15, loss = 0.47205122\n",
      "Iteration 16, loss = 0.46126911\n",
      "Iteration 17, loss = 0.45067585\n",
      "Iteration 18, loss = 0.44032014\n",
      "Iteration 19, loss = 0.43015233\n",
      "Iteration 20, loss = 0.42022211\n",
      "Iteration 21, loss = 0.41051224\n",
      "Iteration 22, loss = 0.40101083\n",
      "Iteration 23, loss = 0.39174301\n",
      "Iteration 24, loss = 0.38268655\n",
      "Iteration 25, loss = 0.37385611\n",
      "Iteration 26, loss = 0.36524989\n",
      "Iteration 27, loss = 0.35685687\n",
      "Iteration 28, loss = 0.34869386\n",
      "Iteration 29, loss = 0.34073305\n",
      "Iteration 30, loss = 0.33299763\n",
      "Iteration 31, loss = 0.32547446\n",
      "Iteration 32, loss = 0.31815688\n",
      "Iteration 33, loss = 0.31105110\n",
      "Iteration 34, loss = 0.30413812\n",
      "Iteration 35, loss = 0.29744432\n",
      "Iteration 36, loss = 0.29092980\n",
      "Iteration 37, loss = 0.28460664\n",
      "Iteration 38, loss = 0.27848666\n",
      "Iteration 39, loss = 0.27253913\n",
      "Iteration 40, loss = 0.26676615\n",
      "Iteration 41, loss = 0.26118403\n",
      "Iteration 42, loss = 0.25576506\n",
      "Iteration 43, loss = 0.25051945\n",
      "Iteration 44, loss = 0.24541751\n",
      "Iteration 45, loss = 0.24049295\n",
      "Iteration 46, loss = 0.23572337\n",
      "Iteration 47, loss = 0.23109056\n",
      "Iteration 48, loss = 0.22660679\n",
      "Iteration 49, loss = 0.22227509\n",
      "Iteration 50, loss = 0.21806656\n",
      "Iteration 51, loss = 0.21399234\n",
      "Iteration 52, loss = 0.21004869\n",
      "Iteration 53, loss = 0.20622780\n",
      "Iteration 54, loss = 0.20253147\n",
      "Iteration 55, loss = 0.19894772\n",
      "Iteration 56, loss = 0.19547206\n",
      "Iteration 57, loss = 0.19211618\n",
      "Iteration 58, loss = 0.18885316\n",
      "Iteration 59, loss = 0.18569758\n",
      "Iteration 60, loss = 0.18263703\n",
      "Iteration 61, loss = 0.17967648\n",
      "Iteration 62, loss = 0.17680402\n",
      "Iteration 63, loss = 0.17401886\n",
      "Iteration 64, loss = 0.17131678\n",
      "Iteration 65, loss = 0.16870153\n",
      "Iteration 66, loss = 0.16616696\n",
      "Iteration 67, loss = 0.16369993\n",
      "Iteration 68, loss = 0.16131073\n",
      "Iteration 69, loss = 0.15899460\n",
      "Iteration 70, loss = 0.15675009\n",
      "Iteration 71, loss = 0.15456759\n",
      "Iteration 72, loss = 0.15244453\n",
      "Iteration 73, loss = 0.15039296\n",
      "Iteration 74, loss = 0.14839474\n",
      "Iteration 75, loss = 0.14645276\n",
      "Iteration 76, loss = 0.14456984\n",
      "Iteration 77, loss = 0.14274341\n",
      "Iteration 78, loss = 0.14096301\n",
      "Iteration 79, loss = 0.13923184\n",
      "Iteration 80, loss = 0.13755344\n",
      "Iteration 81, loss = 0.13591721\n",
      "Iteration 82, loss = 0.13433121\n",
      "Iteration 83, loss = 0.13278213\n",
      "Iteration 84, loss = 0.13128014\n",
      "Iteration 85, loss = 0.12981407\n",
      "Iteration 86, loss = 0.12839148\n",
      "Iteration 87, loss = 0.12700467\n",
      "Iteration 88, loss = 0.12565307\n",
      "Iteration 89, loss = 0.12433804\n",
      "Iteration 90, loss = 0.12305988\n",
      "Iteration 91, loss = 0.12181044\n",
      "Iteration 92, loss = 0.12059651\n",
      "Iteration 93, loss = 0.11940959\n",
      "Iteration 94, loss = 0.11825137\n",
      "Iteration 95, loss = 0.11712967\n",
      "Iteration 96, loss = 0.11602753\n",
      "Iteration 97, loss = 0.11495869\n",
      "Iteration 98, loss = 0.11391665\n",
      "Iteration 99, loss = 0.11289119\n",
      "Iteration 100, loss = 0.11190103\n",
      "Iteration 101, loss = 0.11092781\n",
      "Iteration 102, loss = 0.10997750\n",
      "Iteration 103, loss = 0.10905280\n",
      "Iteration 104, loss = 0.10815090\n",
      "Iteration 105, loss = 0.10726589\n",
      "Iteration 106, loss = 0.10640454\n",
      "Iteration 107, loss = 0.10556079\n",
      "Iteration 108, loss = 0.10473631\n",
      "Iteration 109, loss = 0.10393142\n",
      "Iteration 110, loss = 0.10314486\n",
      "Iteration 111, loss = 0.10237554\n",
      "Iteration 112, loss = 0.10162324\n",
      "Iteration 113, loss = 0.10088601\n",
      "Iteration 114, loss = 0.10016488\n",
      "Iteration 115, loss = 0.09946369\n",
      "Iteration 116, loss = 0.09877270\n",
      "Iteration 117, loss = 0.09810087\n",
      "Iteration 118, loss = 0.09743691\n",
      "Iteration 119, loss = 0.09679055\n",
      "Iteration 120, loss = 0.09615892\n",
      "Iteration 121, loss = 0.09554231\n",
      "Iteration 122, loss = 0.09492975\n",
      "Iteration 123, loss = 0.09433884\n",
      "Iteration 124, loss = 0.09375644\n",
      "Iteration 125, loss = 0.09318822\n",
      "Iteration 126, loss = 0.09262207\n",
      "Iteration 127, loss = 0.09207709\n",
      "Iteration 128, loss = 0.09153787\n",
      "Iteration 129, loss = 0.09101344\n",
      "Iteration 130, loss = 0.09049814\n",
      "Iteration 131, loss = 0.08998951\n",
      "Iteration 132, loss = 0.08949484\n",
      "Iteration 133, loss = 0.08900662\n",
      "Iteration 134, loss = 0.08852777\n",
      "Iteration 135, loss = 0.08805899\n",
      "Iteration 136, loss = 0.08759857\n",
      "Iteration 137, loss = 0.08714614\n",
      "Iteration 138, loss = 0.08670634\n",
      "Iteration 139, loss = 0.08627056\n",
      "Iteration 140, loss = 0.08584361\n",
      "Iteration 141, loss = 0.08542535\n",
      "Iteration 142, loss = 0.08501089\n",
      "Iteration 143, loss = 0.08460550\n",
      "Iteration 144, loss = 0.08421404\n",
      "Iteration 145, loss = 0.08381935\n",
      "Iteration 146, loss = 0.08343787\n",
      "Iteration 147, loss = 0.08305957\n",
      "Iteration 148, loss = 0.08269063\n",
      "Iteration 149, loss = 0.08232618\n",
      "Iteration 150, loss = 0.08197036\n",
      "Iteration 151, loss = 0.08162131\n",
      "Iteration 152, loss = 0.08127336\n",
      "Iteration 153, loss = 0.08093517\n",
      "Iteration 154, loss = 0.08060179\n",
      "Iteration 155, loss = 0.08027405\n",
      "Iteration 156, loss = 0.07995132\n",
      "Iteration 157, loss = 0.07963468\n",
      "Iteration 158, loss = 0.07932299\n",
      "Iteration 159, loss = 0.07901563\n",
      "Iteration 160, loss = 0.07871337\n",
      "Iteration 161, loss = 0.07841975\n",
      "Iteration 162, loss = 0.07812433\n",
      "Iteration 163, loss = 0.07783818\n",
      "Iteration 164, loss = 0.07755685\n",
      "Iteration 165, loss = 0.07727565\n",
      "Iteration 166, loss = 0.07700118\n",
      "Iteration 167, loss = 0.07673075\n",
      "Iteration 168, loss = 0.07646464\n",
      "Iteration 169, loss = 0.07620607\n",
      "Iteration 170, loss = 0.07595297\n",
      "Iteration 171, loss = 0.07569286\n",
      "Iteration 172, loss = 0.07544514\n",
      "Iteration 173, loss = 0.07520007\n",
      "Iteration 174, loss = 0.07495580\n",
      "Iteration 175, loss = 0.07472043\n",
      "Iteration 176, loss = 0.07448521\n",
      "Iteration 177, loss = 0.07425109\n",
      "Iteration 178, loss = 0.07402434\n",
      "Iteration 179, loss = 0.07379901\n",
      "Iteration 180, loss = 0.07358024\n",
      "Iteration 181, loss = 0.07336108\n",
      "Iteration 182, loss = 0.07314377\n",
      "Iteration 183, loss = 0.07293508\n",
      "Iteration 184, loss = 0.07272534\n",
      "Iteration 185, loss = 0.07252004\n",
      "Iteration 186, loss = 0.07231679\n",
      "Iteration 187, loss = 0.07211647\n",
      "Iteration 188, loss = 0.07191798\n",
      "Iteration 189, loss = 0.07172560\n",
      "Iteration 190, loss = 0.07153389\n",
      "Iteration 191, loss = 0.07134492\n",
      "Iteration 192, loss = 0.07115685\n",
      "Iteration 193, loss = 0.07097522\n",
      "Iteration 194, loss = 0.07079381\n",
      "Iteration 195, loss = 0.07061294\n",
      "Iteration 196, loss = 0.07043657\n",
      "Iteration 197, loss = 0.07026238\n",
      "Iteration 198, loss = 0.07009156\n",
      "Iteration 199, loss = 0.06992197\n",
      "Iteration 200, loss = 0.06975672\n",
      "Iteration 201, loss = 0.06958935\n",
      "Iteration 202, loss = 0.06942739\n",
      "Iteration 203, loss = 0.06926671\n",
      "Iteration 204, loss = 0.06910856\n",
      "Iteration 205, loss = 0.06895243\n",
      "Iteration 206, loss = 0.06879794\n",
      "Iteration 207, loss = 0.06864521\n",
      "Iteration 208, loss = 0.06849746\n",
      "Iteration 209, loss = 0.06834577\n",
      "Iteration 210, loss = 0.06819946\n",
      "Iteration 211, loss = 0.06805550\n",
      "Iteration 212, loss = 0.06791228\n",
      "Iteration 213, loss = 0.06777029\n",
      "Iteration 214, loss = 0.06763046\n",
      "Iteration 215, loss = 0.06749433\n",
      "Iteration 216, loss = 0.06736173\n",
      "Iteration 217, loss = 0.06722416\n",
      "Iteration 218, loss = 0.06709278\n",
      "Iteration 219, loss = 0.06696287\n",
      "Iteration 220, loss = 0.06683031\n",
      "Iteration 221, loss = 0.06670280\n",
      "Iteration 222, loss = 0.06657596\n",
      "Iteration 223, loss = 0.06645198\n",
      "Iteration 224, loss = 0.06633034\n",
      "Iteration 225, loss = 0.06620756\n",
      "Iteration 226, loss = 0.06608828\n",
      "Iteration 227, loss = 0.06596762\n",
      "Iteration 228, loss = 0.06585079\n",
      "Iteration 229, loss = 0.06573659\n",
      "Iteration 230, loss = 0.06561961\n",
      "Iteration 231, loss = 0.06550686\n",
      "Iteration 232, loss = 0.06539741\n",
      "Iteration 233, loss = 0.06528841\n",
      "Iteration 234, loss = 0.06517764\n",
      "Iteration 235, loss = 0.06506820\n",
      "Iteration 236, loss = 0.06496196\n",
      "Iteration 237, loss = 0.06485611\n",
      "Iteration 238, loss = 0.06475272\n",
      "Iteration 239, loss = 0.06465050\n",
      "Iteration 240, loss = 0.06454600\n",
      "Iteration 241, loss = 0.06444505\n",
      "Iteration 242, loss = 0.06434392\n",
      "Iteration 243, loss = 0.06424735\n",
      "Iteration 244, loss = 0.06414863\n",
      "Iteration 245, loss = 0.06405284\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 246, loss = 0.06397763\n",
      "Iteration 247, loss = 0.06395326\n",
      "Iteration 248, loss = 0.06393444\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 249, loss = 0.06391973\n",
      "Iteration 250, loss = 0.06391448\n",
      "Iteration 251, loss = 0.06391069\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 252, loss = 0.06390769\n",
      "Iteration 253, loss = 0.06390679\n",
      "Iteration 254, loss = 0.06390600\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 255, loss = 0.06390541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.06390521\n",
      "Iteration 257, loss = 0.06390506\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 258, loss = 0.06390494\n",
      "Iteration 259, loss = 0.06390490\n",
      "Iteration 260, loss = 0.06390487\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HFI = sklearn.neural_network.MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(5,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "HFI.fit(middle_HFI,y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "master training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_per3(X,eps,delta):\n",
    "    return X - eps*np.sign(delta)\n",
    "def adv_per7(X,eps,delta):\n",
    "    return X + eps*np.sign(delta)\n",
    "delta = X0[y == 3].mean(axis=0) - X0[y == 7].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT_perturbed = np.zeros(XT.shape)\n",
    "theta = 0.5\n",
    "middle_SM = np.zeros((len(Ns),len(XT)))\n",
    "middle_HFI = np.zeros(shape=(len(XT),len(Ns),4))\n",
    "epsilon = np.arange(0,0.5,0.01)\n",
    "Thomas = np.zeros((len(Ns),len(epsilon)))\n",
    "score1,score2 = [],[]\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    XT_perturbed[yT == 3] = adv_per3(XT[yT == 3],eps,delta)\n",
    "    XT_perturbed[yT == 7] = adv_per7(XT[yT == 7],eps,delta)\n",
    "    for idx_N,N in enumerate(Ns):\n",
    "        X = hill(XT_perturbed,N,theta)\n",
    "        middle_SM[idx_N]=small_neural.predict(X)\n",
    "        Thomas[idx_N][idx_eps]=small_neural.score(X,yT)\n",
    "        #middle_HFI[idx_N]=small_neural.predict(X)\n",
    "        for i in range(len(X)):\n",
    "            h1=np.matmul(X[i],w1) + b1\n",
    "            h1=np.maximum(0, h1)\n",
    "            middle_HFI[i][idx_N]=h1\n",
    "    \n",
    "    middleT1=np.transpose(middle_SM)\n",
    "    middleT2=middle_HFI.reshape(len(X),4*len(Ns))\n",
    "    score1.append(master_neural.score(middleT1,yT))\n",
    "    score2.append(HFI.score(middleT2,yT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2038) (2038, 40)\n",
      "(2038,)\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2038"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(middle_SM.shape, middleT2.shape )\n",
    "print(yT.shape)\n",
    "print(Ns)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x4d80258748>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(epsilon,score1)\n",
    "ax.plot(epsilon,Thomas[len(Thomas)-1])\n",
    "ax.plot(epsilon,score2)\n",
    "ax.set_title(r\"Comparison of Models; $\\theta$ = %.1f\"%theta)\n",
    "ax.set_xlabel(\"Epsilon\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "plt.legend(['SM','Thomas','HFI'], title = \"       N\")\n",
    "#plt.savefig(\"../Figures/Even-powers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33\n",
      "0.41000000000000003\n",
      "0.23\n"
     ]
    }
   ],
   "source": [
    "final = True\n",
    "for i in range(len(score1)):\n",
    "    if (score1[i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False\n",
    "final = True\n",
    "for i in range(len(Thomas[len(Thomas)-1])):\n",
    "    if (Thomas[len(Thomas)-1][i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False\n",
    "final = True\n",
    "for i in range(len(score1)):\n",
    "    if (score2[i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X0_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-f7dc6025fb0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX0_adv_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX0_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mNs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X0_test' is not defined"
     ]
    }
   ],
   "source": [
    "X0_adv_test = np.zeros(X0_test.shape)\n",
    "theta = 0.5\n",
    "epsilon = np.arange(0,0.5,0.01)\n",
    "Ns = np.arange(11)\n",
    "score = np.zeros((len(Ns),len(epsilon)))\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    X0_adv_test[y_test == 3] = adv_per3(X0_test[y_test == 3],eps,delta)\n",
    "    X0_adv_test[y_test == 7] = adv_per7(X0_test[y_test == 7],eps,delta)\n",
    "    for idx_N,N in enumerate(Ns):\n",
    "        X1_adv_test = hill(X0_adv_test,N,theta)\n",
    "        score[idx_N,idx_eps]=mlp_orig.score(X1_adv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for line in score[range(2,12,2),:]:\n",
    "    ax.plot(epsilon,line)\n",
    "    ax.set_title(r\"Even powers; $\\theta$ = %.1f\"%theta)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.legend([2,4,6,8,10], title = \"       N\")\n",
    "    #plt.savefig(\"../Figures/Even-powers.png\")\n",
    "    \n",
    "fig,ax = plt.subplots()\n",
    "for line in score[range(1,11,2),:]:\n",
    "    ax.plot(epsilon,line)\n",
    "    ax.set_title(r\"Odd powers; $\\theta$ = %.1f\"%theta)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.legend(range(1,11,2), title = \"        N\")\n",
    "    #plt.savefig(\"../Figures/Odd-powers.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_naive = np.zeros((len(epsilon),1))\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    X0_adv_test[y_test == 3] = adv_per3(X0_test[y_test == 3],eps,delta)\n",
    "    X0_adv_test[y_test == 7] = adv_per7(X0_test[y_test == 7],eps,delta)\n",
    "    score_naive[idx_eps]=mlp_orig.score(X0_adv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in score_naive.T:\n",
    "    plt.plot(epsilon,line)\n",
    "    plt.title(\"Naive adversarial perturbation\")\n",
    "    plt.legend([\"No transformation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gradient manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(relu(np.dot(y,W[0])+b[0]),W[1])+b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_score(x,use_hill,N,theta,mlp):\n",
    "    grad = np.zeros(x.shape)\n",
    "    for i in range(len(x)):\n",
    "        yp = x.copy(); yp[i] += 10/255; \n",
    "        ym = x.copy(); ym[i] -= 10/255;\n",
    "        grad[i] = 255/2*(score(yp,use_hill,N,theta,mlp)-score(ym,use_hill,N,theta,mlp))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_grad(x0,use_hill,N,theta,mlp,eps):\n",
    "    num_iter = 0; \n",
    "    x = x0.copy()\n",
    "    pred = mlp.predict(x0.reshape(1,-1))\n",
    "    new_pred = mlp.predict(x.reshape(1,-1))\n",
    "    pm = (pred-5)/2 #3 -> -1; 7 -> +1\n",
    "    while new_pred == pred:\n",
    "        grad = grad_score(x,use_hill,N,theta,mlp)\n",
    "        x -= pm*eps*grad[:]\n",
    "        new_pred = mlp.predict(x.reshape(1,-1))\n",
    "        print(score(x,use_hill,N,theta,mlp))\n",
    "        num_iter += 1\n",
    "    return x,num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hill=True;N=10;theta=0.5\n",
    "x0 = X0_test[y_test==3][1000]\n",
    "grad = grad_score(x0,True,N,theta,mlp_orig)\n",
    "plt.imshow(grad.reshape(28,28),cmap=plt.cm.gray)\n",
    "plt.axis('off');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = X0_test[y_test==7][1000]\n",
    "x1,num_iter = iter_grad(x0,True,10,theta,mlp_orig,5)\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(x0.reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[1].imshow(x1.reshape(28,28),cmap=plt.cm.gray)\n",
    "# ax[2].imshow(hill(x.reshape(28,28),N,theta),cmap=plt.cm.gray)\n",
    "\n",
    "[a.axis('off') for a in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [int(len(X0_test)*random.random()) for _ in range(10)]\n",
    "\n",
    "for i in sample:\n",
    "    x = X0_test[i]\n",
    "    grad_xF = grad_score(x,False,0,0,mlp_orig)\n",
    "    print(sum(grad_xF**2))\n",
    "    grad_xT = grad_score(x,True,10,0.5,mlp_orig)\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    ax[0].imshow(x.reshape(28,28),cmap=plt.cm.gray)\n",
    "    ax[1].imshow((grad_xF).reshape(28,28),cmap=plt.cm.gray)\n",
    "    ax[2].imshow((grad_xT).reshape(28,28),cmap=plt.cm.gray)\n",
    "    [a.axis('off') for a in ax]\n",
    "    plt.show()\n",
    "# print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score(x,False,0,0,mlp_orig))\n",
    "print(y_test[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 1360\n",
    "eps = 0.5\n",
    "use_hill = True\n",
    "N = 2\n",
    "theta = 0.8\n",
    "x = X0_test[y_test == 3][num].copy()\n",
    "pred = y_test[y_test == 3][num]\n",
    "# pred = score(x,use_hill,N,theta,mlp_orig)\n",
    "print(pred)\n",
    "num_iter = 0\n",
    "# while pred == y_test[num]:\n",
    "while pred < 0.98:\n",
    "    print(score(x,use_hill,N,theta,mlp_orig))\n",
    "    grad = grad_score(x,use_hill,N,theta,mlp_orig)\n",
    "    if y_test[num] == 3:\n",
    "        x += eps*grad[:]\n",
    "    elif y_test[num] == 7:\n",
    "        x -= eps*grad[:]\n",
    "#     pred = mlp_orig.predict(x.reshape(1,-1))\n",
    "    pred = score(x,use_hill,N,theta,mlp_orig)\n",
    "    num_iter += 1\n",
    "print(num_iter)\n",
    "fig,ax = plt.subplots(1,3)\n",
    "ax[0].imshow(X0_test[y_test == 3][num].reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[1].imshow(grad.reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[2].imshow(hill(x.reshape(28,28),N,theta),cmap=plt.cm.gray)\n",
    "[a.axis('off') for a in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(2,30,4):\n",
    "    y = hill(x.reshape(1,-1),i,theta)\n",
    "    print(i, mlp_orig.predict(y))\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(hill(X0_test[num].reshape(28,28),i,theta),cmap=plt.cm.gray)\n",
    "    ax[1].imshow(y.reshape(28,28),cmap=plt.cm.gray)\n",
    "    [a.axis('off') for a in ax]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [int(len(X0_test[y_test == 7])*random.random()) for _ in range(5)]\n",
    "print(sample)\n",
    "score7 = [print(score(X0_test[y_test == 7][i],False,0,0,mlp_orig)) for i in sample]\n",
    "for i in sample:\n",
    "    x = X0_test[y_test == 7][i]\n",
    "    plt.imshow(x.reshape(28,28),cmap=plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2; theta = 0.8\n",
    "sample = [1359, 189, 1314, 366, 1129, 1045]\n",
    "epsilon = np.arange(0.2,1.2,0.2)\n",
    "num_iter_table = [np.zeros((len(sample),len(epsilon)))]*2\n",
    "\n",
    "for i,x in enumerate(X0_test[y_test==7][sample]):\n",
    "    for j,eps in enumerate(epsilon):\n",
    "        num_iter_table[0][i,j] = iter_grad(x,7,False,N,theta,mlp_orig,eps)\n",
    "        num_iter_table[1][i,j] = iter_grad(x,7,True,N,theta,mlp_orig,eps)\n",
    "pprint(num_iter_table[0])\n",
    "pprint(num_iter_table[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
