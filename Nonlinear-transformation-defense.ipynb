{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce digits of power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import sklearn.neural_network\n",
    "\n",
    "import neural_network_repn\n",
    "setattr(sklearn,'neural_network',neural_network_repn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten\n",
    "(X, y), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [2,4,6,8,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(sigmoid(np.dot(y,W[0])+b[0]),W[1])+b[1])\n",
    "def grad_score(x,use_hill,N,theta,mlp):\n",
    "    grad = np.zeros(x.shape)\n",
    "    for i in range(len(x)):\n",
    "        yp = x.copy(); yp[i] += 10/255; \n",
    "        ym = x.copy(); ym[i] -= 10/255;\n",
    "        grad[i] = 255/2*(score(yp,use_hill,N,theta,mlp)-score(ym,use_hill,N,theta,mlp))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255\n",
    "XT=x_test/255\n",
    "yT=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(60000, 28**2)\n",
    "XT=XT.reshape(10000, 28**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call MNIST and keep 3s and 7s\n",
    "#mnist = sklearn.datasets.fetch_mldata(\"MNIST original\")\n",
    "\n",
    "# Rescale the data and extract all images for two digits\n",
    "#X, y = mnist.data / 255., mnist.target\n",
    "\n",
    "index = np.where((y == 3) | (y == 7))[0]\n",
    "X0,y = X[index], y[index]\n",
    "\n",
    "Index = np.where((yT == 3) | (yT == 7))[0]\n",
    "XT,yT = XT[Index], yT[Index]\n",
    "\n",
    "X0_train1, X0_train2, y_train1, y_train2 = sklearn.model_selection.train_test_split(X0, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68655119\n",
      "Iteration 2, loss = 0.53746540\n",
      "Iteration 3, loss = 0.45140105\n",
      "Iteration 4, loss = 0.40738373\n",
      "Iteration 5, loss = 0.37916360\n",
      "Iteration 6, loss = 0.35811587\n",
      "Iteration 7, loss = 0.34114340\n",
      "Iteration 8, loss = 0.32654891\n",
      "Iteration 9, loss = 0.31376486\n",
      "Iteration 10, loss = 0.30229209\n",
      "Iteration 11, loss = 0.29183019\n",
      "Iteration 12, loss = 0.28229237\n",
      "Iteration 13, loss = 0.27345527\n",
      "Iteration 14, loss = 0.26528930\n",
      "Iteration 15, loss = 0.25765758\n",
      "Iteration 16, loss = 0.25055468\n",
      "Iteration 17, loss = 0.24389198\n",
      "Iteration 18, loss = 0.23757583\n",
      "Iteration 19, loss = 0.23168831\n",
      "Iteration 20, loss = 0.22608253\n",
      "Iteration 21, loss = 0.22075900\n",
      "Iteration 22, loss = 0.21576501\n",
      "Iteration 23, loss = 0.21096234\n",
      "Iteration 24, loss = 0.20642466\n",
      "Iteration 25, loss = 0.20211295\n",
      "Iteration 26, loss = 0.19802020\n",
      "Iteration 27, loss = 0.19406246\n",
      "Iteration 28, loss = 0.19030553\n",
      "Iteration 29, loss = 0.18671268\n",
      "Iteration 30, loss = 0.18330604\n",
      "Iteration 31, loss = 0.18000282\n",
      "Iteration 32, loss = 0.17684488\n",
      "Iteration 33, loss = 0.17384084\n",
      "Iteration 34, loss = 0.17091603\n",
      "Iteration 35, loss = 0.16814527\n",
      "Iteration 36, loss = 0.16545164\n",
      "Iteration 37, loss = 0.16287564\n",
      "Iteration 38, loss = 0.16041860\n",
      "Iteration 39, loss = 0.15801982\n",
      "Iteration 40, loss = 0.15573196\n",
      "Iteration 41, loss = 0.15352292\n",
      "Iteration 42, loss = 0.15135934\n",
      "Iteration 43, loss = 0.14931298\n",
      "Iteration 44, loss = 0.14733198\n",
      "Iteration 45, loss = 0.14539134\n",
      "Iteration 46, loss = 0.14352407\n",
      "Iteration 47, loss = 0.14172225\n",
      "Iteration 48, loss = 0.13999484\n",
      "Iteration 49, loss = 0.13831033\n",
      "Iteration 50, loss = 0.13664640\n",
      "Iteration 51, loss = 0.13513493\n",
      "Iteration 52, loss = 0.13354501\n",
      "Iteration 53, loss = 0.13206024\n",
      "Iteration 54, loss = 0.13056301\n",
      "Iteration 55, loss = 0.12915661\n",
      "Iteration 56, loss = 0.12777174\n",
      "Iteration 57, loss = 0.12644101\n",
      "Iteration 58, loss = 0.12516081\n",
      "Iteration 59, loss = 0.12391978\n",
      "Iteration 60, loss = 0.12265155\n",
      "Iteration 61, loss = 0.12146848\n",
      "Iteration 62, loss = 0.12029745\n",
      "Iteration 63, loss = 0.11919858\n",
      "Iteration 64, loss = 0.11809583\n",
      "Iteration 65, loss = 0.11698639\n",
      "Iteration 66, loss = 0.11594424\n",
      "Iteration 67, loss = 0.11491558\n",
      "Iteration 68, loss = 0.11390944\n",
      "Iteration 69, loss = 0.11293348\n",
      "Iteration 70, loss = 0.11196994\n",
      "Iteration 71, loss = 0.11105174\n",
      "Iteration 72, loss = 0.11012113\n",
      "Iteration 73, loss = 0.10923963\n",
      "Iteration 74, loss = 0.10837018\n",
      "Iteration 75, loss = 0.10749517\n",
      "Iteration 76, loss = 0.10667832\n",
      "Iteration 77, loss = 0.10584766\n",
      "Iteration 78, loss = 0.10504551\n",
      "Iteration 79, loss = 0.10425805\n",
      "Iteration 80, loss = 0.10352962\n",
      "Iteration 81, loss = 0.10273660\n",
      "Iteration 82, loss = 0.10200140\n",
      "Iteration 83, loss = 0.10126552\n",
      "Iteration 84, loss = 0.10056610\n",
      "Iteration 85, loss = 0.09986751\n",
      "Iteration 86, loss = 0.09918348\n",
      "Iteration 87, loss = 0.09851439\n",
      "Iteration 88, loss = 0.09787487\n",
      "Iteration 89, loss = 0.09721909\n",
      "Iteration 90, loss = 0.09659655\n",
      "Iteration 91, loss = 0.09598153\n",
      "Iteration 92, loss = 0.09538141\n",
      "Iteration 93, loss = 0.09477868\n",
      "Iteration 94, loss = 0.09415511\n",
      "Iteration 95, loss = 0.09357447\n",
      "Iteration 96, loss = 0.09299981\n",
      "Iteration 97, loss = 0.09243359\n",
      "Iteration 98, loss = 0.09188206\n",
      "Iteration 99, loss = 0.09134623\n",
      "Iteration 100, loss = 0.09082316\n",
      "Iteration 101, loss = 0.09031657\n",
      "Iteration 102, loss = 0.08976763\n",
      "Iteration 103, loss = 0.08925982\n",
      "Iteration 104, loss = 0.08875604\n",
      "Iteration 105, loss = 0.08829257\n",
      "Iteration 106, loss = 0.08778446\n",
      "Iteration 107, loss = 0.08732133\n",
      "Iteration 108, loss = 0.08682215\n",
      "Iteration 109, loss = 0.08639372\n",
      "Iteration 110, loss = 0.08587696\n",
      "Iteration 111, loss = 0.08542112\n",
      "Iteration 112, loss = 0.08497207\n",
      "Iteration 113, loss = 0.08454673\n",
      "Iteration 114, loss = 0.08410557\n",
      "Iteration 115, loss = 0.08368312\n",
      "Iteration 116, loss = 0.08323805\n",
      "Iteration 117, loss = 0.08283942\n",
      "Iteration 118, loss = 0.08240915\n",
      "Iteration 119, loss = 0.08198499\n",
      "Iteration 120, loss = 0.08159310\n",
      "Iteration 121, loss = 0.08118468\n",
      "Iteration 122, loss = 0.08079773\n",
      "Iteration 123, loss = 0.08040881\n",
      "Iteration 124, loss = 0.08001165\n",
      "Iteration 125, loss = 0.07963340\n",
      "Iteration 126, loss = 0.07925954\n",
      "Iteration 127, loss = 0.07889382\n",
      "Iteration 128, loss = 0.07851904\n",
      "Iteration 129, loss = 0.07819127\n",
      "Iteration 130, loss = 0.07781559\n",
      "Iteration 131, loss = 0.07746559\n",
      "Iteration 132, loss = 0.07711091\n",
      "Iteration 133, loss = 0.07676703\n",
      "Iteration 134, loss = 0.07645928\n",
      "Iteration 135, loss = 0.07610519\n",
      "Iteration 136, loss = 0.07576703\n",
      "Iteration 137, loss = 0.07542640\n",
      "Iteration 138, loss = 0.07510673\n",
      "Iteration 139, loss = 0.07478799\n",
      "Iteration 140, loss = 0.07451165\n",
      "Iteration 141, loss = 0.07417255\n",
      "Iteration 142, loss = 0.07387492\n",
      "Iteration 143, loss = 0.07356770\n",
      "Iteration 144, loss = 0.07326853\n",
      "Iteration 145, loss = 0.07294602\n",
      "Iteration 146, loss = 0.07269199\n",
      "Iteration 147, loss = 0.07237465\n",
      "Iteration 148, loss = 0.07207184\n",
      "Iteration 149, loss = 0.07180791\n",
      "Iteration 150, loss = 0.07149702\n",
      "Iteration 151, loss = 0.07121360\n",
      "Iteration 152, loss = 0.07094318\n",
      "Iteration 153, loss = 0.07066715\n",
      "Iteration 154, loss = 0.07041222\n",
      "Iteration 155, loss = 0.07016964\n",
      "Iteration 156, loss = 0.06987417\n",
      "Iteration 157, loss = 0.06960558\n",
      "Iteration 158, loss = 0.06936366\n",
      "Iteration 159, loss = 0.06907786\n",
      "Iteration 160, loss = 0.06883961\n",
      "Iteration 161, loss = 0.06859032\n",
      "Iteration 162, loss = 0.06832679\n",
      "Iteration 163, loss = 0.06808395\n",
      "Iteration 164, loss = 0.06782718\n",
      "Iteration 165, loss = 0.06759190\n",
      "Iteration 166, loss = 0.06733186\n",
      "Iteration 167, loss = 0.06709015\n",
      "Iteration 168, loss = 0.06684069\n",
      "Iteration 169, loss = 0.06658616\n",
      "Iteration 170, loss = 0.06636147\n",
      "Iteration 171, loss = 0.06616489\n",
      "Iteration 172, loss = 0.06587314\n",
      "Iteration 173, loss = 0.06564355\n",
      "Iteration 174, loss = 0.06541414\n",
      "Iteration 175, loss = 0.06519282\n",
      "Iteration 176, loss = 0.06494414\n",
      "Iteration 177, loss = 0.06471663\n",
      "Iteration 178, loss = 0.06450176\n",
      "Iteration 179, loss = 0.06424534\n",
      "Iteration 180, loss = 0.06403797\n",
      "Iteration 181, loss = 0.06380121\n",
      "Iteration 182, loss = 0.06357258\n",
      "Iteration 183, loss = 0.06337587\n",
      "Iteration 184, loss = 0.06314687\n",
      "Iteration 185, loss = 0.06291112\n",
      "Iteration 186, loss = 0.06270325\n",
      "Iteration 187, loss = 0.06247655\n",
      "Iteration 188, loss = 0.06228654\n",
      "Iteration 189, loss = 0.06204094\n",
      "Iteration 190, loss = 0.06183719\n",
      "Iteration 191, loss = 0.06162511\n",
      "Iteration 192, loss = 0.06142231\n",
      "Iteration 193, loss = 0.06122093\n",
      "Iteration 194, loss = 0.06105976\n",
      "Iteration 195, loss = 0.06080889\n",
      "Iteration 196, loss = 0.06065664\n",
      "Iteration 197, loss = 0.06039947\n",
      "Iteration 198, loss = 0.06019905\n",
      "Iteration 199, loss = 0.05998166\n",
      "Iteration 200, loss = 0.05979014\n",
      "Iteration 201, loss = 0.05959108\n",
      "Iteration 202, loss = 0.05938867\n",
      "Iteration 203, loss = 0.05921763\n",
      "Iteration 204, loss = 0.05900764\n",
      "Iteration 205, loss = 0.05882792\n",
      "Iteration 206, loss = 0.05863711\n",
      "Iteration 207, loss = 0.05843982\n",
      "Iteration 208, loss = 0.05824533\n",
      "Iteration 209, loss = 0.05805902\n",
      "Iteration 210, loss = 0.05787721\n",
      "Iteration 211, loss = 0.05769395\n",
      "Iteration 212, loss = 0.05751174\n",
      "Iteration 213, loss = 0.05733794\n",
      "Iteration 214, loss = 0.05717042\n",
      "Iteration 215, loss = 0.05697639\n",
      "Iteration 216, loss = 0.05680450\n",
      "Iteration 217, loss = 0.05663276\n",
      "Iteration 218, loss = 0.05646587\n",
      "Iteration 219, loss = 0.05627671\n",
      "Iteration 220, loss = 0.05610165\n",
      "Iteration 221, loss = 0.05593878\n",
      "Iteration 222, loss = 0.05577207\n",
      "Iteration 223, loss = 0.05560136\n",
      "Iteration 224, loss = 0.05542694\n",
      "Iteration 225, loss = 0.05526085\n",
      "Iteration 226, loss = 0.05511538\n",
      "Iteration 227, loss = 0.05496176\n",
      "Iteration 228, loss = 0.05477765\n",
      "Iteration 229, loss = 0.05462832\n",
      "Iteration 230, loss = 0.05448747\n",
      "Iteration 231, loss = 0.05433250\n",
      "Iteration 232, loss = 0.05413630\n",
      "Iteration 233, loss = 0.05398666\n",
      "Iteration 234, loss = 0.05382273\n",
      "Iteration 235, loss = 0.05367419\n",
      "Iteration 236, loss = 0.05349984\n",
      "Iteration 237, loss = 0.05336152\n",
      "Iteration 238, loss = 0.05319932\n",
      "Iteration 239, loss = 0.05302819\n",
      "Iteration 240, loss = 0.05289358\n",
      "Iteration 241, loss = 0.05271010\n",
      "Iteration 242, loss = 0.05257161\n",
      "Iteration 243, loss = 0.05241158\n",
      "Iteration 244, loss = 0.05226681\n",
      "Iteration 245, loss = 0.05213720\n",
      "Iteration 246, loss = 0.05196959\n",
      "Iteration 247, loss = 0.05182326\n",
      "Iteration 248, loss = 0.05168202\n",
      "Iteration 249, loss = 0.05152419\n",
      "Iteration 250, loss = 0.05139115\n",
      "Iteration 251, loss = 0.05124150\n",
      "Iteration 252, loss = 0.05109628\n",
      "Iteration 253, loss = 0.05096694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.05080832\n",
      "Iteration 255, loss = 0.05067554\n",
      "Iteration 256, loss = 0.05053552\n",
      "Iteration 257, loss = 0.05038961\n",
      "Iteration 258, loss = 0.05025626\n",
      "Iteration 259, loss = 0.05012112\n",
      "Iteration 260, loss = 0.04997712\n",
      "Iteration 261, loss = 0.04984102\n",
      "Iteration 262, loss = 0.04972536\n",
      "Iteration 263, loss = 0.04959462\n",
      "Iteration 264, loss = 0.04945665\n",
      "Iteration 265, loss = 0.04932291\n",
      "Iteration 266, loss = 0.04917493\n",
      "Iteration 267, loss = 0.04902825\n",
      "Iteration 268, loss = 0.04893470\n",
      "Iteration 269, loss = 0.04877639\n",
      "Iteration 270, loss = 0.04868884\n",
      "Iteration 271, loss = 0.04851906\n",
      "Iteration 272, loss = 0.04838575\n",
      "Iteration 273, loss = 0.04827261\n",
      "Iteration 274, loss = 0.04813231\n",
      "Iteration 275, loss = 0.04799770\n",
      "Iteration 276, loss = 0.04787925\n",
      "Iteration 277, loss = 0.04775598\n",
      "Iteration 278, loss = 0.04763676\n",
      "Iteration 279, loss = 0.04750724\n",
      "Iteration 280, loss = 0.04738207\n",
      "Iteration 281, loss = 0.04725570\n",
      "Iteration 282, loss = 0.04713729\n",
      "Iteration 283, loss = 0.04701285\n",
      "Iteration 284, loss = 0.04691922\n",
      "Iteration 285, loss = 0.04680223\n",
      "Iteration 286, loss = 0.04664952\n",
      "Iteration 287, loss = 0.04652511\n",
      "Iteration 288, loss = 0.04641147\n",
      "Iteration 289, loss = 0.04628933\n",
      "Iteration 290, loss = 0.04618085\n",
      "Iteration 291, loss = 0.04606072\n",
      "Iteration 292, loss = 0.04595048\n",
      "Iteration 293, loss = 0.04582722\n",
      "Iteration 294, loss = 0.04572869\n",
      "Iteration 295, loss = 0.04557880\n",
      "Iteration 296, loss = 0.04548784\n",
      "Iteration 297, loss = 0.04536514\n",
      "Iteration 298, loss = 0.04523882\n",
      "Iteration 299, loss = 0.04512857\n",
      "Iteration 300, loss = 0.04502511\n",
      "Iteration 301, loss = 0.04490364\n",
      "Iteration 302, loss = 0.04478709\n",
      "Iteration 303, loss = 0.04468818\n",
      "Iteration 304, loss = 0.04459333\n",
      "Iteration 305, loss = 0.04445501\n",
      "Iteration 306, loss = 0.04435482\n",
      "Iteration 307, loss = 0.04424245\n",
      "Iteration 308, loss = 0.04413620\n",
      "Iteration 309, loss = 0.04402023\n",
      "Iteration 310, loss = 0.04391276\n",
      "Iteration 311, loss = 0.04380447\n",
      "Iteration 312, loss = 0.04369809\n",
      "Iteration 313, loss = 0.04359881\n",
      "Iteration 314, loss = 0.04350760\n",
      "Iteration 315, loss = 0.04338790\n",
      "Iteration 316, loss = 0.04329261\n",
      "Iteration 317, loss = 0.04320250\n",
      "Iteration 318, loss = 0.04308172\n",
      "Iteration 319, loss = 0.04297461\n",
      "Iteration 320, loss = 0.04286975\n",
      "Iteration 321, loss = 0.04276217\n",
      "Iteration 322, loss = 0.04265963\n",
      "Iteration 323, loss = 0.04256543\n",
      "Iteration 324, loss = 0.04246209\n",
      "Iteration 325, loss = 0.04235808\n",
      "Iteration 326, loss = 0.04226617\n",
      "Iteration 327, loss = 0.04216004\n",
      "Iteration 328, loss = 0.04209005\n",
      "Iteration 329, loss = 0.04198101\n",
      "Iteration 330, loss = 0.04187744\n",
      "Iteration 331, loss = 0.04176912\n",
      "Iteration 332, loss = 0.04169742\n",
      "Iteration 333, loss = 0.04157715\n",
      "Iteration 334, loss = 0.04149302\n",
      "Iteration 335, loss = 0.04139199\n",
      "Iteration 336, loss = 0.04128554\n",
      "Iteration 337, loss = 0.04119194\n",
      "Iteration 338, loss = 0.04110011\n",
      "Iteration 339, loss = 0.04101317\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 340, loss = 0.04090336\n",
      "Iteration 341, loss = 0.04088255\n",
      "Iteration 342, loss = 0.04086363\n",
      "Iteration 343, loss = 0.04084575\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 344, loss = 0.04082395\n",
      "Iteration 345, loss = 0.04082064\n",
      "Iteration 346, loss = 0.04081532\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 347, loss = 0.04081103\n",
      "Iteration 348, loss = 0.04081015\n",
      "Iteration 349, loss = 0.04080929\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 350, loss = 0.04080854\n",
      "Iteration 351, loss = 0.04080834\n",
      "Iteration 352, loss = 0.04080818\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 353, loss = 0.04080800\n",
      "Iteration 354, loss = 0.04080796\n",
      "Iteration 355, loss = 0.04080794\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(4,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_neural = sklearn.neural_network.MLPClassifier(activation = \"relu\", hidden_layer_sizes=(4,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "small_neural.fit(X0_train1,y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2038, 784)\n"
     ]
    }
   ],
   "source": [
    "use_hill=False;N=10;theta=0.5\n",
    "grad,pm = [],[]\n",
    "print(XT.shape)\n",
    "for i in range(len(XT)):\n",
    "    x0 = XT[i]\n",
    "    grad.append(grad_score(x0,use_hill,N,theta,small_neural))\n",
    "    pm.append((yT[i]-5)/2) #3 -> -1; 7 -> +1\n",
    "grad = np.asarray(grad)\n",
    "pm = np.asarray(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.5\n",
    "middle_SM = np.zeros((len(Ns),len(X0_train2)))\n",
    "for idx_N,N in enumerate(Ns):\n",
    "    X = hill(X0_train2,N,theta)\n",
    "    middle_SM[idx_N]=small_neural.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6198, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_SM=np.transpose(middle_SM)\n",
    "middle_SM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70417587\n",
      "Iteration 2, loss = 0.69864791\n",
      "Iteration 3, loss = 0.69392608\n",
      "Iteration 4, loss = 0.69018760\n",
      "Iteration 5, loss = 0.68713554\n",
      "Iteration 6, loss = 0.68448663\n",
      "Iteration 7, loss = 0.68217140\n",
      "Iteration 8, loss = 0.68009153\n",
      "Iteration 9, loss = 0.67813844\n",
      "Iteration 10, loss = 0.67634379\n",
      "Iteration 11, loss = 0.67463128\n",
      "Iteration 12, loss = 0.67301069\n",
      "Iteration 13, loss = 0.67144582\n",
      "Iteration 14, loss = 0.66996033\n",
      "Iteration 15, loss = 0.66850012\n",
      "Iteration 16, loss = 0.66709858\n",
      "Iteration 17, loss = 0.66573779\n",
      "Iteration 18, loss = 0.66441434\n",
      "Iteration 19, loss = 0.66313359\n",
      "Iteration 20, loss = 0.66187171\n",
      "Iteration 21, loss = 0.66063708\n",
      "Iteration 22, loss = 0.65943268\n",
      "Iteration 23, loss = 0.65826630\n",
      "Iteration 24, loss = 0.65708652\n",
      "Iteration 25, loss = 0.65595637\n",
      "Iteration 26, loss = 0.65481326\n",
      "Iteration 27, loss = 0.65370421\n",
      "Iteration 28, loss = 0.65261270\n",
      "Iteration 29, loss = 0.65151598\n",
      "Iteration 30, loss = 0.65043410\n",
      "Iteration 31, loss = 0.64937189\n",
      "Iteration 32, loss = 0.64830095\n",
      "Iteration 33, loss = 0.64723180\n",
      "Iteration 34, loss = 0.64617678\n",
      "Iteration 35, loss = 0.64511748\n",
      "Iteration 36, loss = 0.64406544\n",
      "Iteration 37, loss = 0.64300696\n",
      "Iteration 38, loss = 0.64195337\n",
      "Iteration 39, loss = 0.64089663\n",
      "Iteration 40, loss = 0.63983419\n",
      "Iteration 41, loss = 0.63875692\n",
      "Iteration 42, loss = 0.63769291\n",
      "Iteration 43, loss = 0.63660805\n",
      "Iteration 44, loss = 0.63552225\n",
      "Iteration 45, loss = 0.63443704\n",
      "Iteration 46, loss = 0.63333425\n",
      "Iteration 47, loss = 0.63222028\n",
      "Iteration 48, loss = 0.63110598\n",
      "Iteration 49, loss = 0.62998035\n",
      "Iteration 50, loss = 0.62884100\n",
      "Iteration 51, loss = 0.62769154\n",
      "Iteration 52, loss = 0.62653510\n",
      "Iteration 53, loss = 0.62536469\n",
      "Iteration 54, loss = 0.62419232\n",
      "Iteration 55, loss = 0.62299677\n",
      "Iteration 56, loss = 0.62179385\n",
      "Iteration 57, loss = 0.62057895\n",
      "Iteration 58, loss = 0.61935515\n",
      "Iteration 59, loss = 0.61811510\n",
      "Iteration 60, loss = 0.61685865\n",
      "Iteration 61, loss = 0.61560254\n",
      "Iteration 62, loss = 0.61432255\n",
      "Iteration 63, loss = 0.61303725\n",
      "Iteration 64, loss = 0.61173471\n",
      "Iteration 65, loss = 0.61042408\n",
      "Iteration 66, loss = 0.60909804\n",
      "Iteration 67, loss = 0.60776846\n",
      "Iteration 68, loss = 0.60641827\n",
      "Iteration 69, loss = 0.60506555\n",
      "Iteration 70, loss = 0.60370101\n",
      "Iteration 71, loss = 0.60232776\n",
      "Iteration 72, loss = 0.60095139\n",
      "Iteration 73, loss = 0.59954424\n",
      "Iteration 74, loss = 0.59815218\n",
      "Iteration 75, loss = 0.59673568\n",
      "Iteration 76, loss = 0.59531315\n",
      "Iteration 77, loss = 0.59388524\n",
      "Iteration 78, loss = 0.59247190\n",
      "Iteration 79, loss = 0.59103305\n",
      "Iteration 80, loss = 0.58956003\n",
      "Iteration 81, loss = 0.58810473\n",
      "Iteration 82, loss = 0.58664129\n",
      "Iteration 83, loss = 0.58517007\n",
      "Iteration 84, loss = 0.58369596\n",
      "Iteration 85, loss = 0.58221511\n",
      "Iteration 86, loss = 0.58072684\n",
      "Iteration 87, loss = 0.57923884\n",
      "Iteration 88, loss = 0.57775201\n",
      "Iteration 89, loss = 0.57623334\n",
      "Iteration 90, loss = 0.57472679\n",
      "Iteration 91, loss = 0.57321212\n",
      "Iteration 92, loss = 0.57169873\n",
      "Iteration 93, loss = 0.57017217\n",
      "Iteration 94, loss = 0.56863795\n",
      "Iteration 95, loss = 0.56710370\n",
      "Iteration 96, loss = 0.56555950\n",
      "Iteration 97, loss = 0.56403037\n",
      "Iteration 98, loss = 0.56246398\n",
      "Iteration 99, loss = 0.56090837\n",
      "Iteration 100, loss = 0.55935821\n",
      "Iteration 101, loss = 0.55777654\n",
      "Iteration 102, loss = 0.55621175\n",
      "Iteration 103, loss = 0.55462988\n",
      "Iteration 104, loss = 0.55303615\n",
      "Iteration 105, loss = 0.55144797\n",
      "Iteration 106, loss = 0.54984855\n",
      "Iteration 107, loss = 0.54824428\n",
      "Iteration 108, loss = 0.54663256\n",
      "Iteration 109, loss = 0.54502774\n",
      "Iteration 110, loss = 0.54340922\n",
      "Iteration 111, loss = 0.54176360\n",
      "Iteration 112, loss = 0.54014519\n",
      "Iteration 113, loss = 0.53849339\n",
      "Iteration 114, loss = 0.53684442\n",
      "Iteration 115, loss = 0.53518973\n",
      "Iteration 116, loss = 0.53352955\n",
      "Iteration 117, loss = 0.53186134\n",
      "Iteration 118, loss = 0.53018355\n",
      "Iteration 119, loss = 0.52850158\n",
      "Iteration 120, loss = 0.52681623\n",
      "Iteration 121, loss = 0.52511601\n",
      "Iteration 122, loss = 0.52342418\n",
      "Iteration 123, loss = 0.52170630\n",
      "Iteration 124, loss = 0.51999413\n",
      "Iteration 125, loss = 0.51826883\n",
      "Iteration 126, loss = 0.51653928\n",
      "Iteration 127, loss = 0.51479830\n",
      "Iteration 128, loss = 0.51305939\n",
      "Iteration 129, loss = 0.51130616\n",
      "Iteration 130, loss = 0.50956067\n",
      "Iteration 131, loss = 0.50778580\n",
      "Iteration 132, loss = 0.50601122\n",
      "Iteration 133, loss = 0.50423389\n",
      "Iteration 134, loss = 0.50245296\n",
      "Iteration 135, loss = 0.50067614\n",
      "Iteration 136, loss = 0.49886826\n",
      "Iteration 137, loss = 0.49705394\n",
      "Iteration 138, loss = 0.49524156\n",
      "Iteration 139, loss = 0.49342780\n",
      "Iteration 140, loss = 0.49160005\n",
      "Iteration 141, loss = 0.48976574\n",
      "Iteration 142, loss = 0.48793114\n",
      "Iteration 143, loss = 0.48608132\n",
      "Iteration 144, loss = 0.48423632\n",
      "Iteration 145, loss = 0.48237193\n",
      "Iteration 146, loss = 0.48051919\n",
      "Iteration 147, loss = 0.47864395\n",
      "Iteration 148, loss = 0.47676822\n",
      "Iteration 149, loss = 0.47488887\n",
      "Iteration 150, loss = 0.47300244\n",
      "Iteration 151, loss = 0.47110882\n",
      "Iteration 152, loss = 0.46922521\n",
      "Iteration 153, loss = 0.46732131\n",
      "Iteration 154, loss = 0.46539677\n",
      "Iteration 155, loss = 0.46348793\n",
      "Iteration 156, loss = 0.46157391\n",
      "Iteration 157, loss = 0.45964553\n",
      "Iteration 158, loss = 0.45771898\n",
      "Iteration 159, loss = 0.45579270\n",
      "Iteration 160, loss = 0.45384576\n",
      "Iteration 161, loss = 0.45191370\n",
      "Iteration 162, loss = 0.44995865\n",
      "Iteration 163, loss = 0.44800779\n",
      "Iteration 164, loss = 0.44606442\n",
      "Iteration 165, loss = 0.44409564\n",
      "Iteration 166, loss = 0.44213415\n",
      "Iteration 167, loss = 0.44017720\n",
      "Iteration 168, loss = 0.43822255\n",
      "Iteration 169, loss = 0.43623347\n",
      "Iteration 170, loss = 0.43425990\n",
      "Iteration 171, loss = 0.43227810\n",
      "Iteration 172, loss = 0.43029605\n",
      "Iteration 173, loss = 0.42831564\n",
      "Iteration 174, loss = 0.42632894\n",
      "Iteration 175, loss = 0.42434194\n",
      "Iteration 176, loss = 0.42235194\n",
      "Iteration 177, loss = 0.42035882\n",
      "Iteration 178, loss = 0.41837504\n",
      "Iteration 179, loss = 0.41637001\n",
      "Iteration 180, loss = 0.41437180\n",
      "Iteration 181, loss = 0.41239602\n",
      "Iteration 182, loss = 0.41037837\n",
      "Iteration 183, loss = 0.40838643\n",
      "Iteration 184, loss = 0.40638684\n",
      "Iteration 185, loss = 0.40437959\n",
      "Iteration 186, loss = 0.40238306\n",
      "Iteration 187, loss = 0.40038171\n",
      "Iteration 188, loss = 0.39838005\n",
      "Iteration 189, loss = 0.39638050\n",
      "Iteration 190, loss = 0.39439342\n",
      "Iteration 191, loss = 0.39239211\n",
      "Iteration 192, loss = 0.39039271\n",
      "Iteration 193, loss = 0.38838672\n",
      "Iteration 194, loss = 0.38640132\n",
      "Iteration 195, loss = 0.38439737\n",
      "Iteration 196, loss = 0.38241195\n",
      "Iteration 197, loss = 0.38041786\n",
      "Iteration 198, loss = 0.37843458\n",
      "Iteration 199, loss = 0.37644103\n",
      "Iteration 200, loss = 0.37445918\n",
      "Iteration 201, loss = 0.37247874\n",
      "Iteration 202, loss = 0.37051032\n",
      "Iteration 203, loss = 0.36852897\n",
      "Iteration 204, loss = 0.36655713\n",
      "Iteration 205, loss = 0.36459843\n",
      "Iteration 206, loss = 0.36261773\n",
      "Iteration 207, loss = 0.36067236\n",
      "Iteration 208, loss = 0.35871649\n",
      "Iteration 209, loss = 0.35674835\n",
      "Iteration 210, loss = 0.35480420\n",
      "Iteration 211, loss = 0.35286203\n",
      "Iteration 212, loss = 0.35091335\n",
      "Iteration 213, loss = 0.34898395\n",
      "Iteration 214, loss = 0.34705047\n",
      "Iteration 215, loss = 0.34513152\n",
      "Iteration 216, loss = 0.34320803\n",
      "Iteration 217, loss = 0.34129710\n",
      "Iteration 218, loss = 0.33939248\n",
      "Iteration 219, loss = 0.33748672\n",
      "Iteration 220, loss = 0.33558935\n",
      "Iteration 221, loss = 0.33370019\n",
      "Iteration 222, loss = 0.33181357\n",
      "Iteration 223, loss = 0.32994189\n",
      "Iteration 224, loss = 0.32806760\n",
      "Iteration 225, loss = 0.32620003\n",
      "Iteration 226, loss = 0.32434548\n",
      "Iteration 227, loss = 0.32249713\n",
      "Iteration 228, loss = 0.32064620\n",
      "Iteration 229, loss = 0.31882431\n",
      "Iteration 230, loss = 0.31698994\n",
      "Iteration 231, loss = 0.31516234\n",
      "Iteration 232, loss = 0.31334875\n",
      "Iteration 233, loss = 0.31154279\n",
      "Iteration 234, loss = 0.30975439\n",
      "Iteration 235, loss = 0.30795940\n",
      "Iteration 236, loss = 0.30617550\n",
      "Iteration 237, loss = 0.30441220\n",
      "Iteration 238, loss = 0.30263710\n",
      "Iteration 239, loss = 0.30088721\n",
      "Iteration 240, loss = 0.29914110\n",
      "Iteration 241, loss = 0.29740832\n",
      "Iteration 242, loss = 0.29566765\n",
      "Iteration 243, loss = 0.29395654\n",
      "Iteration 244, loss = 0.29224736\n",
      "Iteration 245, loss = 0.29054055\n",
      "Iteration 246, loss = 0.28884476\n",
      "Iteration 247, loss = 0.28716674\n",
      "Iteration 248, loss = 0.28548159\n",
      "Iteration 249, loss = 0.28381429\n",
      "Iteration 250, loss = 0.28216498\n",
      "Iteration 251, loss = 0.28051747\n",
      "Iteration 252, loss = 0.27887825\n",
      "Iteration 253, loss = 0.27725156\n",
      "Iteration 254, loss = 0.27563510\n",
      "Iteration 255, loss = 0.27403030\n",
      "Iteration 256, loss = 0.27242367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.27084087\n",
      "Iteration 258, loss = 0.26926758\n",
      "Iteration 259, loss = 0.26768987\n",
      "Iteration 260, loss = 0.26613955\n",
      "Iteration 261, loss = 0.26458267\n",
      "Iteration 262, loss = 0.26304904\n",
      "Iteration 263, loss = 0.26151988\n",
      "Iteration 264, loss = 0.26000527\n",
      "Iteration 265, loss = 0.25849346\n",
      "Iteration 266, loss = 0.25699723\n",
      "Iteration 267, loss = 0.25550764\n",
      "Iteration 268, loss = 0.25403405\n",
      "Iteration 269, loss = 0.25257596\n",
      "Iteration 270, loss = 0.25110810\n",
      "Iteration 271, loss = 0.24966860\n",
      "Iteration 272, loss = 0.24822785\n",
      "Iteration 273, loss = 0.24680373\n",
      "Iteration 274, loss = 0.24539398\n",
      "Iteration 275, loss = 0.24399005\n",
      "Iteration 276, loss = 0.24258859\n",
      "Iteration 277, loss = 0.24121369\n",
      "Iteration 278, loss = 0.23983666\n",
      "Iteration 279, loss = 0.23846574\n",
      "Iteration 280, loss = 0.23711518\n",
      "Iteration 281, loss = 0.23579437\n",
      "Iteration 282, loss = 0.23443812\n",
      "Iteration 283, loss = 0.23312004\n",
      "Iteration 284, loss = 0.23180729\n",
      "Iteration 285, loss = 0.23050917\n",
      "Iteration 286, loss = 0.22922184\n",
      "Iteration 287, loss = 0.22793787\n",
      "Iteration 288, loss = 0.22665733\n",
      "Iteration 289, loss = 0.22539888\n",
      "Iteration 290, loss = 0.22414323\n",
      "Iteration 291, loss = 0.22290637\n",
      "Iteration 292, loss = 0.22167124\n",
      "Iteration 293, loss = 0.22045465\n",
      "Iteration 294, loss = 0.21923900\n",
      "Iteration 295, loss = 0.21803929\n",
      "Iteration 296, loss = 0.21684712\n",
      "Iteration 297, loss = 0.21567078\n",
      "Iteration 298, loss = 0.21448581\n",
      "Iteration 299, loss = 0.21333955\n",
      "Iteration 300, loss = 0.21217985\n",
      "Iteration 301, loss = 0.21103828\n",
      "Iteration 302, loss = 0.20990734\n",
      "Iteration 303, loss = 0.20877809\n",
      "Iteration 304, loss = 0.20768788\n",
      "Iteration 305, loss = 0.20655369\n",
      "Iteration 306, loss = 0.20546844\n",
      "Iteration 307, loss = 0.20438310\n",
      "Iteration 308, loss = 0.20331051\n",
      "Iteration 309, loss = 0.20223175\n",
      "Iteration 310, loss = 0.20117655\n",
      "Iteration 311, loss = 0.20012785\n",
      "Iteration 312, loss = 0.19908516\n",
      "Iteration 313, loss = 0.19805409\n",
      "Iteration 314, loss = 0.19703674\n",
      "Iteration 315, loss = 0.19602738\n",
      "Iteration 316, loss = 0.19501703\n",
      "Iteration 317, loss = 0.19403137\n",
      "Iteration 318, loss = 0.19303866\n",
      "Iteration 319, loss = 0.19206132\n",
      "Iteration 320, loss = 0.19109763\n",
      "Iteration 321, loss = 0.19014329\n",
      "Iteration 322, loss = 0.18918000\n",
      "Iteration 323, loss = 0.18824640\n",
      "Iteration 324, loss = 0.18731090\n",
      "Iteration 325, loss = 0.18637636\n",
      "Iteration 326, loss = 0.18546193\n",
      "Iteration 327, loss = 0.18455215\n",
      "Iteration 328, loss = 0.18366312\n",
      "Iteration 329, loss = 0.18277301\n",
      "Iteration 330, loss = 0.18187111\n",
      "Iteration 331, loss = 0.18099038\n",
      "Iteration 332, loss = 0.18012397\n",
      "Iteration 333, loss = 0.17925896\n",
      "Iteration 334, loss = 0.17841137\n",
      "Iteration 335, loss = 0.17756636\n",
      "Iteration 336, loss = 0.17672398\n",
      "Iteration 337, loss = 0.17590051\n",
      "Iteration 338, loss = 0.17507985\n",
      "Iteration 339, loss = 0.17425669\n",
      "Iteration 340, loss = 0.17345109\n",
      "Iteration 341, loss = 0.17264821\n",
      "Iteration 342, loss = 0.17185261\n",
      "Iteration 343, loss = 0.17106926\n",
      "Iteration 344, loss = 0.17028978\n",
      "Iteration 345, loss = 0.16951946\n",
      "Iteration 346, loss = 0.16876113\n",
      "Iteration 347, loss = 0.16799734\n",
      "Iteration 348, loss = 0.16725244\n",
      "Iteration 349, loss = 0.16650040\n",
      "Iteration 350, loss = 0.16577038\n",
      "Iteration 351, loss = 0.16505196\n",
      "Iteration 352, loss = 0.16431680\n",
      "Iteration 353, loss = 0.16360392\n",
      "Iteration 354, loss = 0.16289389\n",
      "Iteration 355, loss = 0.16218913\n",
      "Iteration 356, loss = 0.16148993\n",
      "Iteration 357, loss = 0.16080090\n",
      "Iteration 358, loss = 0.16011443\n",
      "Iteration 359, loss = 0.15943692\n",
      "Iteration 360, loss = 0.15876710\n",
      "Iteration 361, loss = 0.15810912\n",
      "Iteration 362, loss = 0.15744247\n",
      "Iteration 363, loss = 0.15679124\n",
      "Iteration 364, loss = 0.15614354\n",
      "Iteration 365, loss = 0.15550307\n",
      "Iteration 366, loss = 0.15486975\n",
      "Iteration 367, loss = 0.15423754\n",
      "Iteration 368, loss = 0.15362566\n",
      "Iteration 369, loss = 0.15299818\n",
      "Iteration 370, loss = 0.15239240\n",
      "Iteration 371, loss = 0.15178634\n",
      "Iteration 372, loss = 0.15118299\n",
      "Iteration 373, loss = 0.15058630\n",
      "Iteration 374, loss = 0.14999796\n",
      "Iteration 375, loss = 0.14941634\n",
      "Iteration 376, loss = 0.14883537\n",
      "Iteration 377, loss = 0.14826535\n",
      "Iteration 378, loss = 0.14769122\n",
      "Iteration 379, loss = 0.14713825\n",
      "Iteration 380, loss = 0.14657775\n",
      "Iteration 381, loss = 0.14602276\n",
      "Iteration 382, loss = 0.14547119\n",
      "Iteration 383, loss = 0.14493193\n",
      "Iteration 384, loss = 0.14439958\n",
      "Iteration 385, loss = 0.14386639\n",
      "Iteration 386, loss = 0.14334290\n",
      "Iteration 387, loss = 0.14281978\n",
      "Iteration 388, loss = 0.14229531\n",
      "Iteration 389, loss = 0.14178373\n",
      "Iteration 390, loss = 0.14127426\n",
      "Iteration 391, loss = 0.14077594\n",
      "Iteration 392, loss = 0.14026919\n",
      "Iteration 393, loss = 0.13977539\n",
      "Iteration 394, loss = 0.13928763\n",
      "Iteration 395, loss = 0.13880347\n",
      "Iteration 396, loss = 0.13831802\n",
      "Iteration 397, loss = 0.13784570\n",
      "Iteration 398, loss = 0.13737276\n",
      "Iteration 399, loss = 0.13690366\n",
      "Iteration 400, loss = 0.13645066\n",
      "Iteration 401, loss = 0.13599206\n",
      "Iteration 402, loss = 0.13552458\n",
      "Iteration 403, loss = 0.13507662\n",
      "Iteration 404, loss = 0.13462786\n",
      "Iteration 405, loss = 0.13418421\n",
      "Iteration 406, loss = 0.13374468\n",
      "Iteration 407, loss = 0.13332509\n",
      "Iteration 408, loss = 0.13288349\n",
      "Iteration 409, loss = 0.13245373\n",
      "Iteration 410, loss = 0.13202846\n",
      "Iteration 411, loss = 0.13160860\n",
      "Iteration 412, loss = 0.13120105\n",
      "Iteration 413, loss = 0.13078235\n",
      "Iteration 414, loss = 0.13037548\n",
      "Iteration 415, loss = 0.12998238\n",
      "Iteration 416, loss = 0.12956956\n",
      "Iteration 417, loss = 0.12916810\n",
      "Iteration 418, loss = 0.12877361\n",
      "Iteration 419, loss = 0.12838771\n",
      "Iteration 420, loss = 0.12799798\n",
      "Iteration 421, loss = 0.12761603\n",
      "Iteration 422, loss = 0.12723569\n",
      "Iteration 423, loss = 0.12685567\n",
      "Iteration 424, loss = 0.12648220\n",
      "Iteration 425, loss = 0.12611492\n",
      "Iteration 426, loss = 0.12574664\n",
      "Iteration 427, loss = 0.12538221\n",
      "Iteration 428, loss = 0.12502507\n",
      "Iteration 429, loss = 0.12466698\n",
      "Iteration 430, loss = 0.12431123\n",
      "Iteration 431, loss = 0.12396759\n",
      "Iteration 432, loss = 0.12362145\n",
      "Iteration 433, loss = 0.12327599\n",
      "Iteration 434, loss = 0.12292995\n",
      "Iteration 435, loss = 0.12258893\n",
      "Iteration 436, loss = 0.12225197\n",
      "Iteration 437, loss = 0.12191833\n",
      "Iteration 438, loss = 0.12159177\n",
      "Iteration 439, loss = 0.12126138\n",
      "Iteration 440, loss = 0.12094323\n",
      "Iteration 441, loss = 0.12061915\n",
      "Iteration 442, loss = 0.12029623\n",
      "Iteration 443, loss = 0.11998175\n",
      "Iteration 444, loss = 0.11967063\n",
      "Iteration 445, loss = 0.11935544\n",
      "Iteration 446, loss = 0.11905189\n",
      "Iteration 447, loss = 0.11874474\n",
      "Iteration 448, loss = 0.11844782\n",
      "Iteration 449, loss = 0.11814632\n",
      "Iteration 450, loss = 0.11784525\n",
      "Iteration 451, loss = 0.11755221\n",
      "Iteration 452, loss = 0.11725695\n",
      "Iteration 453, loss = 0.11696698\n",
      "Iteration 454, loss = 0.11668231\n",
      "Iteration 455, loss = 0.11639992\n",
      "Iteration 456, loss = 0.11611415\n",
      "Iteration 457, loss = 0.11583498\n",
      "Iteration 458, loss = 0.11555917\n",
      "Iteration 459, loss = 0.11528299\n",
      "Iteration 460, loss = 0.11500993\n",
      "Iteration 461, loss = 0.11473605\n",
      "Iteration 462, loss = 0.11446780\n",
      "Iteration 463, loss = 0.11420997\n",
      "Iteration 464, loss = 0.11394464\n",
      "Iteration 465, loss = 0.11367471\n",
      "Iteration 466, loss = 0.11341652\n",
      "Iteration 467, loss = 0.11316961\n",
      "Iteration 468, loss = 0.11290734\n",
      "Iteration 469, loss = 0.11265288\n",
      "Iteration 470, loss = 0.11240354\n",
      "Iteration 471, loss = 0.11215281\n",
      "Iteration 472, loss = 0.11190850\n",
      "Iteration 473, loss = 0.11166191\n",
      "Iteration 474, loss = 0.11141882\n",
      "Iteration 475, loss = 0.11118140\n",
      "Iteration 476, loss = 0.11093819\n",
      "Iteration 477, loss = 0.11070368\n",
      "Iteration 478, loss = 0.11046490\n",
      "Iteration 479, loss = 0.11023807\n",
      "Iteration 480, loss = 0.11000803\n",
      "Iteration 481, loss = 0.10977761\n",
      "Iteration 482, loss = 0.10955075\n",
      "Iteration 483, loss = 0.10932275\n",
      "Iteration 484, loss = 0.10909894\n",
      "Iteration 485, loss = 0.10887593\n",
      "Iteration 486, loss = 0.10865728\n",
      "Iteration 487, loss = 0.10843714\n",
      "Iteration 488, loss = 0.10822356\n",
      "Iteration 489, loss = 0.10800662\n",
      "Iteration 490, loss = 0.10779565\n",
      "Iteration 491, loss = 0.10758320\n",
      "Iteration 492, loss = 0.10737064\n",
      "Iteration 493, loss = 0.10716625\n",
      "Iteration 494, loss = 0.10695972\n",
      "Iteration 495, loss = 0.10675460\n",
      "Iteration 496, loss = 0.10655367\n",
      "Iteration 497, loss = 0.10635229\n",
      "Iteration 498, loss = 0.10615043\n",
      "Iteration 499, loss = 0.10595654\n",
      "Iteration 500, loss = 0.10575741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andres\\Documents\\GitHub\\Physics_Hackathon\\neural_network_repn\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.neural_network\n",
    "\n",
    "import neural_network_repn\n",
    "setattr(sklearn,'neural_network',neural_network_repn)\n",
    "master_neural = sklearn.neural_network.MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(5,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "middle_SM = np.log(middle_SM)\n",
    "master_neural.fit(middle_SM,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden-layer feed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights and Biases for Hidden Layer\n",
    "w1 = small_neural.coefs_[0]\n",
    "b1 = small_neural.intercepts_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6198, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "theta = 0.5\n",
    "middle_HFI = np.zeros(shape=(len(X0_train2),len(Ns),4))\n",
    "for idx_N,N in enumerate(Ns):\n",
    "    X = hill(X0_train2,N,theta)\n",
    "    for i in range(len(X0_train2)):\n",
    "        h1=np.matmul(X0_train2[i],w1) + b1\n",
    "        h1=np.maximum(0, h1)\n",
    "        middle_HFI[i][idx_N]=h1\n",
    "print(middle_HFI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_HFI=middle_HFI.reshape(6198,4*len(Ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68236647\n",
      "Iteration 2, loss = 0.67339806\n",
      "Iteration 3, loss = 0.66462351\n",
      "Iteration 4, loss = 0.65673957\n",
      "Iteration 5, loss = 0.64929834\n",
      "Iteration 6, loss = 0.64225894\n",
      "Iteration 7, loss = 0.63538007\n",
      "Iteration 8, loss = 0.62865052\n",
      "Iteration 9, loss = 0.62194936\n",
      "Iteration 10, loss = 0.61525960\n",
      "Iteration 11, loss = 0.60850796\n",
      "Iteration 12, loss = 0.60173220\n",
      "Iteration 13, loss = 0.59487684\n",
      "Iteration 14, loss = 0.58794420\n",
      "Iteration 15, loss = 0.58091307\n",
      "Iteration 16, loss = 0.57379961\n",
      "Iteration 17, loss = 0.56661456\n",
      "Iteration 18, loss = 0.55931191\n",
      "Iteration 19, loss = 0.55194412\n",
      "Iteration 20, loss = 0.54447394\n",
      "Iteration 21, loss = 0.53695919\n",
      "Iteration 22, loss = 0.52935262\n",
      "Iteration 23, loss = 0.52170233\n",
      "Iteration 24, loss = 0.51399761\n",
      "Iteration 25, loss = 0.50623698\n",
      "Iteration 26, loss = 0.49844602\n",
      "Iteration 27, loss = 0.49064128\n",
      "Iteration 28, loss = 0.48280899\n",
      "Iteration 29, loss = 0.47498216\n",
      "Iteration 30, loss = 0.46716185\n",
      "Iteration 31, loss = 0.45934524\n",
      "Iteration 32, loss = 0.45156892\n",
      "Iteration 33, loss = 0.44383859\n",
      "Iteration 34, loss = 0.43615445\n",
      "Iteration 35, loss = 0.42848650\n",
      "Iteration 36, loss = 0.42093819\n",
      "Iteration 37, loss = 0.41342229\n",
      "Iteration 38, loss = 0.40600298\n",
      "Iteration 39, loss = 0.39867542\n",
      "Iteration 40, loss = 0.39142522\n",
      "Iteration 41, loss = 0.38430377\n",
      "Iteration 42, loss = 0.37724824\n",
      "Iteration 43, loss = 0.37033288\n",
      "Iteration 44, loss = 0.36352252\n",
      "Iteration 45, loss = 0.35683825\n",
      "Iteration 46, loss = 0.35026454\n",
      "Iteration 47, loss = 0.34381267\n",
      "Iteration 48, loss = 0.33749701\n",
      "Iteration 49, loss = 0.33130592\n",
      "Iteration 50, loss = 0.32524758\n",
      "Iteration 51, loss = 0.31930806\n",
      "Iteration 52, loss = 0.31351011\n",
      "Iteration 53, loss = 0.30783041\n",
      "Iteration 54, loss = 0.30228986\n",
      "Iteration 55, loss = 0.29687766\n",
      "Iteration 56, loss = 0.29159210\n",
      "Iteration 57, loss = 0.28643679\n",
      "Iteration 58, loss = 0.28140183\n",
      "Iteration 59, loss = 0.27649709\n",
      "Iteration 60, loss = 0.27171451\n",
      "Iteration 61, loss = 0.26705186\n",
      "Iteration 62, loss = 0.26251290\n",
      "Iteration 63, loss = 0.25807106\n",
      "Iteration 64, loss = 0.25377388\n",
      "Iteration 65, loss = 0.24958377\n",
      "Iteration 66, loss = 0.24548046\n",
      "Iteration 67, loss = 0.24150630\n",
      "Iteration 68, loss = 0.23763459\n",
      "Iteration 69, loss = 0.23386523\n",
      "Iteration 70, loss = 0.23019231\n",
      "Iteration 71, loss = 0.22663117\n",
      "Iteration 72, loss = 0.22314725\n",
      "Iteration 73, loss = 0.21977100\n",
      "Iteration 74, loss = 0.21648083\n",
      "Iteration 75, loss = 0.21328242\n",
      "Iteration 76, loss = 0.21016691\n",
      "Iteration 77, loss = 0.20714082\n",
      "Iteration 78, loss = 0.20419424\n",
      "Iteration 79, loss = 0.20132436\n",
      "Iteration 80, loss = 0.19853045\n",
      "Iteration 81, loss = 0.19582846\n",
      "Iteration 82, loss = 0.19317403\n",
      "Iteration 83, loss = 0.19060607\n",
      "Iteration 84, loss = 0.18811096\n",
      "Iteration 85, loss = 0.18567120\n",
      "Iteration 86, loss = 0.18330029\n",
      "Iteration 87, loss = 0.18099814\n",
      "Iteration 88, loss = 0.17875725\n",
      "Iteration 89, loss = 0.17657042\n",
      "Iteration 90, loss = 0.17443928\n",
      "Iteration 91, loss = 0.17236475\n",
      "Iteration 92, loss = 0.17035230\n",
      "Iteration 93, loss = 0.16838708\n",
      "Iteration 94, loss = 0.16647075\n",
      "Iteration 95, loss = 0.16461537\n",
      "Iteration 96, loss = 0.16279248\n",
      "Iteration 97, loss = 0.16102692\n",
      "Iteration 98, loss = 0.15930526\n",
      "Iteration 99, loss = 0.15763047\n",
      "Iteration 100, loss = 0.15598835\n",
      "Iteration 101, loss = 0.15439253\n",
      "Iteration 102, loss = 0.15284450\n",
      "Iteration 103, loss = 0.15132411\n",
      "Iteration 104, loss = 0.14984636\n",
      "Iteration 105, loss = 0.14840931\n",
      "Iteration 106, loss = 0.14700009\n",
      "Iteration 107, loss = 0.14563214\n",
      "Iteration 108, loss = 0.14429396\n",
      "Iteration 109, loss = 0.14299206\n",
      "Iteration 110, loss = 0.14171873\n",
      "Iteration 111, loss = 0.14048007\n",
      "Iteration 112, loss = 0.13926675\n",
      "Iteration 113, loss = 0.13808358\n",
      "Iteration 114, loss = 0.13692927\n",
      "Iteration 115, loss = 0.13580452\n",
      "Iteration 116, loss = 0.13470622\n",
      "Iteration 117, loss = 0.13362642\n",
      "Iteration 118, loss = 0.13257562\n",
      "Iteration 119, loss = 0.13155485\n",
      "Iteration 120, loss = 0.13055114\n",
      "Iteration 121, loss = 0.12957555\n",
      "Iteration 122, loss = 0.12861749\n",
      "Iteration 123, loss = 0.12768466\n",
      "Iteration 124, loss = 0.12677264\n",
      "Iteration 125, loss = 0.12587791\n",
      "Iteration 126, loss = 0.12500837\n",
      "Iteration 127, loss = 0.12415418\n",
      "Iteration 128, loss = 0.12331820\n",
      "Iteration 129, loss = 0.12250502\n",
      "Iteration 130, loss = 0.12171070\n",
      "Iteration 131, loss = 0.12092397\n",
      "Iteration 132, loss = 0.12016531\n",
      "Iteration 133, loss = 0.11941615\n",
      "Iteration 134, loss = 0.11868836\n",
      "Iteration 135, loss = 0.11797008\n",
      "Iteration 136, loss = 0.11727202\n",
      "Iteration 137, loss = 0.11658824\n",
      "Iteration 138, loss = 0.11591405\n",
      "Iteration 139, loss = 0.11525929\n",
      "Iteration 140, loss = 0.11461715\n",
      "Iteration 141, loss = 0.11398689\n",
      "Iteration 142, loss = 0.11337261\n",
      "Iteration 143, loss = 0.11276584\n",
      "Iteration 144, loss = 0.11217489\n",
      "Iteration 145, loss = 0.11159496\n",
      "Iteration 146, loss = 0.11102748\n",
      "Iteration 147, loss = 0.11047109\n",
      "Iteration 148, loss = 0.10992399\n",
      "Iteration 149, loss = 0.10939064\n",
      "Iteration 150, loss = 0.10886632\n",
      "Iteration 151, loss = 0.10835264\n",
      "Iteration 152, loss = 0.10784803\n",
      "Iteration 153, loss = 0.10735937\n",
      "Iteration 154, loss = 0.10687087\n",
      "Iteration 155, loss = 0.10639509\n",
      "Iteration 156, loss = 0.10593230\n",
      "Iteration 157, loss = 0.10547404\n",
      "Iteration 158, loss = 0.10502645\n",
      "Iteration 159, loss = 0.10458636\n",
      "Iteration 160, loss = 0.10415234\n",
      "Iteration 161, loss = 0.10373163\n",
      "Iteration 162, loss = 0.10331463\n",
      "Iteration 163, loss = 0.10290847\n",
      "Iteration 164, loss = 0.10250600\n",
      "Iteration 165, loss = 0.10211609\n",
      "Iteration 166, loss = 0.10172795\n",
      "Iteration 167, loss = 0.10134994\n",
      "Iteration 168, loss = 0.10097752\n",
      "Iteration 169, loss = 0.10061362\n",
      "Iteration 170, loss = 0.10025498\n",
      "Iteration 171, loss = 0.09990098\n",
      "Iteration 172, loss = 0.09955373\n",
      "Iteration 173, loss = 0.09921526\n",
      "Iteration 174, loss = 0.09888121\n",
      "Iteration 175, loss = 0.09855276\n",
      "Iteration 176, loss = 0.09822899\n",
      "Iteration 177, loss = 0.09791242\n",
      "Iteration 178, loss = 0.09759989\n",
      "Iteration 179, loss = 0.09729615\n",
      "Iteration 180, loss = 0.09699582\n",
      "Iteration 181, loss = 0.09669847\n",
      "Iteration 182, loss = 0.09640746\n",
      "Iteration 183, loss = 0.09612226\n",
      "Iteration 184, loss = 0.09583760\n",
      "Iteration 185, loss = 0.09556525\n",
      "Iteration 186, loss = 0.09529321\n",
      "Iteration 187, loss = 0.09502188\n",
      "Iteration 188, loss = 0.09476303\n",
      "Iteration 189, loss = 0.09449924\n",
      "Iteration 190, loss = 0.09424698\n",
      "Iteration 191, loss = 0.09399733\n",
      "Iteration 192, loss = 0.09374854\n",
      "Iteration 193, loss = 0.09350956\n",
      "Iteration 194, loss = 0.09326845\n",
      "Iteration 195, loss = 0.09303470\n",
      "Iteration 196, loss = 0.09280251\n",
      "Iteration 197, loss = 0.09257462\n",
      "Iteration 198, loss = 0.09235358\n",
      "Iteration 199, loss = 0.09213291\n",
      "Iteration 200, loss = 0.09191348\n",
      "Iteration 201, loss = 0.09170299\n",
      "Iteration 202, loss = 0.09148862\n",
      "Iteration 203, loss = 0.09128389\n",
      "Iteration 204, loss = 0.09107715\n",
      "Iteration 205, loss = 0.09087824\n",
      "Iteration 206, loss = 0.09068198\n",
      "Iteration 207, loss = 0.09048633\n",
      "Iteration 208, loss = 0.09029345\n",
      "Iteration 209, loss = 0.09010394\n",
      "Iteration 210, loss = 0.08991891\n",
      "Iteration 211, loss = 0.08973525\n",
      "Iteration 212, loss = 0.08955378\n",
      "Iteration 213, loss = 0.08937701\n",
      "Iteration 214, loss = 0.08919993\n",
      "Iteration 215, loss = 0.08902927\n",
      "Iteration 216, loss = 0.08886056\n",
      "Iteration 217, loss = 0.08868892\n",
      "Iteration 218, loss = 0.08852424\n",
      "Iteration 219, loss = 0.08836109\n",
      "Iteration 220, loss = 0.08819962\n",
      "Iteration 221, loss = 0.08804138\n",
      "Iteration 222, loss = 0.08788440\n",
      "Iteration 223, loss = 0.08773125\n",
      "Iteration 224, loss = 0.08757818\n",
      "Iteration 225, loss = 0.08742881\n",
      "Iteration 226, loss = 0.08728334\n",
      "Iteration 227, loss = 0.08713671\n",
      "Iteration 228, loss = 0.08699218\n",
      "Iteration 229, loss = 0.08685051\n",
      "Iteration 230, loss = 0.08671218\n",
      "Iteration 231, loss = 0.08657289\n",
      "Iteration 232, loss = 0.08643930\n",
      "Iteration 233, loss = 0.08630423\n",
      "Iteration 234, loss = 0.08617162\n",
      "Iteration 235, loss = 0.08604001\n",
      "Iteration 236, loss = 0.08591126\n",
      "Iteration 237, loss = 0.08578533\n",
      "Iteration 238, loss = 0.08565945\n",
      "Iteration 239, loss = 0.08553425\n",
      "Iteration 240, loss = 0.08541316\n",
      "Iteration 241, loss = 0.08529290\n",
      "Iteration 242, loss = 0.08517491\n",
      "Iteration 243, loss = 0.08505599\n",
      "Iteration 244, loss = 0.08493987\n",
      "Iteration 245, loss = 0.08482546\n",
      "Iteration 246, loss = 0.08471522\n",
      "Iteration 247, loss = 0.08460232\n",
      "Iteration 248, loss = 0.08449097\n",
      "Iteration 249, loss = 0.08438237\n",
      "Iteration 250, loss = 0.08427538\n",
      "Iteration 251, loss = 0.08416900\n",
      "Iteration 252, loss = 0.08406587\n",
      "Iteration 253, loss = 0.08396213\n",
      "Iteration 254, loss = 0.08386156\n",
      "Iteration 255, loss = 0.08376046\n",
      "Iteration 256, loss = 0.08365873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.08356249\n",
      "Iteration 258, loss = 0.08346626\n",
      "Iteration 259, loss = 0.08337005\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 260, loss = 0.08329644\n",
      "Iteration 261, loss = 0.08327135\n",
      "Iteration 262, loss = 0.08325239\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 263, loss = 0.08323743\n",
      "Iteration 264, loss = 0.08323266\n",
      "Iteration 265, loss = 0.08322880\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 266, loss = 0.08322598\n",
      "Iteration 267, loss = 0.08322499\n",
      "Iteration 268, loss = 0.08322421\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 269, loss = 0.08322364\n",
      "Iteration 270, loss = 0.08322345\n",
      "Iteration 271, loss = 0.08322330\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 272, loss = 0.08322318\n",
      "Iteration 273, loss = 0.08322314\n",
      "Iteration 274, loss = 0.08322311\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    }
   ],
   "source": [
    "HFI = sklearn.neural_network.MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(5,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "for i, middle in enumerate(middle_HFI):\n",
    "    middle[middle==0] = 1\n",
    "    middle_HFI[i] = middle\n",
    "middle_HFI = np.log(np.abs(middle_HFI))\n",
    "HFI.fit(middle_HFI,y_train2)\n",
    "(a,b)=HFI.coefs_\n",
    "a2 = a.flatten()\n",
    "c = []\n",
    "for i in range(len(a2)):\n",
    "    if a2[i] >= 0:\n",
    "        c.append(int(a2[i]+1/2))\n",
    "    else:\n",
    "        c.append(-int(np.abs(a2[i]-1/2)))\n",
    "\n",
    "c = np.array(c).reshape(a.shape)\n",
    "HFI.coefs_[0] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "master training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_per3(X,eps,delta):\n",
    "    return X - eps*np.sign(delta)\n",
    "def adv_per7(X,eps,delta):\n",
    "    return X + eps*np.sign(delta)\n",
    "delta = X0[y == 3].mean(axis=0) - X0[y == 7].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT_perturbed = np.zeros(XT.shape)\n",
    "theta = 0.5\n",
    "middle_SM = np.zeros((len(Ns),len(XT)))\n",
    "middle_HFI = np.zeros(shape=(len(XT),len(Ns),4))\n",
    "epsilon = np.arange(0,10,1)\n",
    "Thomas = np.zeros((len(Ns),len(epsilon)))\n",
    "score1,score2 = [],[]\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    XT_perturbed = []\n",
    "    for i in range(len(pm)):\n",
    "        XT_perturbed.append(XT[i] - pm[i]*eps*grad[i])\n",
    "    XT_perturbed = np.asarray(XT_perturbed)\n",
    "    #XT_perturbed[yT == 3] = adv_per3(XT[yT == 3],eps,delta)\n",
    "    #XT_perturbed[yT == 7] = adv_per7(XT[yT == 7],eps,delta)\n",
    "    for idx_N,N in enumerate(Ns):\n",
    "        X = hill(XT_perturbed,N,theta)\n",
    "        middle_SM[idx_N]=small_neural.predict(X)\n",
    "        Thomas[idx_N][idx_eps]=small_neural.score(X,yT)\n",
    "        for i in range(len(X)):\n",
    "            h1=np.matmul(X[i],w1) + b1\n",
    "            h1=np.maximum(0, h1)\n",
    "            middle_HFI[i][idx_N]=h1\n",
    "    \n",
    "    middleT1=np.transpose(middle_SM)\n",
    "    middleT2=middle_HFI.reshape(len(X),4*len(Ns))\n",
    "    score1.append(master_neural.score(middleT1,yT))\n",
    "    score2.append(HFI.score(middleT2,yT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [-1, -1, -1,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0, -1, -1,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [-1,  0,  0,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0, -1, -1,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0, -1,  1,  1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HFI.coefs_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b831abfd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "#ax.plot(epsilon,score1)\n",
    "ax.plot(epsilon,Thomas[len(Thomas)-1])\n",
    "ax.plot(epsilon,score2)\n",
    "ax.set_title(r\"Comparison of Models; $\\theta$ = %.1f\"%theta)\n",
    "ax.set_xlabel(\"Epsilon\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "plt.legend(['Stacked Model','Highest Power','Hidden layer feed in'], title = \"       Model\")\n",
    "#plt.savefig(\"../Figures/Even-powers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "final = True\n",
    "for i in range(len(score1)):\n",
    "    if (score1[i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False\n",
    "final = True\n",
    "for i in range(len(Thomas[len(Thomas)-1])):\n",
    "    if (Thomas[len(Thomas)-1][i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False\n",
    "final = True\n",
    "for i in range(len(score1)):\n",
    "    if (score2[i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X0_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f7dc6025fb0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX0_adv_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX0_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mNs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X0_test' is not defined"
     ]
    }
   ],
   "source": [
    "X0_adv_test = np.zeros(X0_test.shape)\n",
    "theta = 0.5\n",
    "epsilon = np.arange(0,0.5,0.01)\n",
    "Ns = np.arange(11)\n",
    "score = np.zeros((len(Ns),len(epsilon)))\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    X0_adv_test[y_test == 3] = adv_per3(X0_test[y_test == 3],eps,delta)\n",
    "    X0_adv_test[y_test == 7] = adv_per7(X0_test[y_test == 7],eps,delta)\n",
    "    for idx_N,N in enumerate(Ns):\n",
    "        X1_adv_test = hill(X0_adv_test,N,theta)\n",
    "        score[idx_N,idx_eps]=mlp_orig.score(X1_adv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for line in score[range(2,12,2),:]:\n",
    "    ax.plot(epsilon,line)\n",
    "    ax.set_title(r\"Even powers; $\\theta$ = %.1f\"%theta)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.legend([2,4,6,8,10], title = \"       N\")\n",
    "    #plt.savefig(\"../Figures/Even-powers.png\")\n",
    "    \n",
    "fig,ax = plt.subplots()\n",
    "for line in score[range(1,11,2),:]:\n",
    "    ax.plot(epsilon,line)\n",
    "    ax.set_title(r\"Odd powers; $\\theta$ = %.1f\"%theta)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.legend(range(1,11,2), title = \"        N\")\n",
    "    #plt.savefig(\"../Figures/Odd-powers.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_naive = np.zeros((len(epsilon),1))\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    X0_adv_test[y_test == 3] = adv_per3(X0_test[y_test == 3],eps,delta)\n",
    "    X0_adv_test[y_test == 7] = adv_per7(X0_test[y_test == 7],eps,delta)\n",
    "    score_naive[idx_eps]=mlp_orig.score(X0_adv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in score_naive.T:\n",
    "    plt.plot(epsilon,line)\n",
    "    plt.title(\"Naive adversarial perturbation\")\n",
    "    plt.legend([\"No transformation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gradient manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(relu(np.dot(y,W[0])+b[0]),W[1])+b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(relu(np.dot(y,W[0])+b[0]),W[1])+b[1])\n",
    "def grad_score(x,use_hill,N,theta,mlp):\n",
    "    grad = np.zeros(x.shape)\n",
    "    for i in range(len(x)):\n",
    "        yp = x.copy(); yp[i] += 10/255; \n",
    "        ym = x.copy(); ym[i] -= 10/255;\n",
    "        grad[i] = 255/2*(score(yp,use_hill,N,theta,mlp)-score(ym,use_hill,N,theta,mlp))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_grad(x0,use_hill,N,theta,mlp,eps):\n",
    "    num_iter = 0; \n",
    "    x = x0.copy()\n",
    "    pred = mlp.predict(x0.reshape(1,-1))\n",
    "    new_pred = mlp.predict(x.reshape(1,-1))\n",
    "    pm = (pred-5)/2 #3 -> -1; 7 -> +1\n",
    "    while new_pred == pred:\n",
    "        grad = grad_score(x,use_hill,N,theta,mlp)\n",
    "        x -= pm*eps*grad[:]\n",
    "        new_pred = mlp.predict(x.reshape(1,-1))\n",
    "        print(score(x,use_hill,N,theta,mlp))\n",
    "        num_iter += 1\n",
    "    return x,num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hill=True;N=10;theta=0.5\n",
    "x0 = X0_test[y_test==3][1000]\n",
    "grad = grad_score(x0,True,N,theta,mlp_orig)\n",
    "plt.imshow(grad.reshape(28,28),cmap=plt.cm.gray)\n",
    "plt.axis('off');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = X0_test[y_test==7][1000]\n",
    "x1,num_iter = iter_grad(x0,True,10,theta,mlp_orig,5)\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(x0.reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[1].imshow(x1.reshape(28,28),cmap=plt.cm.gray)\n",
    "# ax[2].imshow(hill(x.reshape(28,28),N,theta),cmap=plt.cm.gray)\n",
    "\n",
    "[a.axis('off') for a in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [int(len(X0_test)*random.random()) for _ in range(10)]\n",
    "\n",
    "for i in sample:\n",
    "    x = X0_test[i]\n",
    "    grad_xF = grad_score(x,False,0,0,mlp_orig)\n",
    "    print(sum(grad_xF**2))\n",
    "    grad_xT = grad_score(x,True,10,0.5,mlp_orig)\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    ax[0].imshow(x.reshape(28,28),cmap=plt.cm.gray)\n",
    "    ax[1].imshow((grad_xF).reshape(28,28),cmap=plt.cm.gray)\n",
    "    ax[2].imshow((grad_xT).reshape(28,28),cmap=plt.cm.gray)\n",
    "    [a.axis('off') for a in ax]\n",
    "    plt.show()\n",
    "# print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score(x,False,0,0,mlp_orig))\n",
    "print(y_test[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 1360\n",
    "eps = 0.5\n",
    "use_hill = True\n",
    "N = 2\n",
    "theta = 0.8\n",
    "x = X0_test[y_test == 3][num].copy()\n",
    "pred = y_test[y_test == 3][num]\n",
    "# pred = score(x,use_hill,N,theta,mlp_orig)\n",
    "print(pred)\n",
    "num_iter = 0\n",
    "# while pred == y_test[num]:\n",
    "while pred < 0.98:\n",
    "    print(score(x,use_hill,N,theta,mlp_orig))\n",
    "    grad = grad_score(x,use_hill,N,theta,mlp_orig)\n",
    "    if y_test[num] == 3:\n",
    "        x += eps*grad[:]\n",
    "    elif y_test[num] == 7:\n",
    "        x -= eps*grad[:]\n",
    "#     pred = mlp_orig.predict(x.reshape(1,-1))\n",
    "    pred = score(x,use_hill,N,theta,mlp_orig)\n",
    "    num_iter += 1\n",
    "print(num_iter)\n",
    "fig,ax = plt.subplots(1,3)\n",
    "ax[0].imshow(X0_test[y_test == 3][num].reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[1].imshow(grad.reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[2].imshow(hill(x.reshape(28,28),N,theta),cmap=plt.cm.gray)\n",
    "[a.axis('off') for a in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(2,30,4):\n",
    "    y = hill(x.reshape(1,-1),i,theta)\n",
    "    print(i, mlp_orig.predict(y))\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(hill(X0_test[num].reshape(28,28),i,theta),cmap=plt.cm.gray)\n",
    "    ax[1].imshow(y.reshape(28,28),cmap=plt.cm.gray)\n",
    "    [a.axis('off') for a in ax]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [int(len(X0_test[y_test == 7])*random.random()) for _ in range(5)]\n",
    "print(sample)\n",
    "score7 = [print(score(X0_test[y_test == 7][i],False,0,0,mlp_orig)) for i in sample]\n",
    "for i in sample:\n",
    "    x = X0_test[y_test == 7][i]\n",
    "    plt.imshow(x.reshape(28,28),cmap=plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2; theta = 0.8\n",
    "sample = [1359, 189, 1314, 366, 1129, 1045]\n",
    "epsilon = np.arange(0.2,1.2,0.2)\n",
    "num_iter_table = [np.zeros((len(sample),len(epsilon)))]*2\n",
    "\n",
    "for i,x in enumerate(X0_test[y_test==7][sample]):\n",
    "    for j,eps in enumerate(epsilon):\n",
    "        num_iter_table[0][i,j] = iter_grad(x,7,False,N,theta,mlp_orig,eps)\n",
    "        num_iter_table[1][i,j] = iter_grad(x,7,True,N,theta,mlp_orig,eps)\n",
    "pprint(num_iter_table[0])\n",
    "pprint(num_iter_table[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
