{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce digits of power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import sklearn.neural_network\n",
    "\n",
    "import neural_network_repn\n",
    "setattr(sklearn,'neural_network',neural_network_repn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten\n",
    "(X, y), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [2,4,6,8,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(sigmoid(np.dot(y,W[0])+b[0]),W[1])+b[1])\n",
    "def grad_score(x,use_hill,N,theta,mlp):\n",
    "    grad = np.zeros(x.shape)\n",
    "    for i in range(len(x)):\n",
    "        yp = x.copy(); yp[i] += 10/255; \n",
    "        ym = x.copy(); ym[i] -= 10/255;\n",
    "        grad[i] = 255/2*(score(yp,use_hill,N,theta,mlp)-score(ym,use_hill,N,theta,mlp))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255\n",
    "XT=x_test/255\n",
    "yT=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(len(X), 28**2)\n",
    "XT=XT.reshape(len(XT), 28**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call MNIST and keep 3s and 7s\n",
    "#mnist = sklearn.datasets.fetch_mldata(\"MNIST original\")\n",
    "\n",
    "# Rescale the data and extract all images for two digits\n",
    "#X, y = mnist.data / 255., mnist.target\n",
    "\n",
    "index = np.where((y == 3) | (y == 7))[0]\n",
    "X0,y = X[index], y[index]\n",
    "\n",
    "Index = np.where((yT == 3) | (yT == 7))[0]\n",
    "XT,yT = XT[Index], yT[Index]\n",
    "\n",
    "X0_train1, X0_train2, y_train1, y_train2 = sklearn.model_selection.train_test_split(X0, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68532570\n",
      "Iteration 2, loss = 0.54169603\n",
      "Iteration 3, loss = 0.45590040\n",
      "Iteration 4, loss = 0.41169568\n",
      "Iteration 5, loss = 0.38324967\n",
      "Iteration 6, loss = 0.36206907\n",
      "Iteration 7, loss = 0.34476738\n",
      "Iteration 8, loss = 0.32997236\n",
      "Iteration 9, loss = 0.31707564\n",
      "Iteration 10, loss = 0.30546932\n",
      "Iteration 11, loss = 0.29496854\n",
      "Iteration 12, loss = 0.28542674\n",
      "Iteration 13, loss = 0.27649154\n",
      "Iteration 14, loss = 0.26825278\n",
      "Iteration 15, loss = 0.26055771\n",
      "Iteration 16, loss = 0.25338672\n",
      "Iteration 17, loss = 0.24660802\n",
      "Iteration 18, loss = 0.24032469\n",
      "Iteration 19, loss = 0.23432757\n",
      "Iteration 20, loss = 0.22868381\n",
      "Iteration 21, loss = 0.22334239\n",
      "Iteration 22, loss = 0.21829543\n",
      "Iteration 23, loss = 0.21351398\n",
      "Iteration 24, loss = 0.20899333\n",
      "Iteration 25, loss = 0.20460361\n",
      "Iteration 26, loss = 0.20048637\n",
      "Iteration 27, loss = 0.19651923\n",
      "Iteration 28, loss = 0.19276620\n",
      "Iteration 29, loss = 0.18915783\n",
      "Iteration 30, loss = 0.18572761\n",
      "Iteration 31, loss = 0.18240273\n",
      "Iteration 32, loss = 0.17923768\n",
      "Iteration 33, loss = 0.17619652\n",
      "Iteration 34, loss = 0.17327373\n",
      "Iteration 35, loss = 0.17050258\n",
      "Iteration 36, loss = 0.16779226\n",
      "Iteration 37, loss = 0.16519403\n",
      "Iteration 38, loss = 0.16269833\n",
      "Iteration 39, loss = 0.16029354\n",
      "Iteration 40, loss = 0.15796500\n",
      "Iteration 41, loss = 0.15573541\n",
      "Iteration 42, loss = 0.15359528\n",
      "Iteration 43, loss = 0.15149640\n",
      "Iteration 44, loss = 0.14948246\n",
      "Iteration 45, loss = 0.14753437\n",
      "Iteration 46, loss = 0.14566573\n",
      "Iteration 47, loss = 0.14384194\n",
      "Iteration 48, loss = 0.14208409\n",
      "Iteration 49, loss = 0.14036741\n",
      "Iteration 50, loss = 0.13870422\n",
      "Iteration 51, loss = 0.13707670\n",
      "Iteration 52, loss = 0.13552712\n",
      "Iteration 53, loss = 0.13401840\n",
      "Iteration 54, loss = 0.13251702\n",
      "Iteration 55, loss = 0.13108444\n",
      "Iteration 56, loss = 0.12970059\n",
      "Iteration 57, loss = 0.12830638\n",
      "Iteration 58, loss = 0.12699148\n",
      "Iteration 59, loss = 0.12568630\n",
      "Iteration 60, loss = 0.12445163\n",
      "Iteration 61, loss = 0.12320771\n",
      "Iteration 62, loss = 0.12204496\n",
      "Iteration 63, loss = 0.12084134\n",
      "Iteration 64, loss = 0.11973119\n",
      "Iteration 65, loss = 0.11861331\n",
      "Iteration 66, loss = 0.11756292\n",
      "Iteration 67, loss = 0.11646731\n",
      "Iteration 68, loss = 0.11542550\n",
      "Iteration 69, loss = 0.11443823\n",
      "Iteration 70, loss = 0.11345976\n",
      "Iteration 71, loss = 0.11250635\n",
      "Iteration 72, loss = 0.11155558\n",
      "Iteration 73, loss = 0.11063804\n",
      "Iteration 74, loss = 0.10972317\n",
      "Iteration 75, loss = 0.10883954\n",
      "Iteration 76, loss = 0.10801603\n",
      "Iteration 77, loss = 0.10716393\n",
      "Iteration 78, loss = 0.10632705\n",
      "Iteration 79, loss = 0.10552631\n",
      "Iteration 80, loss = 0.10472214\n",
      "Iteration 81, loss = 0.10393558\n",
      "Iteration 82, loss = 0.10319346\n",
      "Iteration 83, loss = 0.10244344\n",
      "Iteration 84, loss = 0.10169878\n",
      "Iteration 85, loss = 0.10098090\n",
      "Iteration 86, loss = 0.10026575\n",
      "Iteration 87, loss = 0.09958252\n",
      "Iteration 88, loss = 0.09889802\n",
      "Iteration 89, loss = 0.09825830\n",
      "Iteration 90, loss = 0.09756364\n",
      "Iteration 91, loss = 0.09693158\n",
      "Iteration 92, loss = 0.09630748\n",
      "Iteration 93, loss = 0.09569503\n",
      "Iteration 94, loss = 0.09506800\n",
      "Iteration 95, loss = 0.09450028\n",
      "Iteration 96, loss = 0.09385849\n",
      "Iteration 97, loss = 0.09333259\n",
      "Iteration 98, loss = 0.09272008\n",
      "Iteration 99, loss = 0.09216195\n",
      "Iteration 100, loss = 0.09159152\n",
      "Iteration 101, loss = 0.09104977\n",
      "Iteration 102, loss = 0.09050694\n",
      "Iteration 103, loss = 0.08997095\n",
      "Iteration 104, loss = 0.08944222\n",
      "Iteration 105, loss = 0.08893732\n",
      "Iteration 106, loss = 0.08842934\n",
      "Iteration 107, loss = 0.08793327\n",
      "Iteration 108, loss = 0.08745513\n",
      "Iteration 109, loss = 0.08695374\n",
      "Iteration 110, loss = 0.08647716\n",
      "Iteration 111, loss = 0.08601372\n",
      "Iteration 112, loss = 0.08557632\n",
      "Iteration 113, loss = 0.08508485\n",
      "Iteration 114, loss = 0.08465314\n",
      "Iteration 115, loss = 0.08428219\n",
      "Iteration 116, loss = 0.08374113\n",
      "Iteration 117, loss = 0.08333244\n",
      "Iteration 118, loss = 0.08290965\n",
      "Iteration 119, loss = 0.08248570\n",
      "Iteration 120, loss = 0.08206918\n",
      "Iteration 121, loss = 0.08166259\n",
      "Iteration 122, loss = 0.08124862\n",
      "Iteration 123, loss = 0.08086789\n",
      "Iteration 124, loss = 0.08045562\n",
      "Iteration 125, loss = 0.08007562\n",
      "Iteration 126, loss = 0.07968322\n",
      "Iteration 127, loss = 0.07935150\n",
      "Iteration 128, loss = 0.07896731\n",
      "Iteration 129, loss = 0.07856744\n",
      "Iteration 130, loss = 0.07821807\n",
      "Iteration 131, loss = 0.07785921\n",
      "Iteration 132, loss = 0.07749534\n",
      "Iteration 133, loss = 0.07714325\n",
      "Iteration 134, loss = 0.07681469\n",
      "Iteration 135, loss = 0.07646957\n",
      "Iteration 136, loss = 0.07612286\n",
      "Iteration 137, loss = 0.07578464\n",
      "Iteration 138, loss = 0.07546371\n",
      "Iteration 139, loss = 0.07518423\n",
      "Iteration 140, loss = 0.07481677\n",
      "Iteration 141, loss = 0.07450121\n",
      "Iteration 142, loss = 0.07418136\n",
      "Iteration 143, loss = 0.07389425\n",
      "Iteration 144, loss = 0.07356194\n",
      "Iteration 145, loss = 0.07325396\n",
      "Iteration 146, loss = 0.07297290\n",
      "Iteration 147, loss = 0.07266634\n",
      "Iteration 148, loss = 0.07235994\n",
      "Iteration 149, loss = 0.07206455\n",
      "Iteration 150, loss = 0.07176358\n",
      "Iteration 151, loss = 0.07147305\n",
      "Iteration 152, loss = 0.07122552\n",
      "Iteration 153, loss = 0.07094800\n",
      "Iteration 154, loss = 0.07065062\n",
      "Iteration 155, loss = 0.07035567\n",
      "Iteration 156, loss = 0.07010470\n",
      "Iteration 157, loss = 0.06982005\n",
      "Iteration 158, loss = 0.06954393\n",
      "Iteration 159, loss = 0.06928523\n",
      "Iteration 160, loss = 0.06902757\n",
      "Iteration 161, loss = 0.06876655\n",
      "Iteration 162, loss = 0.06851281\n",
      "Iteration 163, loss = 0.06826324\n",
      "Iteration 164, loss = 0.06800142\n",
      "Iteration 165, loss = 0.06777240\n",
      "Iteration 166, loss = 0.06750758\n",
      "Iteration 167, loss = 0.06728010\n",
      "Iteration 168, loss = 0.06702287\n",
      "Iteration 169, loss = 0.06679149\n",
      "Iteration 170, loss = 0.06656884\n",
      "Iteration 171, loss = 0.06630415\n",
      "Iteration 172, loss = 0.06606267\n",
      "Iteration 173, loss = 0.06584120\n",
      "Iteration 174, loss = 0.06560888\n",
      "Iteration 175, loss = 0.06539839\n",
      "Iteration 176, loss = 0.06514599\n",
      "Iteration 177, loss = 0.06493433\n",
      "Iteration 178, loss = 0.06471133\n",
      "Iteration 179, loss = 0.06449579\n",
      "Iteration 180, loss = 0.06427132\n",
      "Iteration 181, loss = 0.06404203\n",
      "Iteration 182, loss = 0.06386052\n",
      "Iteration 183, loss = 0.06362406\n",
      "Iteration 184, loss = 0.06340781\n",
      "Iteration 185, loss = 0.06320418\n",
      "Iteration 186, loss = 0.06300024\n",
      "Iteration 187, loss = 0.06280867\n",
      "Iteration 188, loss = 0.06259588\n",
      "Iteration 189, loss = 0.06239784\n",
      "Iteration 190, loss = 0.06217875\n",
      "Iteration 191, loss = 0.06197784\n",
      "Iteration 192, loss = 0.06177782\n",
      "Iteration 193, loss = 0.06157947\n",
      "Iteration 194, loss = 0.06139168\n",
      "Iteration 195, loss = 0.06120330\n",
      "Iteration 196, loss = 0.06101446\n",
      "Iteration 197, loss = 0.06082677\n",
      "Iteration 198, loss = 0.06063413\n",
      "Iteration 199, loss = 0.06045952\n",
      "Iteration 200, loss = 0.06027049\n",
      "Iteration 201, loss = 0.06008439\n",
      "Iteration 202, loss = 0.05988927\n",
      "Iteration 203, loss = 0.05974559\n",
      "Iteration 204, loss = 0.05953532\n",
      "Iteration 205, loss = 0.05936947\n",
      "Iteration 206, loss = 0.05916008\n",
      "Iteration 207, loss = 0.05898521\n",
      "Iteration 208, loss = 0.05883024\n",
      "Iteration 209, loss = 0.05864715\n",
      "Iteration 210, loss = 0.05846686\n",
      "Iteration 211, loss = 0.05829221\n",
      "Iteration 212, loss = 0.05812132\n",
      "Iteration 213, loss = 0.05794940\n",
      "Iteration 214, loss = 0.05780038\n",
      "Iteration 215, loss = 0.05760650\n",
      "Iteration 216, loss = 0.05746312\n",
      "Iteration 217, loss = 0.05727858\n",
      "Iteration 218, loss = 0.05712399\n",
      "Iteration 219, loss = 0.05694416\n",
      "Iteration 220, loss = 0.05678511\n",
      "Iteration 221, loss = 0.05666324\n",
      "Iteration 222, loss = 0.05645980\n",
      "Iteration 223, loss = 0.05631359\n",
      "Iteration 224, loss = 0.05615260\n",
      "Iteration 225, loss = 0.05598195\n",
      "Iteration 226, loss = 0.05583746\n",
      "Iteration 227, loss = 0.05569115\n",
      "Iteration 228, loss = 0.05552253\n",
      "Iteration 229, loss = 0.05537460\n",
      "Iteration 230, loss = 0.05520860\n",
      "Iteration 231, loss = 0.05506665\n",
      "Iteration 232, loss = 0.05493096\n",
      "Iteration 233, loss = 0.05478007\n",
      "Iteration 234, loss = 0.05463540\n",
      "Iteration 235, loss = 0.05446863\n",
      "Iteration 236, loss = 0.05431385\n",
      "Iteration 237, loss = 0.05418071\n",
      "Iteration 238, loss = 0.05403517\n",
      "Iteration 239, loss = 0.05391130\n",
      "Iteration 240, loss = 0.05373603\n",
      "Iteration 241, loss = 0.05360677\n",
      "Iteration 242, loss = 0.05346964\n",
      "Iteration 243, loss = 0.05330710\n",
      "Iteration 244, loss = 0.05318601\n",
      "Iteration 245, loss = 0.05305180\n",
      "Iteration 246, loss = 0.05292891\n",
      "Iteration 247, loss = 0.05281786\n",
      "Iteration 248, loss = 0.05262505\n",
      "Iteration 249, loss = 0.05257068\n",
      "Iteration 250, loss = 0.05235097\n",
      "Iteration 251, loss = 0.05222972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.05208441\n",
      "Iteration 253, loss = 0.05193910\n",
      "Iteration 254, loss = 0.05181898\n",
      "Iteration 255, loss = 0.05167718\n",
      "Iteration 256, loss = 0.05153749\n",
      "Iteration 257, loss = 0.05140896\n",
      "Iteration 258, loss = 0.05131116\n",
      "Iteration 259, loss = 0.05121417\n",
      "Iteration 260, loss = 0.05099777\n",
      "Iteration 261, loss = 0.05088870\n",
      "Iteration 262, loss = 0.05073963\n",
      "Iteration 263, loss = 0.05060010\n",
      "Iteration 264, loss = 0.05047814\n",
      "Iteration 265, loss = 0.05033955\n",
      "Iteration 266, loss = 0.05020421\n",
      "Iteration 267, loss = 0.05007508\n",
      "Iteration 268, loss = 0.04995605\n",
      "Iteration 269, loss = 0.04985030\n",
      "Iteration 270, loss = 0.04969391\n",
      "Iteration 271, loss = 0.04959453\n",
      "Iteration 272, loss = 0.04944792\n",
      "Iteration 273, loss = 0.04932505\n",
      "Iteration 274, loss = 0.04919684\n",
      "Iteration 275, loss = 0.04908907\n",
      "Iteration 276, loss = 0.04897236\n",
      "Iteration 277, loss = 0.04882965\n",
      "Iteration 278, loss = 0.04872629\n",
      "Iteration 279, loss = 0.04862656\n",
      "Iteration 280, loss = 0.04846689\n",
      "Iteration 281, loss = 0.04835050\n",
      "Iteration 282, loss = 0.04825727\n",
      "Iteration 283, loss = 0.04813952\n",
      "Iteration 284, loss = 0.04806360\n",
      "Iteration 285, loss = 0.04787557\n",
      "Iteration 286, loss = 0.04776277\n",
      "Iteration 287, loss = 0.04765096\n",
      "Iteration 288, loss = 0.04754698\n",
      "Iteration 289, loss = 0.04742175\n",
      "Iteration 290, loss = 0.04729402\n",
      "Iteration 291, loss = 0.04718366\n",
      "Iteration 292, loss = 0.04707849\n",
      "Iteration 293, loss = 0.04696735\n",
      "Iteration 294, loss = 0.04686688\n",
      "Iteration 295, loss = 0.04673471\n",
      "Iteration 296, loss = 0.04662334\n",
      "Iteration 297, loss = 0.04650803\n",
      "Iteration 298, loss = 0.04639966\n",
      "Iteration 299, loss = 0.04629451\n",
      "Iteration 300, loss = 0.04620526\n",
      "Iteration 301, loss = 0.04607423\n",
      "Iteration 302, loss = 0.04596761\n",
      "Iteration 303, loss = 0.04585261\n",
      "Iteration 304, loss = 0.04574178\n",
      "Iteration 305, loss = 0.04564376\n",
      "Iteration 306, loss = 0.04553140\n",
      "Iteration 307, loss = 0.04542808\n",
      "Iteration 308, loss = 0.04532393\n",
      "Iteration 309, loss = 0.04521482\n",
      "Iteration 310, loss = 0.04509257\n",
      "Iteration 311, loss = 0.04499018\n",
      "Iteration 312, loss = 0.04489187\n",
      "Iteration 313, loss = 0.04477697\n",
      "Iteration 314, loss = 0.04467742\n",
      "Iteration 315, loss = 0.04457756\n",
      "Iteration 316, loss = 0.04447450\n",
      "Iteration 317, loss = 0.04437845\n",
      "Iteration 318, loss = 0.04427182\n",
      "Iteration 319, loss = 0.04415523\n",
      "Iteration 320, loss = 0.04406414\n",
      "Iteration 321, loss = 0.04394418\n",
      "Iteration 322, loss = 0.04385408\n",
      "Iteration 323, loss = 0.04374008\n",
      "Iteration 324, loss = 0.04365001\n",
      "Iteration 325, loss = 0.04355309\n",
      "Iteration 326, loss = 0.04344022\n",
      "Iteration 327, loss = 0.04335437\n",
      "Iteration 328, loss = 0.04326743\n",
      "Iteration 329, loss = 0.04315235\n",
      "Iteration 330, loss = 0.04305777\n",
      "Iteration 331, loss = 0.04294685\n",
      "Iteration 332, loss = 0.04287596\n",
      "Iteration 333, loss = 0.04276051\n",
      "Iteration 334, loss = 0.04267644\n",
      "Iteration 335, loss = 0.04256472\n",
      "Iteration 336, loss = 0.04246412\n",
      "Iteration 337, loss = 0.04237903\n",
      "Iteration 338, loss = 0.04227413\n",
      "Iteration 339, loss = 0.04217830\n",
      "Iteration 340, loss = 0.04206938\n",
      "Iteration 341, loss = 0.04201039\n",
      "Iteration 342, loss = 0.04195376\n",
      "Iteration 343, loss = 0.04180534\n",
      "Iteration 344, loss = 0.04170524\n",
      "Iteration 345, loss = 0.04161921\n",
      "Iteration 346, loss = 0.04152222\n",
      "Iteration 347, loss = 0.04142889\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 348, loss = 0.04132702\n",
      "Iteration 349, loss = 0.04130291\n",
      "Iteration 350, loss = 0.04128389\n",
      "Iteration 351, loss = 0.04126436\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 352, loss = 0.04124583\n",
      "Iteration 353, loss = 0.04124017\n",
      "Iteration 354, loss = 0.04123629\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 355, loss = 0.04123219\n",
      "Iteration 356, loss = 0.04123130\n",
      "Iteration 357, loss = 0.04123056\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 358, loss = 0.04122964\n",
      "Iteration 359, loss = 0.04122945\n",
      "Iteration 360, loss = 0.04122930\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 361, loss = 0.04122911\n",
      "Iteration 362, loss = 0.04122908\n",
      "Iteration 363, loss = 0.04122904\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(4,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_neural = sklearn.neural_network.MLPClassifier(activation = \"relu\", hidden_layer_sizes=(4,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "small_neural.fit(X0_train1,y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2038, 784)\n"
     ]
    }
   ],
   "source": [
    "use_hill=False;N=10;theta=0.5\n",
    "grad,pm = [],[]\n",
    "print(XT.shape)\n",
    "for i in range(len(XT)):\n",
    "    x0 = XT[i]\n",
    "    grad.append(grad_score(x0,use_hill,N,theta,small_neural))\n",
    "    pm.append((yT[i]-5)/2) #3 -> -1; 7 -> +1\n",
    "grad = np.asarray(grad)\n",
    "pm = np.asarray(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.5\n",
    "middle_SM = np.zeros((len(Ns),len(X0_train2)))\n",
    "for idx_N,N in enumerate(Ns):\n",
    "    X = hill(X0_train2,N,theta)\n",
    "    middle_SM[idx_N]=small_neural.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6198, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_SM=np.transpose(middle_SM)\n",
    "middle_SM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71727759\n",
      "Iteration 2, loss = 0.68896476\n",
      "Iteration 3, loss = 0.67292751\n",
      "Iteration 4, loss = 0.66452384\n",
      "Iteration 5, loss = 0.65927791\n",
      "Iteration 6, loss = 0.65522958\n",
      "Iteration 7, loss = 0.65181601\n",
      "Iteration 8, loss = 0.64894296\n",
      "Iteration 9, loss = 0.64647141\n",
      "Iteration 10, loss = 0.64431654\n",
      "Iteration 11, loss = 0.64233506\n",
      "Iteration 12, loss = 0.64048504\n",
      "Iteration 13, loss = 0.63871656\n",
      "Iteration 14, loss = 0.63701906\n",
      "Iteration 15, loss = 0.63534583\n",
      "Iteration 16, loss = 0.63371670\n",
      "Iteration 17, loss = 0.63211948\n",
      "Iteration 18, loss = 0.63055115\n",
      "Iteration 19, loss = 0.62899100\n",
      "Iteration 20, loss = 0.62744207\n",
      "Iteration 21, loss = 0.62590948\n",
      "Iteration 22, loss = 0.62437521\n",
      "Iteration 23, loss = 0.62286226\n",
      "Iteration 24, loss = 0.62133596\n",
      "Iteration 25, loss = 0.61980221\n",
      "Iteration 26, loss = 0.61828448\n",
      "Iteration 27, loss = 0.61675650\n",
      "Iteration 28, loss = 0.61525405\n",
      "Iteration 29, loss = 0.61369044\n",
      "Iteration 30, loss = 0.61214700\n",
      "Iteration 31, loss = 0.61060381\n",
      "Iteration 32, loss = 0.60905054\n",
      "Iteration 33, loss = 0.60746709\n",
      "Iteration 34, loss = 0.60588571\n",
      "Iteration 35, loss = 0.60430951\n",
      "Iteration 36, loss = 0.60270603\n",
      "Iteration 37, loss = 0.60110223\n",
      "Iteration 38, loss = 0.59948779\n",
      "Iteration 39, loss = 0.59782463\n",
      "Iteration 40, loss = 0.59617659\n",
      "Iteration 41, loss = 0.59452054\n",
      "Iteration 42, loss = 0.59283691\n",
      "Iteration 43, loss = 0.59115932\n",
      "Iteration 44, loss = 0.58942135\n",
      "Iteration 45, loss = 0.58770131\n",
      "Iteration 46, loss = 0.58595804\n",
      "Iteration 47, loss = 0.58420494\n",
      "Iteration 48, loss = 0.58241697\n",
      "Iteration 49, loss = 0.58063867\n",
      "Iteration 50, loss = 0.57881507\n",
      "Iteration 51, loss = 0.57696271\n",
      "Iteration 52, loss = 0.57510509\n",
      "Iteration 53, loss = 0.57323732\n",
      "Iteration 54, loss = 0.57137504\n",
      "Iteration 55, loss = 0.56941814\n",
      "Iteration 56, loss = 0.56747864\n",
      "Iteration 57, loss = 0.56554467\n",
      "Iteration 58, loss = 0.56352438\n",
      "Iteration 59, loss = 0.56152476\n",
      "Iteration 60, loss = 0.55950015\n",
      "Iteration 61, loss = 0.55743425\n",
      "Iteration 62, loss = 0.55536953\n",
      "Iteration 63, loss = 0.55319399\n",
      "Iteration 64, loss = 0.55108538\n",
      "Iteration 65, loss = 0.54891676\n",
      "Iteration 66, loss = 0.54670748\n",
      "Iteration 67, loss = 0.54445624\n",
      "Iteration 68, loss = 0.54217104\n",
      "Iteration 69, loss = 0.53985993\n",
      "Iteration 70, loss = 0.53751416\n",
      "Iteration 71, loss = 0.53513706\n",
      "Iteration 72, loss = 0.53270103\n",
      "Iteration 73, loss = 0.53020735\n",
      "Iteration 74, loss = 0.52771266\n",
      "Iteration 75, loss = 0.52510367\n",
      "Iteration 76, loss = 0.52244742\n",
      "Iteration 77, loss = 0.51972722\n",
      "Iteration 78, loss = 0.51696091\n",
      "Iteration 79, loss = 0.51414202\n",
      "Iteration 80, loss = 0.51120634\n",
      "Iteration 81, loss = 0.50822103\n",
      "Iteration 82, loss = 0.50516018\n",
      "Iteration 83, loss = 0.50203430\n",
      "Iteration 84, loss = 0.49887611\n",
      "Iteration 85, loss = 0.49568353\n",
      "Iteration 86, loss = 0.49245925\n",
      "Iteration 87, loss = 0.48927329\n",
      "Iteration 88, loss = 0.48609131\n",
      "Iteration 89, loss = 0.48292559\n",
      "Iteration 90, loss = 0.47982411\n",
      "Iteration 91, loss = 0.47675062\n",
      "Iteration 92, loss = 0.47373999\n",
      "Iteration 93, loss = 0.47073083\n",
      "Iteration 94, loss = 0.46771856\n",
      "Iteration 95, loss = 0.46466425\n",
      "Iteration 96, loss = 0.46166601\n",
      "Iteration 97, loss = 0.45863118\n",
      "Iteration 98, loss = 0.45562402\n",
      "Iteration 99, loss = 0.45258580\n",
      "Iteration 100, loss = 0.44954529\n",
      "Iteration 101, loss = 0.44651365\n",
      "Iteration 102, loss = 0.44346295\n",
      "Iteration 103, loss = 0.44043011\n",
      "Iteration 104, loss = 0.43737692\n",
      "Iteration 105, loss = 0.43434302\n",
      "Iteration 106, loss = 0.43126030\n",
      "Iteration 107, loss = 0.42824545\n",
      "Iteration 108, loss = 0.42521858\n",
      "Iteration 109, loss = 0.42209816\n",
      "Iteration 110, loss = 0.41905621\n",
      "Iteration 111, loss = 0.41600976\n",
      "Iteration 112, loss = 0.41295126\n",
      "Iteration 113, loss = 0.40983682\n",
      "Iteration 114, loss = 0.40679583\n",
      "Iteration 115, loss = 0.40374783\n",
      "Iteration 116, loss = 0.40067372\n",
      "Iteration 117, loss = 0.39763034\n",
      "Iteration 118, loss = 0.39458023\n",
      "Iteration 119, loss = 0.39154694\n",
      "Iteration 120, loss = 0.38849909\n",
      "Iteration 121, loss = 0.38549133\n",
      "Iteration 122, loss = 0.38245293\n",
      "Iteration 123, loss = 0.37943821\n",
      "Iteration 124, loss = 0.37641679\n",
      "Iteration 125, loss = 0.37342023\n",
      "Iteration 126, loss = 0.37044413\n",
      "Iteration 127, loss = 0.36745165\n",
      "Iteration 128, loss = 0.36447763\n",
      "Iteration 129, loss = 0.36150803\n",
      "Iteration 130, loss = 0.35860241\n",
      "Iteration 131, loss = 0.35560946\n",
      "Iteration 132, loss = 0.35271958\n",
      "Iteration 133, loss = 0.34973487\n",
      "Iteration 134, loss = 0.34686285\n",
      "Iteration 135, loss = 0.34394708\n",
      "Iteration 136, loss = 0.34107384\n",
      "Iteration 137, loss = 0.33821611\n",
      "Iteration 138, loss = 0.33534665\n",
      "Iteration 139, loss = 0.33254382\n",
      "Iteration 140, loss = 0.32967934\n",
      "Iteration 141, loss = 0.32690678\n",
      "Iteration 142, loss = 0.32410279\n",
      "Iteration 143, loss = 0.32135689\n",
      "Iteration 144, loss = 0.31857977\n",
      "Iteration 145, loss = 0.31587816\n",
      "Iteration 146, loss = 0.31317488\n",
      "Iteration 147, loss = 0.31043862\n",
      "Iteration 148, loss = 0.30776374\n",
      "Iteration 149, loss = 0.30515118\n",
      "Iteration 150, loss = 0.30245565\n",
      "Iteration 151, loss = 0.29983729\n",
      "Iteration 152, loss = 0.29724031\n",
      "Iteration 153, loss = 0.29467354\n",
      "Iteration 154, loss = 0.29210687\n",
      "Iteration 155, loss = 0.28955866\n",
      "Iteration 156, loss = 0.28704625\n",
      "Iteration 157, loss = 0.28455048\n",
      "Iteration 158, loss = 0.28207039\n",
      "Iteration 159, loss = 0.27963540\n",
      "Iteration 160, loss = 0.27718230\n",
      "Iteration 161, loss = 0.27477415\n",
      "Iteration 162, loss = 0.27241073\n",
      "Iteration 163, loss = 0.27003021\n",
      "Iteration 164, loss = 0.26767782\n",
      "Iteration 165, loss = 0.26546346\n",
      "Iteration 166, loss = 0.26308638\n",
      "Iteration 167, loss = 0.26086031\n",
      "Iteration 168, loss = 0.25857483\n",
      "Iteration 169, loss = 0.25632119\n",
      "Iteration 170, loss = 0.25412485\n",
      "Iteration 171, loss = 0.25196474\n",
      "Iteration 172, loss = 0.24985866\n",
      "Iteration 173, loss = 0.24763099\n",
      "Iteration 174, loss = 0.24552699\n",
      "Iteration 175, loss = 0.24344211\n",
      "Iteration 176, loss = 0.24136184\n",
      "Iteration 177, loss = 0.23932428\n",
      "Iteration 178, loss = 0.23729348\n",
      "Iteration 179, loss = 0.23530168\n",
      "Iteration 180, loss = 0.23332716\n",
      "Iteration 181, loss = 0.23137927\n",
      "Iteration 182, loss = 0.22944446\n",
      "Iteration 183, loss = 0.22753468\n",
      "Iteration 184, loss = 0.22564830\n",
      "Iteration 185, loss = 0.22374838\n",
      "Iteration 186, loss = 0.22193538\n",
      "Iteration 187, loss = 0.22009489\n",
      "Iteration 188, loss = 0.21829978\n",
      "Iteration 189, loss = 0.21654719\n",
      "Iteration 190, loss = 0.21479666\n",
      "Iteration 191, loss = 0.21306090\n",
      "Iteration 192, loss = 0.21132314\n",
      "Iteration 193, loss = 0.20962436\n",
      "Iteration 194, loss = 0.20795875\n",
      "Iteration 195, loss = 0.20629343\n",
      "Iteration 196, loss = 0.20469799\n",
      "Iteration 197, loss = 0.20306138\n",
      "Iteration 198, loss = 0.20146333\n",
      "Iteration 199, loss = 0.19990780\n",
      "Iteration 200, loss = 0.19836999\n",
      "Iteration 201, loss = 0.19680960\n",
      "Iteration 202, loss = 0.19529638\n",
      "Iteration 203, loss = 0.19385307\n",
      "Iteration 204, loss = 0.19236657\n",
      "Iteration 205, loss = 0.19087148\n",
      "Iteration 206, loss = 0.18944947\n",
      "Iteration 207, loss = 0.18802604\n",
      "Iteration 208, loss = 0.18666664\n",
      "Iteration 209, loss = 0.18525792\n",
      "Iteration 210, loss = 0.18386102\n",
      "Iteration 211, loss = 0.18251737\n",
      "Iteration 212, loss = 0.18119522\n",
      "Iteration 213, loss = 0.17989255\n",
      "Iteration 214, loss = 0.17860420\n",
      "Iteration 215, loss = 0.17729340\n",
      "Iteration 216, loss = 0.17604252\n",
      "Iteration 217, loss = 0.17477537\n",
      "Iteration 218, loss = 0.17364720\n",
      "Iteration 219, loss = 0.17233112\n",
      "Iteration 220, loss = 0.17114996\n",
      "Iteration 221, loss = 0.16996240\n",
      "Iteration 222, loss = 0.16879140\n",
      "Iteration 223, loss = 0.16763151\n",
      "Iteration 224, loss = 0.16651173\n",
      "Iteration 225, loss = 0.16537020\n",
      "Iteration 226, loss = 0.16427029\n",
      "Iteration 227, loss = 0.16319116\n",
      "Iteration 228, loss = 0.16208748\n",
      "Iteration 229, loss = 0.16100051\n",
      "Iteration 230, loss = 0.15995371\n",
      "Iteration 231, loss = 0.15892542\n",
      "Iteration 232, loss = 0.15787952\n",
      "Iteration 233, loss = 0.15685826\n",
      "Iteration 234, loss = 0.15587404\n",
      "Iteration 235, loss = 0.15485763\n",
      "Iteration 236, loss = 0.15388099\n",
      "Iteration 237, loss = 0.15291656\n",
      "Iteration 238, loss = 0.15199028\n",
      "Iteration 239, loss = 0.15101623\n",
      "Iteration 240, loss = 0.15009652\n",
      "Iteration 241, loss = 0.14917766\n",
      "Iteration 242, loss = 0.14824953\n",
      "Iteration 243, loss = 0.14735534\n",
      "Iteration 244, loss = 0.14646957\n",
      "Iteration 245, loss = 0.14558930\n",
      "Iteration 246, loss = 0.14474737\n",
      "Iteration 247, loss = 0.14388556\n",
      "Iteration 248, loss = 0.14304231\n",
      "Iteration 249, loss = 0.14221578\n",
      "Iteration 250, loss = 0.14137217\n",
      "Iteration 251, loss = 0.14056355\n",
      "Iteration 252, loss = 0.13974341\n",
      "Iteration 253, loss = 0.13895714\n",
      "Iteration 254, loss = 0.13817009\n",
      "Iteration 255, loss = 0.13739656\n",
      "Iteration 256, loss = 0.13663243\n",
      "Iteration 257, loss = 0.13586292\n",
      "Iteration 258, loss = 0.13512813\n",
      "Iteration 259, loss = 0.13441219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.13364122\n",
      "Iteration 261, loss = 0.13294005\n",
      "Iteration 262, loss = 0.13221867\n",
      "Iteration 263, loss = 0.13153380\n",
      "Iteration 264, loss = 0.13082748\n",
      "Iteration 265, loss = 0.13016063\n",
      "Iteration 266, loss = 0.12948950\n",
      "Iteration 267, loss = 0.12884053\n",
      "Iteration 268, loss = 0.12818120\n",
      "Iteration 269, loss = 0.12753881\n",
      "Iteration 270, loss = 0.12690060\n",
      "Iteration 271, loss = 0.12626513\n",
      "Iteration 272, loss = 0.12567181\n",
      "Iteration 273, loss = 0.12504343\n",
      "Iteration 274, loss = 0.12444514\n",
      "Iteration 275, loss = 0.12386888\n",
      "Iteration 276, loss = 0.12330343\n",
      "Iteration 277, loss = 0.12272019\n",
      "Iteration 278, loss = 0.12215459\n",
      "Iteration 279, loss = 0.12160550\n",
      "Iteration 280, loss = 0.12107575\n",
      "Iteration 281, loss = 0.12053788\n",
      "Iteration 282, loss = 0.12001190\n",
      "Iteration 283, loss = 0.11952200\n",
      "Iteration 284, loss = 0.11899082\n",
      "Iteration 285, loss = 0.11849125\n",
      "Iteration 286, loss = 0.11799853\n",
      "Iteration 287, loss = 0.11751882\n",
      "Iteration 288, loss = 0.11704670\n",
      "Iteration 289, loss = 0.11658362\n",
      "Iteration 290, loss = 0.11610909\n",
      "Iteration 291, loss = 0.11565436\n",
      "Iteration 292, loss = 0.11524269\n",
      "Iteration 293, loss = 0.11480760\n",
      "Iteration 294, loss = 0.11431615\n",
      "Iteration 295, loss = 0.11388847\n",
      "Iteration 296, loss = 0.11346565\n",
      "Iteration 297, loss = 0.11304485\n",
      "Iteration 298, loss = 0.11264045\n",
      "Iteration 299, loss = 0.11224821\n",
      "Iteration 300, loss = 0.11182781\n",
      "Iteration 301, loss = 0.11142473\n",
      "Iteration 302, loss = 0.11103163\n",
      "Iteration 303, loss = 0.11067759\n",
      "Iteration 304, loss = 0.11028200\n",
      "Iteration 305, loss = 0.10987881\n",
      "Iteration 306, loss = 0.10952758\n",
      "Iteration 307, loss = 0.10917499\n",
      "Iteration 308, loss = 0.10879897\n",
      "Iteration 309, loss = 0.10843337\n",
      "Iteration 310, loss = 0.10809695\n",
      "Iteration 311, loss = 0.10773775\n",
      "Iteration 312, loss = 0.10738771\n",
      "Iteration 313, loss = 0.10703072\n",
      "Iteration 314, loss = 0.10669920\n",
      "Iteration 315, loss = 0.10637090\n",
      "Iteration 316, loss = 0.10605503\n",
      "Iteration 317, loss = 0.10571998\n",
      "Iteration 318, loss = 0.10539905\n",
      "Iteration 319, loss = 0.10507719\n",
      "Iteration 320, loss = 0.10477149\n",
      "Iteration 321, loss = 0.10447518\n",
      "Iteration 322, loss = 0.10416490\n",
      "Iteration 323, loss = 0.10386494\n",
      "Iteration 324, loss = 0.10355655\n",
      "Iteration 325, loss = 0.10326113\n",
      "Iteration 326, loss = 0.10298378\n",
      "Iteration 327, loss = 0.10269507\n",
      "Iteration 328, loss = 0.10240588\n",
      "Iteration 329, loss = 0.10210492\n",
      "Iteration 330, loss = 0.10185129\n",
      "Iteration 331, loss = 0.10154520\n",
      "Iteration 332, loss = 0.10127606\n",
      "Iteration 333, loss = 0.10101839\n",
      "Iteration 334, loss = 0.10073689\n",
      "Iteration 335, loss = 0.10047842\n",
      "Iteration 336, loss = 0.10022039\n",
      "Iteration 337, loss = 0.09997732\n",
      "Iteration 338, loss = 0.09970318\n",
      "Iteration 339, loss = 0.09946357\n",
      "Iteration 340, loss = 0.09920500\n",
      "Iteration 341, loss = 0.09896060\n",
      "Iteration 342, loss = 0.09871527\n",
      "Iteration 343, loss = 0.09848126\n",
      "Iteration 344, loss = 0.09826594\n",
      "Iteration 345, loss = 0.09801233\n",
      "Iteration 346, loss = 0.09780167\n",
      "Iteration 347, loss = 0.09755203\n",
      "Iteration 348, loss = 0.09732184\n",
      "Iteration 349, loss = 0.09711204\n",
      "Iteration 350, loss = 0.09686381\n",
      "Iteration 351, loss = 0.09666857\n",
      "Iteration 352, loss = 0.09643429\n",
      "Iteration 353, loss = 0.09621132\n",
      "Iteration 354, loss = 0.09601726\n",
      "Iteration 355, loss = 0.09581348\n",
      "Iteration 356, loss = 0.09558584\n",
      "Iteration 357, loss = 0.09538156\n",
      "Iteration 358, loss = 0.09518458\n",
      "Iteration 359, loss = 0.09497127\n",
      "Iteration 360, loss = 0.09476577\n",
      "Iteration 361, loss = 0.09458088\n",
      "Iteration 362, loss = 0.09438953\n",
      "Iteration 363, loss = 0.09419469\n",
      "Iteration 364, loss = 0.09400144\n",
      "Iteration 365, loss = 0.09380472\n",
      "Iteration 366, loss = 0.09364312\n",
      "Iteration 367, loss = 0.09347241\n",
      "Iteration 368, loss = 0.09325309\n",
      "Iteration 369, loss = 0.09309082\n",
      "Iteration 370, loss = 0.09288465\n",
      "Iteration 371, loss = 0.09270842\n",
      "Iteration 372, loss = 0.09253723\n",
      "Iteration 373, loss = 0.09240099\n",
      "Iteration 374, loss = 0.09219418\n",
      "Iteration 375, loss = 0.09204313\n",
      "Iteration 376, loss = 0.09186312\n",
      "Iteration 377, loss = 0.09168225\n",
      "Iteration 378, loss = 0.09152857\n",
      "Iteration 379, loss = 0.09136435\n",
      "Iteration 380, loss = 0.09120106\n",
      "Iteration 381, loss = 0.09107573\n",
      "Iteration 382, loss = 0.09087712\n",
      "Iteration 383, loss = 0.09072741\n",
      "Iteration 384, loss = 0.09056519\n",
      "Iteration 385, loss = 0.09041225\n",
      "Iteration 386, loss = 0.09027591\n",
      "Iteration 387, loss = 0.09017187\n",
      "Iteration 388, loss = 0.08997010\n",
      "Iteration 389, loss = 0.08979408\n",
      "Iteration 390, loss = 0.08965728\n",
      "Iteration 391, loss = 0.08952916\n",
      "Iteration 392, loss = 0.08937410\n",
      "Iteration 393, loss = 0.08923894\n",
      "Iteration 394, loss = 0.08909328\n",
      "Iteration 395, loss = 0.08895290\n",
      "Iteration 396, loss = 0.08880058\n",
      "Iteration 397, loss = 0.08868336\n",
      "Iteration 398, loss = 0.08853541\n",
      "Iteration 399, loss = 0.08839942\n",
      "Iteration 400, loss = 0.08826967\n",
      "Iteration 401, loss = 0.08813990\n",
      "Iteration 402, loss = 0.08800567\n",
      "Iteration 403, loss = 0.08788502\n",
      "Iteration 404, loss = 0.08774640\n",
      "Iteration 405, loss = 0.08764457\n",
      "Iteration 406, loss = 0.08749600\n",
      "Iteration 407, loss = 0.08737416\n",
      "Iteration 408, loss = 0.08724498\n",
      "Iteration 409, loss = 0.08713092\n",
      "Iteration 410, loss = 0.08705219\n",
      "Iteration 411, loss = 0.08688651\n",
      "Iteration 412, loss = 0.08676890\n",
      "Iteration 413, loss = 0.08664407\n",
      "Iteration 414, loss = 0.08654066\n",
      "Iteration 415, loss = 0.08644822\n",
      "Iteration 416, loss = 0.08632191\n",
      "Iteration 417, loss = 0.08619534\n",
      "Iteration 418, loss = 0.08609549\n",
      "Iteration 419, loss = 0.08597595\n",
      "Iteration 420, loss = 0.08586181\n",
      "Iteration 421, loss = 0.08575416\n",
      "Iteration 422, loss = 0.08565172\n",
      "Iteration 423, loss = 0.08555510\n",
      "Iteration 424, loss = 0.08544652\n",
      "Iteration 425, loss = 0.08533255\n",
      "Iteration 426, loss = 0.08526628\n",
      "Iteration 427, loss = 0.08514590\n",
      "Iteration 428, loss = 0.08501278\n",
      "Iteration 429, loss = 0.08495989\n",
      "Iteration 430, loss = 0.08483719\n",
      "Iteration 431, loss = 0.08470841\n",
      "Iteration 432, loss = 0.08461837\n",
      "Iteration 433, loss = 0.08452453\n",
      "Iteration 434, loss = 0.08445223\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 435, loss = 0.08434699\n",
      "Iteration 436, loss = 0.08431280\n",
      "Iteration 437, loss = 0.08429432\n",
      "Iteration 438, loss = 0.08427517\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 439, loss = 0.08425455\n",
      "Iteration 440, loss = 0.08425180\n",
      "Iteration 441, loss = 0.08424743\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 442, loss = 0.08424433\n",
      "Iteration 443, loss = 0.08424316\n",
      "Iteration 444, loss = 0.08424241\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 445, loss = 0.08424166\n",
      "Iteration 446, loss = 0.08424153\n",
      "Iteration 447, loss = 0.08424131\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 448, loss = 0.08424118\n",
      "Iteration 449, loss = 0.08424113\n",
      "Iteration 450, loss = 0.08424110\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=5, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import sklearn.neural_network\n",
    "\n",
    "import neural_network_repn\n",
    "setattr(sklearn,'neural_network',neural_network_repn)'''\n",
    "master_neural = sklearn.neural_network.MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(5,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "#middle_SM = np.log(middle_SM)\n",
    "master_neural.fit(middle_SM,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden-layer feed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights and Biases for Hidden Layer\n",
    "w1 = small_neural.coefs_[0]\n",
    "b1 = small_neural.intercepts_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6198, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "theta = 0.5\n",
    "middle_HFI = np.zeros(shape=(len(X0_train2),len(Ns),4))\n",
    "for idx_N,N in enumerate(Ns):\n",
    "    X = hill(X0_train2,N,theta)\n",
    "    for i in range(len(X0_train2)):\n",
    "        h1=np.matmul(X0_train2[i],w1) + b1\n",
    "        h1=np.maximum(0, h1)\n",
    "        middle_HFI[i][idx_N]=h1\n",
    "print(middle_HFI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_HFI=middle_HFI.reshape(6198,4*len(Ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65377235\n",
      "Iteration 2, loss = 0.63225205\n",
      "Iteration 3, loss = 0.61178907\n",
      "Iteration 4, loss = 0.59311702\n",
      "Iteration 5, loss = 0.57545666\n",
      "Iteration 6, loss = 0.55871529\n",
      "Iteration 7, loss = 0.54260954\n",
      "Iteration 8, loss = 0.52732164\n",
      "Iteration 9, loss = 0.51260418\n",
      "Iteration 10, loss = 0.49849456\n",
      "Iteration 11, loss = 0.48488527\n",
      "Iteration 12, loss = 0.47175075\n",
      "Iteration 13, loss = 0.45903245\n",
      "Iteration 14, loss = 0.44673731\n",
      "Iteration 15, loss = 0.43482396\n",
      "Iteration 16, loss = 0.42325258\n",
      "Iteration 17, loss = 0.41205540\n",
      "Iteration 18, loss = 0.40122556\n",
      "Iteration 19, loss = 0.39070701\n",
      "Iteration 20, loss = 0.38055175\n",
      "Iteration 21, loss = 0.37071399\n",
      "Iteration 22, loss = 0.36118817\n",
      "Iteration 23, loss = 0.35199309\n",
      "Iteration 24, loss = 0.34309333\n",
      "Iteration 25, loss = 0.33449860\n",
      "Iteration 26, loss = 0.32619586\n",
      "Iteration 27, loss = 0.31816934\n",
      "Iteration 28, loss = 0.31042251\n",
      "Iteration 29, loss = 0.30293393\n",
      "Iteration 30, loss = 0.29570907\n",
      "Iteration 31, loss = 0.28872487\n",
      "Iteration 32, loss = 0.28198155\n",
      "Iteration 33, loss = 0.27548073\n",
      "Iteration 34, loss = 0.26919967\n",
      "Iteration 35, loss = 0.26312561\n",
      "Iteration 36, loss = 0.25727547\n",
      "Iteration 37, loss = 0.25162098\n",
      "Iteration 38, loss = 0.24616648\n",
      "Iteration 39, loss = 0.24088866\n",
      "Iteration 40, loss = 0.23579895\n",
      "Iteration 41, loss = 0.23088733\n",
      "Iteration 42, loss = 0.22613664\n",
      "Iteration 43, loss = 0.22154661\n",
      "Iteration 44, loss = 0.21711806\n",
      "Iteration 45, loss = 0.21283553\n",
      "Iteration 46, loss = 0.20870100\n",
      "Iteration 47, loss = 0.20470231\n",
      "Iteration 48, loss = 0.20084273\n",
      "Iteration 49, loss = 0.19710174\n",
      "Iteration 50, loss = 0.19348814\n",
      "Iteration 51, loss = 0.18999961\n",
      "Iteration 52, loss = 0.18662391\n",
      "Iteration 53, loss = 0.18335262\n",
      "Iteration 54, loss = 0.18019345\n",
      "Iteration 55, loss = 0.17712651\n",
      "Iteration 56, loss = 0.17417606\n",
      "Iteration 57, loss = 0.17130369\n",
      "Iteration 58, loss = 0.16852635\n",
      "Iteration 59, loss = 0.16583976\n",
      "Iteration 60, loss = 0.16323006\n",
      "Iteration 61, loss = 0.16070941\n",
      "Iteration 62, loss = 0.15826564\n",
      "Iteration 63, loss = 0.15589282\n",
      "Iteration 64, loss = 0.15359444\n",
      "Iteration 65, loss = 0.15136414\n",
      "Iteration 66, loss = 0.14920082\n",
      "Iteration 67, loss = 0.14710590\n",
      "Iteration 68, loss = 0.14506931\n",
      "Iteration 69, loss = 0.14309210\n",
      "Iteration 70, loss = 0.14117828\n",
      "Iteration 71, loss = 0.13931018\n",
      "Iteration 72, loss = 0.13750411\n",
      "Iteration 73, loss = 0.13574499\n",
      "Iteration 74, loss = 0.13403830\n",
      "Iteration 75, loss = 0.13237936\n",
      "Iteration 76, loss = 0.13076496\n",
      "Iteration 77, loss = 0.12919512\n",
      "Iteration 78, loss = 0.12767174\n",
      "Iteration 79, loss = 0.12618468\n",
      "Iteration 80, loss = 0.12474319\n",
      "Iteration 81, loss = 0.12333556\n",
      "Iteration 82, loss = 0.12196969\n",
      "Iteration 83, loss = 0.12063704\n",
      "Iteration 84, loss = 0.11934005\n",
      "Iteration 85, loss = 0.11807826\n",
      "Iteration 86, loss = 0.11684659\n",
      "Iteration 87, loss = 0.11565008\n",
      "Iteration 88, loss = 0.11447757\n",
      "Iteration 89, loss = 0.11333800\n",
      "Iteration 90, loss = 0.11223169\n",
      "Iteration 91, loss = 0.11114170\n",
      "Iteration 92, loss = 0.11009015\n",
      "Iteration 93, loss = 0.10905720\n",
      "Iteration 94, loss = 0.10805276\n",
      "Iteration 95, loss = 0.10706899\n",
      "Iteration 96, loss = 0.10611207\n",
      "Iteration 97, loss = 0.10517545\n",
      "Iteration 98, loss = 0.10426339\n",
      "Iteration 99, loss = 0.10336797\n",
      "Iteration 100, loss = 0.10249737\n",
      "Iteration 101, loss = 0.10164765\n",
      "Iteration 102, loss = 0.10081270\n",
      "Iteration 103, loss = 0.10000326\n",
      "Iteration 104, loss = 0.09920182\n",
      "Iteration 105, loss = 0.09842714\n",
      "Iteration 106, loss = 0.09766636\n",
      "Iteration 107, loss = 0.09692079\n",
      "Iteration 108, loss = 0.09619477\n",
      "Iteration 109, loss = 0.09548038\n",
      "Iteration 110, loss = 0.09478559\n",
      "Iteration 111, loss = 0.09410178\n",
      "Iteration 112, loss = 0.09343453\n",
      "Iteration 113, loss = 0.09277921\n",
      "Iteration 114, loss = 0.09214249\n",
      "Iteration 115, loss = 0.09151306\n",
      "Iteration 116, loss = 0.09089714\n",
      "Iteration 117, loss = 0.09029626\n",
      "Iteration 118, loss = 0.08970757\n",
      "Iteration 119, loss = 0.08912954\n",
      "Iteration 120, loss = 0.08856351\n",
      "Iteration 121, loss = 0.08800625\n",
      "Iteration 122, loss = 0.08746314\n",
      "Iteration 123, loss = 0.08692811\n",
      "Iteration 124, loss = 0.08640352\n",
      "Iteration 125, loss = 0.08589184\n",
      "Iteration 126, loss = 0.08538717\n",
      "Iteration 127, loss = 0.08489252\n",
      "Iteration 128, loss = 0.08440713\n",
      "Iteration 129, loss = 0.08393121\n",
      "Iteration 130, loss = 0.08346462\n",
      "Iteration 131, loss = 0.08300509\n",
      "Iteration 132, loss = 0.08255414\n",
      "Iteration 133, loss = 0.08211436\n",
      "Iteration 134, loss = 0.08167927\n",
      "Iteration 135, loss = 0.08125121\n",
      "Iteration 136, loss = 0.08083300\n",
      "Iteration 137, loss = 0.08041969\n",
      "Iteration 138, loss = 0.08001620\n",
      "Iteration 139, loss = 0.07961852\n",
      "Iteration 140, loss = 0.07922710\n",
      "Iteration 141, loss = 0.07884496\n",
      "Iteration 142, loss = 0.07846595\n",
      "Iteration 143, loss = 0.07809511\n",
      "Iteration 144, loss = 0.07772992\n",
      "Iteration 145, loss = 0.07737170\n",
      "Iteration 146, loss = 0.07701893\n",
      "Iteration 147, loss = 0.07667190\n",
      "Iteration 148, loss = 0.07633029\n",
      "Iteration 149, loss = 0.07599600\n",
      "Iteration 150, loss = 0.07566337\n",
      "Iteration 151, loss = 0.07533867\n",
      "Iteration 152, loss = 0.07501983\n",
      "Iteration 153, loss = 0.07470500\n",
      "Iteration 154, loss = 0.07439571\n",
      "Iteration 155, loss = 0.07408821\n",
      "Iteration 156, loss = 0.07379018\n",
      "Iteration 157, loss = 0.07349415\n",
      "Iteration 158, loss = 0.07320278\n",
      "Iteration 159, loss = 0.07291679\n",
      "Iteration 160, loss = 0.07263432\n",
      "Iteration 161, loss = 0.07235506\n",
      "Iteration 162, loss = 0.07208106\n",
      "Iteration 163, loss = 0.07181262\n",
      "Iteration 164, loss = 0.07154600\n",
      "Iteration 165, loss = 0.07128509\n",
      "Iteration 166, loss = 0.07102609\n",
      "Iteration 167, loss = 0.07077355\n",
      "Iteration 168, loss = 0.07052002\n",
      "Iteration 169, loss = 0.07027361\n",
      "Iteration 170, loss = 0.07003103\n",
      "Iteration 171, loss = 0.06978996\n",
      "Iteration 172, loss = 0.06955482\n",
      "Iteration 173, loss = 0.06932061\n",
      "Iteration 174, loss = 0.06909116\n",
      "Iteration 175, loss = 0.06886330\n",
      "Iteration 176, loss = 0.06863986\n",
      "Iteration 177, loss = 0.06841969\n",
      "Iteration 178, loss = 0.06820133\n",
      "Iteration 179, loss = 0.06798639\n",
      "Iteration 180, loss = 0.06777567\n",
      "Iteration 181, loss = 0.06756675\n",
      "Iteration 182, loss = 0.06735882\n",
      "Iteration 183, loss = 0.06715752\n",
      "Iteration 184, loss = 0.06695593\n",
      "Iteration 185, loss = 0.06675848\n",
      "Iteration 186, loss = 0.06656274\n",
      "Iteration 187, loss = 0.06637211\n",
      "Iteration 188, loss = 0.06617995\n",
      "Iteration 189, loss = 0.06599110\n",
      "Iteration 190, loss = 0.06580546\n",
      "Iteration 191, loss = 0.06562336\n",
      "Iteration 192, loss = 0.06544338\n",
      "Iteration 193, loss = 0.06526408\n",
      "Iteration 194, loss = 0.06508867\n",
      "Iteration 195, loss = 0.06491392\n",
      "Iteration 196, loss = 0.06474210\n",
      "Iteration 197, loss = 0.06457325\n",
      "Iteration 198, loss = 0.06440454\n",
      "Iteration 199, loss = 0.06424056\n",
      "Iteration 200, loss = 0.06407460\n",
      "Iteration 201, loss = 0.06391588\n",
      "Iteration 202, loss = 0.06375391\n",
      "Iteration 203, loss = 0.06359740\n",
      "Iteration 204, loss = 0.06344156\n",
      "Iteration 205, loss = 0.06328658\n",
      "Iteration 206, loss = 0.06313457\n",
      "Iteration 207, loss = 0.06298479\n",
      "Iteration 208, loss = 0.06283602\n",
      "Iteration 209, loss = 0.06268923\n",
      "Iteration 210, loss = 0.06254673\n",
      "Iteration 211, loss = 0.06240133\n",
      "Iteration 212, loss = 0.06225931\n",
      "Iteration 213, loss = 0.06211854\n",
      "Iteration 214, loss = 0.06198213\n",
      "Iteration 215, loss = 0.06184454\n",
      "Iteration 216, loss = 0.06170878\n",
      "Iteration 217, loss = 0.06157537\n",
      "Iteration 218, loss = 0.06144169\n",
      "Iteration 219, loss = 0.06131205\n",
      "Iteration 220, loss = 0.06118275\n",
      "Iteration 221, loss = 0.06105491\n",
      "Iteration 222, loss = 0.06092974\n",
      "Iteration 223, loss = 0.06080387\n",
      "Iteration 224, loss = 0.06067899\n",
      "Iteration 225, loss = 0.06055784\n",
      "Iteration 226, loss = 0.06043614\n",
      "Iteration 227, loss = 0.06031657\n",
      "Iteration 228, loss = 0.06019916\n",
      "Iteration 229, loss = 0.06008048\n",
      "Iteration 230, loss = 0.05996604\n",
      "Iteration 231, loss = 0.05985069\n",
      "Iteration 232, loss = 0.05973566\n",
      "Iteration 233, loss = 0.05962503\n",
      "Iteration 234, loss = 0.05951293\n",
      "Iteration 235, loss = 0.05940380\n",
      "Iteration 236, loss = 0.05929612\n",
      "Iteration 237, loss = 0.05918819\n",
      "Iteration 238, loss = 0.05908143\n",
      "Iteration 239, loss = 0.05897611\n",
      "Iteration 240, loss = 0.05887127\n",
      "Iteration 241, loss = 0.05876806\n",
      "Iteration 242, loss = 0.05866687\n",
      "Iteration 243, loss = 0.05856390\n",
      "Iteration 244, loss = 0.05846438\n",
      "Iteration 245, loss = 0.05836550\n",
      "Iteration 246, loss = 0.05826826\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 247, loss = 0.05819366\n",
      "Iteration 248, loss = 0.05816708\n",
      "Iteration 249, loss = 0.05814741\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 250, loss = 0.05813287\n",
      "Iteration 251, loss = 0.05812781\n",
      "Iteration 252, loss = 0.05812389\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 253, loss = 0.05812094\n",
      "Iteration 254, loss = 0.05811993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.05811916\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 256, loss = 0.05811856\n",
      "Iteration 257, loss = 0.05811836\n",
      "Iteration 258, loss = 0.05811821\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 259, loss = 0.05811809\n",
      "Iteration 260, loss = 0.05811805\n",
      "Iteration 261, loss = 0.05811802\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(a,b)=HFI.coefs_\\na2 = a.flatten()\\nc = []\\nfor i in range(len(a2)):\\n    if a2[i] >= 0:\\n        c.append(int(a2[i]+1/2))\\n    else:\\n        c.append(-int(np.abs(a2[i]-1/2)))\\n\\nc = np.array(c).reshape(a.shape)\\nHFI.coefs_[0] = c'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HFI = sklearn.neural_network.MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(5,), max_iter = 500, solver='sgd', random_state=5, learning_rate = 'adaptive',verbose = 1)\n",
    "'''for i, middle in enumerate(middle_HFI):\n",
    "    middle[middle==0] = 1\n",
    "    middle_HFI[i] = middle\n",
    "middle_HFI = np.log(np.abs(middle_HFI))'''\n",
    "HFI.fit(middle_HFI,y_train2)\n",
    "'''(a,b)=HFI.coefs_\n",
    "a2 = a.flatten()\n",
    "c = []\n",
    "for i in range(len(a2)):\n",
    "    if a2[i] >= 0:\n",
    "        c.append(int(a2[i]+1/2))\n",
    "    else:\n",
    "        c.append(-int(np.abs(a2[i]-1/2)))\n",
    "\n",
    "c = np.array(c).reshape(a.shape)\n",
    "HFI.coefs_[0] = c'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "master training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_per3(X,eps,delta):\n",
    "    return X - eps*np.sign(delta)\n",
    "def adv_per7(X,eps,delta):\n",
    "    return X + eps*np.sign(delta)\n",
    "delta = X0[y == 3].mean(axis=0) - X0[y == 7].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT_perturbed = np.zeros(XT.shape)\n",
    "theta = 0.5\n",
    "middle_SM = np.zeros((len(Ns),len(XT)))\n",
    "middle_HFI = np.zeros(shape=(len(XT),len(Ns),4))\n",
    "epsilon = np.arange(0,14,0.1)\n",
    "Thomas = np.zeros((len(Ns),len(epsilon)))\n",
    "score1,score2,score3 = [],[],[]\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    XT_perturbed = []\n",
    "    for i in range(len(pm)):\n",
    "        XT_perturbed.append(XT[i] - pm[i]*eps*grad[i])\n",
    "    XT_perturbed = np.asarray(XT_perturbed)\n",
    "    #XT_perturbed[yT == 4] = adv_per3(XT[yT == 4],eps,delta)\n",
    "    #XT_perturbed[yT == 9] = adv_per7(XT[yT == 9],eps,delta)\n",
    "    for idx_N,N in enumerate(Ns):\n",
    "        X = hill(XT_perturbed,N,theta)\n",
    "        middle_SM[idx_N]=small_neural.predict(X)\n",
    "        Thomas[idx_N][idx_eps]=small_neural.score(X,yT)\n",
    "        for i in range(len(X)):\n",
    "            h1=np.matmul(X[i],w1) + b1\n",
    "            h1=np.maximum(0, h1)\n",
    "            middle_HFI[i][idx_N]=h1\n",
    "    \n",
    "    middleT1=np.transpose(middle_SM)\n",
    "    middleT2=middle_HFI.reshape(len(X),4*len(Ns))\n",
    "    score1.append(master_neural.score(middleT1,yT))\n",
    "    score2.append(HFI.score(middleT2,yT))\n",
    "    score3.append(small_neural.score(XT_perturbed,yT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=3\n",
    "XT_perturbed = []\n",
    "for i in range(len(pm)):\n",
    "    XT_perturbed.append(XT[i] - pm[i]*eps*grad[i])\n",
    "XT_perturbed = np.asarray(XT_perturbed)\n",
    "plt.imshow(XT_perturbed[yT==3][4].reshape(28,28),cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT_perturbed = []\n",
    "for i in range(len(pm)):\n",
    "    XT_perturbed.append(XT[i] - pm[i]*eps*grad[i])\n",
    "XT_perturbed = np.asarray(XT_perturbed)\n",
    "plt.imshow(XT_perturbed[yT==7][7].reshape(28,28),cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(epsilon,score1)\n",
    "ax.plot(epsilon,Thomas[len(Thomas)-1])\n",
    "ax.plot(epsilon,score2)\n",
    "ax.plot(epsilon,score3)\n",
    "ax.set_title(r\"Comparison of Models; $\\theta$ = %.1f\"%theta)\n",
    "ax.set_xlabel(\"Epsilon\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "plt.legend(['Stacked Model','Highest Power','Hidden layer feed in','naive'], title = \"       Model\", loc=3)\n",
    "plt.savefig(\"rs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = True\n",
    "for i in range(len(score1)):\n",
    "    if (score1[i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False\n",
    "final = True\n",
    "for i in range(len(Thomas[len(Thomas)-1])):\n",
    "    if (Thomas[len(Thomas)-1][i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False\n",
    "final = True\n",
    "for i in range(len(score1)):\n",
    "    if (score2[i]<0.8 and final):\n",
    "        print(epsilon[i])\n",
    "        final = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X0_adv_test = np.zeros(X0_test.shape)\n",
    "theta = 0.5\n",
    "epsilon = np.arange(0,0.5,0.01)\n",
    "Ns = np.arange(11)\n",
    "score = np.zeros((len(Ns),len(epsilon)))\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    X0_adv_test[y_test == 3] = adv_per3(X0_test[y_test == 3],eps,delta)\n",
    "    X0_adv_test[y_test == 7] = adv_per7(X0_test[y_test == 7],eps,delta)\n",
    "    for idx_N,N in enumerate(Ns):\n",
    "        X1_adv_test = hill(X0_adv_test,N,theta)\n",
    "        score[idx_N,idx_eps]=mlp_orig.score(X1_adv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for line in score[range(2,12,2),:]:\n",
    "    ax.plot(epsilon,line)\n",
    "    ax.set_title(r\"Even powers; $\\theta$ = %.1f\"%theta)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.legend([2,4,6,8,10], title = \"       N\")\n",
    "    #plt.savefig(\"../Figures/Even-powers.png\")\n",
    "    \n",
    "fig,ax = plt.subplots()\n",
    "for line in score[range(1,11,2),:]:\n",
    "    ax.plot(epsilon,line)\n",
    "    ax.set_title(r\"Odd powers; $\\theta$ = %.1f\"%theta)\n",
    "    ax.set_xlabel(\"Epsilon\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.legend(range(1,11,2), title = \"        N\")\n",
    "    #plt.savefig(\"../Figures/Odd-powers.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_naive = np.zeros((len(epsilon),1))\n",
    "for idx_eps,eps in enumerate(epsilon):\n",
    "    X0_adv_test[y_test == 3] = adv_per3(X0_test[y_test == 3],eps,delta)\n",
    "    X0_adv_test[y_test == 7] = adv_per7(X0_test[y_test == 7],eps,delta)\n",
    "    score_naive[idx_eps]=mlp_orig.score(X0_adv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in score_naive.T:\n",
    "    plt.plot(epsilon,line)\n",
    "    plt.title(\"Naive adversarial perturbation\")\n",
    "    plt.legend([\"No transformation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gradient manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(relu(np.dot(y,W[0])+b[0]),W[1])+b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = np.max((0,x[i]))\n",
    "    return x\n",
    "def hill(x,n,theta):\n",
    "#     return x**n\n",
    "    return x**n/(x**n + theta**n)\n",
    "def score(x,use_hill,N,theta,mlp):\n",
    "    W = mlp.coefs_\n",
    "    b = mlp.intercepts_\n",
    "    if use_hill:\n",
    "        y = hill(x,N,theta)\n",
    "    else:\n",
    "        y = x[:]\n",
    "    return sigmoid(np.dot(relu(np.dot(y,W[0])+b[0]),W[1])+b[1])\n",
    "def grad_score(x,use_hill,N,theta,mlp):\n",
    "    grad = np.zeros(x.shape)\n",
    "    for i in range(len(x)):\n",
    "        yp = x.copy(); yp[i] += 10/255; \n",
    "        ym = x.copy(); ym[i] -= 10/255;\n",
    "        grad[i] = 255/2*(score(yp,use_hill,N,theta,mlp)-score(ym,use_hill,N,theta,mlp))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_grad(x0,use_hill,N,theta,mlp,eps):\n",
    "    num_iter = 0; \n",
    "    x = x0.copy()\n",
    "    pred = mlp.predict(x0.reshape(1,-1))\n",
    "    new_pred = mlp.predict(x.reshape(1,-1))\n",
    "    pm = (pred-5)/2 #3 -> -1; 7 -> +1\n",
    "    while new_pred == pred:\n",
    "        grad = grad_score(x,use_hill,N,theta,mlp)\n",
    "        x -= pm*eps*grad[:]\n",
    "        new_pred = mlp.predict(x.reshape(1,-1))\n",
    "        print(score(x,use_hill,N,theta,mlp))\n",
    "        num_iter += 1\n",
    "    return x,num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hill=True;N=10;theta=0.5\n",
    "x0 = X0_test[y_test==3][1000]\n",
    "grad = grad_score(x0,True,N,theta,mlp_orig)\n",
    "plt.imshow(grad.reshape(28,28),cmap=plt.cm.gray)\n",
    "plt.axis('off');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = X0_test[y_test==7][1000]\n",
    "x1,num_iter = iter_grad(x0,True,10,theta,mlp_orig,5)\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(x0.reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[1].imshow(x1.reshape(28,28),cmap=plt.cm.gray)\n",
    "# ax[2].imshow(hill(x.reshape(28,28),N,theta),cmap=plt.cm.gray)\n",
    "\n",
    "[a.axis('off') for a in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [int(len(X0_test)*random.random()) for _ in range(10)]\n",
    "\n",
    "for i in sample:\n",
    "    x = X0_test[i]\n",
    "    grad_xF = grad_score(x,False,0,0,mlp_orig)\n",
    "    print(sum(grad_xF**2))\n",
    "    grad_xT = grad_score(x,True,10,0.5,mlp_orig)\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    ax[0].imshow(x.reshape(28,28),cmap=plt.cm.gray)\n",
    "    ax[1].imshow((grad_xF).reshape(28,28),cmap=plt.cm.gray)\n",
    "    ax[2].imshow((grad_xT).reshape(28,28),cmap=plt.cm.gray)\n",
    "    [a.axis('off') for a in ax]\n",
    "    plt.show()\n",
    "# print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score(x,False,0,0,mlp_orig))\n",
    "print(y_test[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 1360\n",
    "eps = 0.5\n",
    "use_hill = True\n",
    "N = 2\n",
    "theta = 0.8\n",
    "x = X0_test[y_test == 3][num].copy()\n",
    "pred = y_test[y_test == 3][num]\n",
    "# pred = score(x,use_hill,N,theta,mlp_orig)\n",
    "print(pred)\n",
    "num_iter = 0\n",
    "# while pred == y_test[num]:\n",
    "while pred < 0.98:\n",
    "    print(score(x,use_hill,N,theta,mlp_orig))\n",
    "    grad = grad_score(x,use_hill,N,theta,mlp_orig)\n",
    "    if y_test[num] == 3:\n",
    "        x += eps*grad[:]\n",
    "    elif y_test[num] == 7:\n",
    "        x -= eps*grad[:]\n",
    "#     pred = mlp_orig.predict(x.reshape(1,-1))\n",
    "    pred = score(x,use_hill,N,theta,mlp_orig)\n",
    "    num_iter += 1\n",
    "print(num_iter)\n",
    "fig,ax = plt.subplots(1,3)\n",
    "ax[0].imshow(X0_test[y_test == 3][num].reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[1].imshow(grad.reshape(28,28),cmap=plt.cm.gray)\n",
    "ax[2].imshow(hill(x.reshape(28,28),N,theta),cmap=plt.cm.gray)\n",
    "[a.axis('off') for a in ax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(2,30,4):\n",
    "    y = hill(x.reshape(1,-1),i,theta)\n",
    "    print(i, mlp_orig.predict(y))\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(hill(X0_test[num].reshape(28,28),i,theta),cmap=plt.cm.gray)\n",
    "    ax[1].imshow(y.reshape(28,28),cmap=plt.cm.gray)\n",
    "    [a.axis('off') for a in ax]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [int(len(X0_test[y_test == 7])*random.random()) for _ in range(5)]\n",
    "print(sample)\n",
    "score7 = [print(score(X0_test[y_test == 7][i],False,0,0,mlp_orig)) for i in sample]\n",
    "for i in sample:\n",
    "    x = X0_test[y_test == 7][i]\n",
    "    plt.imshow(x.reshape(28,28),cmap=plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2; theta = 0.8\n",
    "sample = [1359, 189, 1314, 366, 1129, 1045]\n",
    "epsilon = np.arange(0.2,1.2,0.2)\n",
    "num_iter_table = [np.zeros((len(sample),len(epsilon)))]*2\n",
    "\n",
    "for i,x in enumerate(X0_test[y_test==7][sample]):\n",
    "    for j,eps in enumerate(epsilon):\n",
    "        num_iter_table[0][i,j] = iter_grad(x,7,False,N,theta,mlp_orig,eps)\n",
    "        num_iter_table[1][i,j] = iter_grad(x,7,True,N,theta,mlp_orig,eps)\n",
    "pprint(num_iter_table[0])\n",
    "pprint(num_iter_table[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
